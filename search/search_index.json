{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Metagenome Tutorial \u00b6 These tutorials describe some current approaches for metagenome analysis using a genome-resolved approach. Authors include: Aaron Darling Matt DeMaere The tutorial makes heavy use of a pig metagenomic timeseries and Hi-C dataset that was generated by Daniela Gaio and Kay Anantanawat. Several people have made key contributions to the overarching project in which the pig metagenomic data was generated, including Toni Chapman (NSW DPI), Steven P. Djordjevic (UTS), Michael YZ Liu, Tiziana Zingali, Linda Falconer, Joyce To, and Leigh Monahan.","title":"Welcome to the Metagenome Tutorial"},{"location":"#welcome-to-the-metagenome-tutorial","text":"These tutorials describe some current approaches for metagenome analysis using a genome-resolved approach. Authors include: Aaron Darling Matt DeMaere The tutorial makes heavy use of a pig metagenomic timeseries and Hi-C dataset that was generated by Daniela Gaio and Kay Anantanawat. Several people have made key contributions to the overarching project in which the pig metagenomic data was generated, including Toni Chapman (NSW DPI), Steven P. Djordjevic (UTS), Michael YZ Liu, Tiziana Zingali, Linda Falconer, Joyce To, and Leigh Monahan.","title":"Welcome to the Metagenome Tutorial"},{"location":"anvio/","text":"Visualization of metagenome data \u00b6 In this section we will explore ways to visualize the metagenomic data, with a view toward understanding the MAGs that we have reconstructed from our metagenomes. For this we will use software called anvi'o, a highly versatile visualization and analysis environment for metagenomic (and pan-genomic) data. The anvi'o software is extensively documented on the anvi'o website , and rather than recapitulate all of that material here we will just go through the basic steps involved in getting our data loaded into anvi'o. For further details on using the variety of features in anvi'o you can refer to the above site. Installing anvi'o \u00b6 The simplest way to install anvi'o is via conda: conda create -n anvio5 -c bioconda -c conda-forge anvio=5.5.0 Notice this command is slightly different to our previous conda software installations, in this command we are invoking conda create which creates a new conda environment for anvi'o. Once the installation has completed, we will need to activate the environment with conda activate anvio5 in order to use anvi'o. Preparing data for anvi'o \u00b6 First we need to prepare our data for use in anvi'o. Because we already have binning results from MetaBAT2 we will ask anvi'o to import those bins rather than computing new bins. This will allow us to use anvi'o to browse the bins and interactively check and refine them as necessary. But before we get to that, we need to get anvi'o to process the metagenome assembly contigs and the bam files of mapped reads: anvi-gen-contigs-database -f contigs-fixnames.fa -o contigs.db -n 'A gene school DB' anvi-run-hmms -c contigs.db for bam in `ls *.bam`; do anvi-profile -i $bam -c contigs.db; done anvi-merge */PROFILE.db -o SAMPLES-MERGED -c contigs.db --skip-hierarchical-clustering --skip-concoct-binning The above series of commands will take us from assembly contigs to a working anvi'o database, but there is a lot of compute along the way so if the data set is anything more than extremely trivial you will have to be very patient . The pig metagenome timeseries dataset we are using in this tutorial requires over 900 hours of CPU time to process with the above commands. If you have access to a large multicore machine (or large AWS instance) this process can be sped up by running many threads via the -T command-line parameter. Next, we need to create a file that will allow us to import our MetaBAT2 bins into anvi'o. This is a pretty simple process: cd contigs-fixnames.fa.metabat-bins/ grep \">\" bin.*fa | perl -p -i -e \"s/bin\\.(.+).fa:>(.+)/\\$2\\tbin_\\$1/g\" > ../binning_results.txt cd .. anvi-import-collection binning_results.txt -p SAMPLES-MERGED/PROFILE.db -c contigs.db -C \"MetaBAT2\" --contigs-mode anvi-summarize -p SAMPLES-MERGED/PROFILE.db -c contigs.db -C MetaBAT2 -o MERGED_SUMMARY The idea is to create a tab-delimited text file with two columns: the first is contig name and the second is the name of the bin that contig belongs to. The command grep \">\" bin.*fa pulls out the contig names from each FastA bin file, and pipes the result to this command perl -p -i -e \"s/(.+).fa:>(.+)/\\$2\\t\\$1/g\" which is using a perl regular expression to extract the contig name (in $2) and bin ID (in $1) and report them in two columns separated by the tab character \\t . The results are saved in a file called binning_results.txt . We can then import the binning results into our anvi'o database with anvi-import-collection , and then compute some useful summaries of the bins with anvi-summarize . Connecting to an anvi'o server \u00b6 When used in interactive mode anvi'o is usually expected to be running locally on your own machine. However, we are working on VMs in the cloud and so we need to start up an anvi'o server on our remote machine and then connect to it with our web browser. Note that this will only work with google chrome browser . anvi'o's interactive mode does not currently work with any other browsers. You can try it of course but you're on your own when something goes wrong (which, in fact, could be said of almost anything in bioinformatics). anvi-interactive -p SAMPLES-MERGED/PROFILE.db -c contigs.db --server-only -P 8080 --password-protected -C MetaBAT2 when anvi'o launches it will ask you to provide a password. Make one up, and be sure to choose one you can remember at least long enough to log into the server! Once the server is running you can log into it via the chrome web browser by providing the IP address and port 8080 in the location bar, e.g. http://AA.BB.CC.DD:8080 where AA.BB.CC.DD is the IP of your VM. Alternatively if you are using the a provided workshop VM you can just open (in a new tab) the anvio.html file from Jupyter and it will redirect the browser to port 8080. Refining MAGs with anvi'o \u00b6 While the automated binning process implemented in MetaBAT2 is relatively easy to apply even to large datasets, these methods are not perfect and sometimes they can produce erroneous genome bins. anvi'o offers some useful data visualizations methods that can help us to identify cases where a MAG appears to have dubious features. This might be especially relevant if there is a genome that is particularly relevant for your study, for example a nitrogen fixing organism associated with a plant. You might want to confirm that the genome bin looks correct prior to metabolic analysis, for example, to predict culture conditions for isolating an organism. With anvi'o we can inspect individual genome bins, and even modify them interactively, using the anvi-refine command. As with anvi-interactive above this runs via a web server/client structure, so we can launch it on our VM and connect to it with our browser in the same way. For example if we want to refine bin 42 we would run: anvi-refine -p SAMPLES-MERGED/PROFILE.db -c contigs.db --server-only -P 8080 --password-protected -C MetaBAT2 -b bin_42 and then point our browser at the anvi'o server as noted above. For more details about bin refinement with anvi'o check out the tutorials and notes on the anvi'o website: Refining MAGs with anvi'o Notes on bin refinement with anvi'o Challenge exercises \u00b6 Using anvi-interactive Find a bin with a high predicted redundancy rate and another with a low rate. Then load each bin in anvi-refine . How do their profiles differ? If we had a metagenome with two strains where 80% of the gene content was common to both strains, and the binning software reconstructed the most abundant one as a bin, what would the coverage profile for that bin look like in anvi-refine ? What about the coverage standard deviation? What can Single Nucleotide Variants (SNVs) tell us about a genome bin?","title":"Visualization"},{"location":"anvio/#visualization-of-metagenome-data","text":"In this section we will explore ways to visualize the metagenomic data, with a view toward understanding the MAGs that we have reconstructed from our metagenomes. For this we will use software called anvi'o, a highly versatile visualization and analysis environment for metagenomic (and pan-genomic) data. The anvi'o software is extensively documented on the anvi'o website , and rather than recapitulate all of that material here we will just go through the basic steps involved in getting our data loaded into anvi'o. For further details on using the variety of features in anvi'o you can refer to the above site.","title":"Visualization of metagenome data"},{"location":"anvio/#installing-anvio","text":"The simplest way to install anvi'o is via conda: conda create -n anvio5 -c bioconda -c conda-forge anvio=5.5.0 Notice this command is slightly different to our previous conda software installations, in this command we are invoking conda create which creates a new conda environment for anvi'o. Once the installation has completed, we will need to activate the environment with conda activate anvio5 in order to use anvi'o.","title":"Installing anvi'o"},{"location":"anvio/#preparing-data-for-anvio","text":"First we need to prepare our data for use in anvi'o. Because we already have binning results from MetaBAT2 we will ask anvi'o to import those bins rather than computing new bins. This will allow us to use anvi'o to browse the bins and interactively check and refine them as necessary. But before we get to that, we need to get anvi'o to process the metagenome assembly contigs and the bam files of mapped reads: anvi-gen-contigs-database -f contigs-fixnames.fa -o contigs.db -n 'A gene school DB' anvi-run-hmms -c contigs.db for bam in `ls *.bam`; do anvi-profile -i $bam -c contigs.db; done anvi-merge */PROFILE.db -o SAMPLES-MERGED -c contigs.db --skip-hierarchical-clustering --skip-concoct-binning The above series of commands will take us from assembly contigs to a working anvi'o database, but there is a lot of compute along the way so if the data set is anything more than extremely trivial you will have to be very patient . The pig metagenome timeseries dataset we are using in this tutorial requires over 900 hours of CPU time to process with the above commands. If you have access to a large multicore machine (or large AWS instance) this process can be sped up by running many threads via the -T command-line parameter. Next, we need to create a file that will allow us to import our MetaBAT2 bins into anvi'o. This is a pretty simple process: cd contigs-fixnames.fa.metabat-bins/ grep \">\" bin.*fa | perl -p -i -e \"s/bin\\.(.+).fa:>(.+)/\\$2\\tbin_\\$1/g\" > ../binning_results.txt cd .. anvi-import-collection binning_results.txt -p SAMPLES-MERGED/PROFILE.db -c contigs.db -C \"MetaBAT2\" --contigs-mode anvi-summarize -p SAMPLES-MERGED/PROFILE.db -c contigs.db -C MetaBAT2 -o MERGED_SUMMARY The idea is to create a tab-delimited text file with two columns: the first is contig name and the second is the name of the bin that contig belongs to. The command grep \">\" bin.*fa pulls out the contig names from each FastA bin file, and pipes the result to this command perl -p -i -e \"s/(.+).fa:>(.+)/\\$2\\t\\$1/g\" which is using a perl regular expression to extract the contig name (in $2) and bin ID (in $1) and report them in two columns separated by the tab character \\t . The results are saved in a file called binning_results.txt . We can then import the binning results into our anvi'o database with anvi-import-collection , and then compute some useful summaries of the bins with anvi-summarize .","title":"Preparing data for anvi'o"},{"location":"anvio/#connecting-to-an-anvio-server","text":"When used in interactive mode anvi'o is usually expected to be running locally on your own machine. However, we are working on VMs in the cloud and so we need to start up an anvi'o server on our remote machine and then connect to it with our web browser. Note that this will only work with google chrome browser . anvi'o's interactive mode does not currently work with any other browsers. You can try it of course but you're on your own when something goes wrong (which, in fact, could be said of almost anything in bioinformatics). anvi-interactive -p SAMPLES-MERGED/PROFILE.db -c contigs.db --server-only -P 8080 --password-protected -C MetaBAT2 when anvi'o launches it will ask you to provide a password. Make one up, and be sure to choose one you can remember at least long enough to log into the server! Once the server is running you can log into it via the chrome web browser by providing the IP address and port 8080 in the location bar, e.g. http://AA.BB.CC.DD:8080 where AA.BB.CC.DD is the IP of your VM. Alternatively if you are using the a provided workshop VM you can just open (in a new tab) the anvio.html file from Jupyter and it will redirect the browser to port 8080.","title":"Connecting to an anvi'o server"},{"location":"anvio/#refining-mags-with-anvio","text":"While the automated binning process implemented in MetaBAT2 is relatively easy to apply even to large datasets, these methods are not perfect and sometimes they can produce erroneous genome bins. anvi'o offers some useful data visualizations methods that can help us to identify cases where a MAG appears to have dubious features. This might be especially relevant if there is a genome that is particularly relevant for your study, for example a nitrogen fixing organism associated with a plant. You might want to confirm that the genome bin looks correct prior to metabolic analysis, for example, to predict culture conditions for isolating an organism. With anvi'o we can inspect individual genome bins, and even modify them interactively, using the anvi-refine command. As with anvi-interactive above this runs via a web server/client structure, so we can launch it on our VM and connect to it with our browser in the same way. For example if we want to refine bin 42 we would run: anvi-refine -p SAMPLES-MERGED/PROFILE.db -c contigs.db --server-only -P 8080 --password-protected -C MetaBAT2 -b bin_42 and then point our browser at the anvi'o server as noted above. For more details about bin refinement with anvi'o check out the tutorials and notes on the anvi'o website: Refining MAGs with anvi'o Notes on bin refinement with anvi'o","title":"Refining MAGs with anvi'o"},{"location":"anvio/#challenge-exercises","text":"Using anvi-interactive Find a bin with a high predicted redundancy rate and another with a low rate. Then load each bin in anvi-refine . How do their profiles differ? If we had a metagenome with two strains where 80% of the gene content was common to both strains, and the binning software reconstructed the most abundant one as a bin, what would the coverage profile for that bin look like in anvi-refine ? What about the coverage standard deviation? What can Single Nucleotide Variants (SNVs) tell us about a genome bin?","title":"Challenge exercises"},{"location":"assembly/","text":"Metagenome assembly \u00b6 What is assembly? \u00b6 Assembly is the process of reconstructing long DNA sequences from a collection of overlapping shorter pieces. In the context of single genomes (e.g. microbial isolates) the goal is usually to reconstruct a single chromosome and any associated plasmids or other replicons. When presented with metagenomic data though, the microbial community might in reality have millions of genomes present in it. Generally speaking, it is impossible to accurately reconstruct every genome of every cell in a metagenomic sample with current technology. Instead we usually aim to reconstruct chromosome sequences that represent a consensus of many cells. Assembling metagenomes \u00b6 There are several available metagenome assembly tools. Some examples include metaSPAdes and MEGAHIT . In this tutorial we will use MEGAHIT, and we will use it via a docker container. As such, no installation step is necessary. Coassembly or single sample? \u00b6 Just as important as deciding what assembler to use is deciding on what to assemble . One approach is to assemble all of the metagenome samples together. This can be useful if there is not much data per sample, or if low abundance organisms exist in the samples that would not have enough coverage in a single sample to be assembled. But it can create problems if closely related species or strains exist in the different samples. Limitations of metagenome assembly \u00b6 Current assembly methods are limited in their ability to resolve chromosomes of closely related species and strains in a sample. This is because most assembly methods collapse sequences > 95% or 98% identity into a single contig sequence, usually as a means to cope with sequencing error. Therefore as a consensus representation, these assembled chromosome sequences may mask fine-scale genetic variation in the population. Assembling some example data \u00b6 parallel-fastq-dump -t 4 --outdir asm --split-files --gzip -s SRR8960410 -s SRR8960409 -s SRR8960402 -s SRR8960368 -s SRR8960420 -s SRR8960739 -s SRR8960679 -s SRR8960627 -s SRR8960591 -s SRR8960887 --minSpotId 0 --maxSpotId 50000 && mv asm assembly The above command will download the first 50000 read-pairs of a set of samples. All of these samples come from the same pig, and were collected at different time points in consecutive weeks. Now we can assemble with megahit: singularity exec -B ~/assembly/:/data docker://quay.io/biocontainers/megahit:1.1.3--py36_0 megahit -1 /data/SRR8960410_1.fastq.gz,/data/SRR8960409_1.fastq.gz,/data/SRR8960402_1.fastq.gz,/data/SRR8960368_1.fastq.gz,/data/SRR8960420_1.fastq.gz,/data/SRR8960739_1.fastq.gz,/data/SRR8960679_1.fastq.gz,/data/SRR8960627_1.fastq.gz,/data/SRR8960591_1.fastq.gz,/data/SRR8960887_1.fastq.gz -2 /data/SRR8960410_2.fastq.gz,/data/SRR8960409_2.fastq.gz,/data/SRR8960402_2.fastq.gz,/data/SRR8960368_2.fastq.gz,/data/SRR8960420_2.fastq.gz,/data/SRR8960739_2.fastq.gz,/data/SRR8960679_2.fastq.gz,/data/SRR8960627_2.fastq.gz,/data/SRR8960591_2.fastq.gz,/data/SRR8960887_2.fastq.gz -o /data/metaasm That's a big command-line, so let's unpack what's happening. First, we're invoking singularity . Singularity is a container service that can download and run programs that have been packaged as containers -- a system that allows all needed dependency software to be specified and obtained automatically. By invoking singularity exec we are saying that we want to run a command inside a container. Simply put it allows us to run the software easily and reliably, avoiding the manual software install process. The container we want to use is specified as docker://quay.io/biocontainers/megahit:1.1.3--py36_0 . This is a docker container, and the docker:// syntax tells singularity that it can download the container from a public server. The :1.1.3--py36_0 specifies the exact version of the megahit container to use. Before the container specification we have the argument -B ~/assembly/:/data . This argument binds the assembly/ directory in our current path to appear as /data inside the running container. Therefore the programs running inside the container (e.g. megahit) will be able to see all the files inside ~/assembly/ at the path /data . Next we have the megahit command line. This includes parameters -1 and -2 with a list of the FastQ files we want to assemble. Finally we ask megahit to save the assembly in the container path /data/metaasm , so it will show up in ~/assembly/metaasm when the container has finished running. At the end of this process we will have a metagenome assembly saved in the file ~/assembly/final.contigs.fa . We can use this file for subsequent analyses. One small detail we need to resolve is the formatting of contig names in the assembly file. megahit creates contigs with names like ``, but the whitespace in these names causes problems for certain downstream analyses, such as visualization with anvi'o. Therefore as a final step in the assembly process we need to rename the contigs with the following commands: cd assembly/metaasm cp final.contigs.fa contigs-fixnames.fa perl -p -i -e \"s/(>\\w+) flag.*/\\$1/g\" contigs-fixnames.fa","title":"Assembly"},{"location":"assembly/#metagenome-assembly","text":"","title":"Metagenome assembly"},{"location":"assembly/#what-is-assembly","text":"Assembly is the process of reconstructing long DNA sequences from a collection of overlapping shorter pieces. In the context of single genomes (e.g. microbial isolates) the goal is usually to reconstruct a single chromosome and any associated plasmids or other replicons. When presented with metagenomic data though, the microbial community might in reality have millions of genomes present in it. Generally speaking, it is impossible to accurately reconstruct every genome of every cell in a metagenomic sample with current technology. Instead we usually aim to reconstruct chromosome sequences that represent a consensus of many cells.","title":"What is assembly?"},{"location":"assembly/#assembling-metagenomes","text":"There are several available metagenome assembly tools. Some examples include metaSPAdes and MEGAHIT . In this tutorial we will use MEGAHIT, and we will use it via a docker container. As such, no installation step is necessary.","title":"Assembling metagenomes"},{"location":"assembly/#coassembly-or-single-sample","text":"Just as important as deciding what assembler to use is deciding on what to assemble . One approach is to assemble all of the metagenome samples together. This can be useful if there is not much data per sample, or if low abundance organisms exist in the samples that would not have enough coverage in a single sample to be assembled. But it can create problems if closely related species or strains exist in the different samples.","title":"Coassembly or single sample?"},{"location":"assembly/#limitations-of-metagenome-assembly","text":"Current assembly methods are limited in their ability to resolve chromosomes of closely related species and strains in a sample. This is because most assembly methods collapse sequences > 95% or 98% identity into a single contig sequence, usually as a means to cope with sequencing error. Therefore as a consensus representation, these assembled chromosome sequences may mask fine-scale genetic variation in the population.","title":"Limitations of metagenome assembly"},{"location":"assembly/#assembling-some-example-data","text":"parallel-fastq-dump -t 4 --outdir asm --split-files --gzip -s SRR8960410 -s SRR8960409 -s SRR8960402 -s SRR8960368 -s SRR8960420 -s SRR8960739 -s SRR8960679 -s SRR8960627 -s SRR8960591 -s SRR8960887 --minSpotId 0 --maxSpotId 50000 && mv asm assembly The above command will download the first 50000 read-pairs of a set of samples. All of these samples come from the same pig, and were collected at different time points in consecutive weeks. Now we can assemble with megahit: singularity exec -B ~/assembly/:/data docker://quay.io/biocontainers/megahit:1.1.3--py36_0 megahit -1 /data/SRR8960410_1.fastq.gz,/data/SRR8960409_1.fastq.gz,/data/SRR8960402_1.fastq.gz,/data/SRR8960368_1.fastq.gz,/data/SRR8960420_1.fastq.gz,/data/SRR8960739_1.fastq.gz,/data/SRR8960679_1.fastq.gz,/data/SRR8960627_1.fastq.gz,/data/SRR8960591_1.fastq.gz,/data/SRR8960887_1.fastq.gz -2 /data/SRR8960410_2.fastq.gz,/data/SRR8960409_2.fastq.gz,/data/SRR8960402_2.fastq.gz,/data/SRR8960368_2.fastq.gz,/data/SRR8960420_2.fastq.gz,/data/SRR8960739_2.fastq.gz,/data/SRR8960679_2.fastq.gz,/data/SRR8960627_2.fastq.gz,/data/SRR8960591_2.fastq.gz,/data/SRR8960887_2.fastq.gz -o /data/metaasm That's a big command-line, so let's unpack what's happening. First, we're invoking singularity . Singularity is a container service that can download and run programs that have been packaged as containers -- a system that allows all needed dependency software to be specified and obtained automatically. By invoking singularity exec we are saying that we want to run a command inside a container. Simply put it allows us to run the software easily and reliably, avoiding the manual software install process. The container we want to use is specified as docker://quay.io/biocontainers/megahit:1.1.3--py36_0 . This is a docker container, and the docker:// syntax tells singularity that it can download the container from a public server. The :1.1.3--py36_0 specifies the exact version of the megahit container to use. Before the container specification we have the argument -B ~/assembly/:/data . This argument binds the assembly/ directory in our current path to appear as /data inside the running container. Therefore the programs running inside the container (e.g. megahit) will be able to see all the files inside ~/assembly/ at the path /data . Next we have the megahit command line. This includes parameters -1 and -2 with a list of the FastQ files we want to assemble. Finally we ask megahit to save the assembly in the container path /data/metaasm , so it will show up in ~/assembly/metaasm when the container has finished running. At the end of this process we will have a metagenome assembly saved in the file ~/assembly/final.contigs.fa . We can use this file for subsequent analyses. One small detail we need to resolve is the formatting of contig names in the assembly file. megahit creates contigs with names like ``, but the whitespace in these names causes problems for certain downstream analyses, such as visualization with anvi'o. Therefore as a final step in the assembly process we need to rename the contigs with the following commands: cd assembly/metaasm cp final.contigs.fa contigs-fixnames.fa perl -p -i -e \"s/(>\\w+) flag.*/\\$1/g\" contigs-fixnames.fa","title":"Assembling some example data"},{"location":"binning/","text":"Metagenome Assembled Genomes (MAGs) from metagenome binning \u00b6 What is a MAG? \u00b6 A MAG, or Metagenome Assembled Genome, is a genome that has been reconstructed from metagenomic data. Because these genomes do not derive from a clonal culture they typically represent a consensus of the genomes of many closely related cells in one or more metagenomic samples, and for this reason they are sometimes called population genomes . It is important to remember that because of the way MAGs are inferred from the data the resulting sequence is usually fragmentary and may not accurately represent the genome of any cell in the community. It is merely an average estimate, and there are well-known problems with taking averages, see for example the wikipedia page on Simpson's paradox to get an idea of how averages can go wrong. What is a good MAG? \u00b6 The international genomics community has made an effort to define quality standards for MAGs via the Minimum Information about a Metagenome Assembled Genome (MIMAG) standards. The basic idea is to require certain minimum levels of estimated completenes and maximal levels of estimated contamination in the MAG for it to be considered as one of \"Low-quality\", \"Medium-quality\", \"High-quality\", or \"Finished\". See Table 1 in the above-linked paper for the full details. Generally speaking it is hard to achieve \"High-quality\" and nearly impossible to get \"Finished\" with short read sequencing data alone. Long read metagenome data has been shown to produce results in these categories, although it can be difficult to obtain sufficient DNA for those methods, and they currently remain more expensive than short read sequencing. Making MAGs \u00b6 The process of reconstructing genomes from a metagenome is often referred to as metagenome binning or just binning , from the process of assigning contigs to one 'bin' per genome. There are many binning tools available to extract MAGs from metagenome assemblies. When timeseries data is available, MetaBAT2 is a good choice because it is both easy to use and offers good performance. MetaBAT2 can be installed via conda as follows: conda install -c bioconda metabat2 MetaBAT2 input data \u00b6 As input, MetaBAT2 requires reads from each of the shotgun metagenome samples to be mapped back to the metagenome assembly. We will not cover the read mapping process in this tutorial, but it can be carried out using standard mapping software such as bwa mem or bowtie2 . Be sure to sort and index the bam files with samtools prior to running metabat. Assuming we have bam files of mapped reads and the metagenome assembly available in a directory called ~/data we can compute genome bins as follows: mkdir ~/metabat ; cd ~/metabat runMetaBat.sh ~/data/contigs-fixnames.fa ~/data/*.bam Depending on how much data you've got this can take a long time to compute. Luckily MetaBAT2 has a good parallel implementation so it can go faster if you run on a large multi-core machine. At the end of the process MetaBAT2 will produce a directory called contigs-fixnames.fa.metabat-bins , which contains one FastA file of contigs for each genome bin that it inferred.","title":"Binning"},{"location":"binning/#metagenome-assembled-genomes-mags-from-metagenome-binning","text":"","title":"Metagenome Assembled Genomes (MAGs) from metagenome binning"},{"location":"binning/#what-is-a-mag","text":"A MAG, or Metagenome Assembled Genome, is a genome that has been reconstructed from metagenomic data. Because these genomes do not derive from a clonal culture they typically represent a consensus of the genomes of many closely related cells in one or more metagenomic samples, and for this reason they are sometimes called population genomes . It is important to remember that because of the way MAGs are inferred from the data the resulting sequence is usually fragmentary and may not accurately represent the genome of any cell in the community. It is merely an average estimate, and there are well-known problems with taking averages, see for example the wikipedia page on Simpson's paradox to get an idea of how averages can go wrong.","title":"What is a MAG?"},{"location":"binning/#what-is-a-good-mag","text":"The international genomics community has made an effort to define quality standards for MAGs via the Minimum Information about a Metagenome Assembled Genome (MIMAG) standards. The basic idea is to require certain minimum levels of estimated completenes and maximal levels of estimated contamination in the MAG for it to be considered as one of \"Low-quality\", \"Medium-quality\", \"High-quality\", or \"Finished\". See Table 1 in the above-linked paper for the full details. Generally speaking it is hard to achieve \"High-quality\" and nearly impossible to get \"Finished\" with short read sequencing data alone. Long read metagenome data has been shown to produce results in these categories, although it can be difficult to obtain sufficient DNA for those methods, and they currently remain more expensive than short read sequencing.","title":"What is a good MAG?"},{"location":"binning/#making-mags","text":"The process of reconstructing genomes from a metagenome is often referred to as metagenome binning or just binning , from the process of assigning contigs to one 'bin' per genome. There are many binning tools available to extract MAGs from metagenome assemblies. When timeseries data is available, MetaBAT2 is a good choice because it is both easy to use and offers good performance. MetaBAT2 can be installed via conda as follows: conda install -c bioconda metabat2","title":"Making MAGs"},{"location":"binning/#metabat2-input-data","text":"As input, MetaBAT2 requires reads from each of the shotgun metagenome samples to be mapped back to the metagenome assembly. We will not cover the read mapping process in this tutorial, but it can be carried out using standard mapping software such as bwa mem or bowtie2 . Be sure to sort and index the bam files with samtools prior to running metabat. Assuming we have bam files of mapped reads and the metagenome assembly available in a directory called ~/data we can compute genome bins as follows: mkdir ~/metabat ; cd ~/metabat runMetaBat.sh ~/data/contigs-fixnames.fa ~/data/*.bam Depending on how much data you've got this can take a long time to compute. Luckily MetaBAT2 has a good parallel implementation so it can go faster if you run on a large multi-core machine. At the end of the process MetaBAT2 will produce a directory called contigs-fixnames.fa.metabat-bins , which contains one FastA file of contigs for each genome bin that it inferred.","title":"MetaBAT2 input data"},{"location":"everythingelse/","text":"What we didn't cover \u00b6 Experimental design \u00b6 There are an almost endless array of questions that can be addressed with metagenomics and in this tutorial material we have only begun to scratch the surface of what is possible. As a research tool, metagenomics is now beginning to move away from being purely discovery-oriented towards becoming a tool that is used to test specific hypotheses. There are many, many things to consider when designing an experiment that involves metagenomics as a means to test a hypothesis. First among these is whether the effect would be measurable via metagenomic data, e.g. is metagenomics the right tool for the job? Then comes questions like: How deeply will you sequence? How many samples will you need? Will samples be structured as a time-series, or transect of some kind? How much background variation exists in the metagenomes that will be sampled? And based on the above, using a particular set of metagenome analysis tools, what is our statistical power to reject the hypothesis in question? Simulation for experimental design \u00b6 One way to design and power a large metagenomic experiment is to carry out a computational simulation of the metagenomic sequencing that follows the experimental structure. While this may seem tedious, I would argue that if the experiment is worth doing, then it is almost certainly worth doing a simulation study first. There are a number of advantages to this kind of an approach. For example: Hidden obstacles to the data analysis will be identified a priori , before the long and expensive experimental work and data generation begins. A data analysis workflow can be developed up front, so that when the data arrives the work to analyse it becomes simpler. The experiment can be rationally designed, with clear expectations about the number and type of samples required to measure an effect of a particular size. One of the challenges to taking this approach is that reasonable simulation parameters may not be known. If you're working in a well-studied microbial ecosystem like the human gut it may be possible to design an experiment entirely using knowledge gained from existing public datasets. But if you're working in a more obscure system then there may not be much, if any, public data to use for experimental design. In general the solution to this problem requires an iterative process, in which an initial small batch of pilot data is created, from which reasonable parameters can be estimated for the design of a larger study. Resolving strain mixtures \u00b6 When a metagenomic sample contains genomes from more than one strain of the same species, or from two closely related species, it can cause the assembly to become highly fragmentary. The resulting assembly contigs can be very difficult to analyse with standard genome binning methods, for multiple reasons. First, many binning tools will not process contigs if they are below a particular size. Depending on the exact level of sequence identity among the two strains, only a small fraction of the genome might be present in contigs above the lower size limit. Second, some of the contigs can represent a coassembly of the two strains. Not only can this result in contig sequences that look unlike any of the individual genomes, but it also poses a problem for standard metagenome binning software because the contig belongs in more than one bin, yet these softwares can only assign contigs to single bins. While there has been some work to try to resolve strain mixtures from metagenome assemblies, such as that implemented in the DESMAN software , the problem remains challenging and better solutions are needed. Alpha and beta diversity, ecological networks \u00b6 Although people usually carry out 16S amplicon sequencing to answer questions about microbial ecology it is also possible to address these with metagenomic data. There are a range of computational methods and tools for taxonomy-driven metagenome community profiling. Some of these were reviewed and evaluated in the CAMI publication . The taxonomy-based methods are limited, however, in that they can only resolve named taxonomic groups. Much of the microbial diversity on our planet remains undescribed, and this is where phylogenetic methods have the potential to greatly outperform the taxonomic methods. Phylogenetic methods don't depend on human-curated taxonomic structures, and therefore are capable of analysing wholly novel groups of organisms. The equivalent class of methods in the 16S amplicon sequencing world are the reference-free de novo OTU or Amplicon Sequence Variant (ASV) analysis methods. The list goes on \u00b6 There are many, many more ways to analyze metagenomic data. Once MAGs have been reconstructed from the metagenomes pretty much any of the analyses that could be carried out on isolate genomes can be applied. There are also a range of metagenome-specific analyses that could be applied to MAGs, such as association studies, phylogenetic studies, functional and metabolic analyses, and more.","title":"Everything Else"},{"location":"everythingelse/#what-we-didnt-cover","text":"","title":"What we didn't cover"},{"location":"everythingelse/#experimental-design","text":"There are an almost endless array of questions that can be addressed with metagenomics and in this tutorial material we have only begun to scratch the surface of what is possible. As a research tool, metagenomics is now beginning to move away from being purely discovery-oriented towards becoming a tool that is used to test specific hypotheses. There are many, many things to consider when designing an experiment that involves metagenomics as a means to test a hypothesis. First among these is whether the effect would be measurable via metagenomic data, e.g. is metagenomics the right tool for the job? Then comes questions like: How deeply will you sequence? How many samples will you need? Will samples be structured as a time-series, or transect of some kind? How much background variation exists in the metagenomes that will be sampled? And based on the above, using a particular set of metagenome analysis tools, what is our statistical power to reject the hypothesis in question?","title":"Experimental design"},{"location":"everythingelse/#simulation-for-experimental-design","text":"One way to design and power a large metagenomic experiment is to carry out a computational simulation of the metagenomic sequencing that follows the experimental structure. While this may seem tedious, I would argue that if the experiment is worth doing, then it is almost certainly worth doing a simulation study first. There are a number of advantages to this kind of an approach. For example: Hidden obstacles to the data analysis will be identified a priori , before the long and expensive experimental work and data generation begins. A data analysis workflow can be developed up front, so that when the data arrives the work to analyse it becomes simpler. The experiment can be rationally designed, with clear expectations about the number and type of samples required to measure an effect of a particular size. One of the challenges to taking this approach is that reasonable simulation parameters may not be known. If you're working in a well-studied microbial ecosystem like the human gut it may be possible to design an experiment entirely using knowledge gained from existing public datasets. But if you're working in a more obscure system then there may not be much, if any, public data to use for experimental design. In general the solution to this problem requires an iterative process, in which an initial small batch of pilot data is created, from which reasonable parameters can be estimated for the design of a larger study.","title":"Simulation for experimental design"},{"location":"everythingelse/#resolving-strain-mixtures","text":"When a metagenomic sample contains genomes from more than one strain of the same species, or from two closely related species, it can cause the assembly to become highly fragmentary. The resulting assembly contigs can be very difficult to analyse with standard genome binning methods, for multiple reasons. First, many binning tools will not process contigs if they are below a particular size. Depending on the exact level of sequence identity among the two strains, only a small fraction of the genome might be present in contigs above the lower size limit. Second, some of the contigs can represent a coassembly of the two strains. Not only can this result in contig sequences that look unlike any of the individual genomes, but it also poses a problem for standard metagenome binning software because the contig belongs in more than one bin, yet these softwares can only assign contigs to single bins. While there has been some work to try to resolve strain mixtures from metagenome assemblies, such as that implemented in the DESMAN software , the problem remains challenging and better solutions are needed.","title":"Resolving strain mixtures"},{"location":"everythingelse/#alpha-and-beta-diversity-ecological-networks","text":"Although people usually carry out 16S amplicon sequencing to answer questions about microbial ecology it is also possible to address these with metagenomic data. There are a range of computational methods and tools for taxonomy-driven metagenome community profiling. Some of these were reviewed and evaluated in the CAMI publication . The taxonomy-based methods are limited, however, in that they can only resolve named taxonomic groups. Much of the microbial diversity on our planet remains undescribed, and this is where phylogenetic methods have the potential to greatly outperform the taxonomic methods. Phylogenetic methods don't depend on human-curated taxonomic structures, and therefore are capable of analysing wholly novel groups of organisms. The equivalent class of methods in the 16S amplicon sequencing world are the reference-free de novo OTU or Amplicon Sequence Variant (ASV) analysis methods.","title":"Alpha and beta diversity, ecological networks"},{"location":"everythingelse/#the-list-goes-on","text":"There are many, many more ways to analyze metagenomic data. Once MAGs have been reconstructed from the metagenomes pretty much any of the analyses that could be carried out on isolate genomes can be applied. There are also a range of metagenome-specific analyses that could be applied to MAGs, such as association studies, phylogenetic studies, functional and metabolic analyses, and more.","title":"The list goes on"},{"location":"hic/","text":"Metagenomic Hi-C \u00b6 What is Metagenomic Hi-C? \u00b6 As the name suggests, Metagenomic Hi-C is an adaptation of the Hi-C protocol for chromosome 3D structur profiling to shotgun metagenome analysis. Metagenomic Hi-C is a useful addition to the metagenome analysis toolbox because it enables things like genome binning to be carried out accurately and precisely from a single sample rather than a large collection of samples. The Hi-C protocol was designed to capture genome-wide evidence of DNA molecules in close physical proximity in vivo , prior to cellular lysis. With the flexible nature of a chromosome and its capacity to bend back on itself, the most frequently observed interactions are usually those between nearby loci on the same chromosome (intra-chromosomal). After this, the next most frequent interactions are between chromosomes within the same cell (inter-chromosomal). The least common are inter-cellular interactions and are often seen at rates far below those found within a cell. The original purpose of Hi-C was to study the 3-dimensional conformation of the human genome [1], but this captured proximity information can be readily applied to improve draft genome and metagenome assemblies. In metagenomics we can use the proximity information to infer which assembly fragments (contigs) originated from the same cell (or chromosome). Though the power of the \"proximity signal\" should provide a level of precision down to individual chromosomes, metagenomic analysis tools currently aim to associate assembly fragments at the genome level into MAGs (as described in the earlier tutorial section on genome binning). The Hi-C Library Protocol \u00b6 Major steps of the Hi-C protocol Basic Concept \u00b6 Though Hi-C read-sets are generated using conventional high-throughput Illumina paired-end sequencing, the more complicated Hi-C protocol produces a very different library. Unlike traditional shotgun sequencing, where read pairs originate from nearby regions on the same contiguous DNA molecule (i.e. chromosome, plasmid, etc), the Hi-C protocol can generate read-pairs from any two strands of DNA that were in close physical proximity within the cell. So long as the two DNA loci interact, regardless of their location with the genome, there is a chance of producing a Hi-C read-pair from that interaction. Hi-C read pairs can therefore originate from far-away regions of the same molecule or from entirely different molecules. To achieve this, the Hi-C library protocol involves several steps upstream of Illumina adapter ligation and is more challenging to execute in the lab. As a result, commercial kits have been developed which can simplify the process and improve quality and consistency of the resulting libraries. Protocol Outline \u00b6 DNA Fixation: Beginning with intact cells, the first step of the Hi-C protocol is formalin fixation. The act of cross-linking \"locks in\" close-by conformation arrangements within the DNA that existed in the cells at the time of fixation. Cell Lysis: The cells are lysed and DNA-protein complexes extracted and purified. Restriction Digest: The DNA is digested using a restriction endonuclease. In metagenomic Hi-C, enzymes with large overhangs and 4-nt recognition sites are common choices (i.e. Sau3AI, MluCI, DpnII). Differences in the GC bias of the recognition site and the target DNA is an important factor, as inefficient digestion will produce fewer Hi-C read-pairs. Biotin Tagging: The overhangs produced during digestion are end-filled with biotinylated nucleotides. Free-end Ligation: in dilute conditions or immobilised on a substrate, free-ends protruding from the DNA-protein complexes are ligated. This stochastic process favours any two ends which were nearby within the complex, however random ligation and self-ligation can occur and the reads resulting from such events create noise/error in the data. Crosslink Reversal: The bonds created by formalin fixation are removed, allowing the now free DNA to be purified. Proteinase digestion is common following this step. Un-ligated End Clean-up: Free-ends which failed to form ligation products are unwanted in subsequent steps but could still be biotin tagged. To minimise their inclusion, a light exonuclease 3' to 5' favoured chew-back can be applied. DNA Shearing: With ligation completed, the DNA is mechanically sheared and the size range of the resulting fragment selected suitability with Illumina paired-end sequencing. DNA repair: Sonication can lead to damaged ends and nicks, which can be repaired. Proximity Ligation Enrichment: Biotin tagged fragments are pulled down using affinity purification. Adapter Ligation: Illumina paired-end adapter ligation and associated steps to produce a sequencing library are now applied. Quality Control: is my library ok? \u00b6 Due to the highly variable nature of environmental microbe samples, the possible presence of enzyme inhibitors, and other reasons, the metagenomic Hi-C protocol may not always work with high efficiency, or at all. Problems with sample processing can sometimes be identified prior to sequencing, for example if the measured library yield is very low. But other problems may be difficult to diagnose prior to sequencing, for example, it is very difficult to estimate the efficiency of proximity ligation and thus the fraction of sequencing reads that will be Hi-C reads without actually sequencing the sample. The reads that are not Hi-C reads are essentially conventional shotgun read-pairs, which in the context of Hi-C are uninformative. The value of the proximity information contained within a Hi-C read-set is high enough that even with a low efficiency library, it can be worthwhile to compensate for low efficiency by sequencing more deeply to obtain more Hi-C read pairs. Thus, determining the percentage of Hi-C read-pairs in the library is important, because it can tell us how deeply we need to sequence our library. Rather than submit a library to a large and costly sequencing run immediately, a small pilot sequencing run can be sufficient to confidently estimate the Hi-C efficiency. Armed with this information, a researcher can make informed decisions about further action; such as whether a candidate Hi-C library should be fully sequenced and to what depth. We now discuss various ways to estimate the fraction of Hi-C reads in a library. Evidence of Proximity Ligation \u00b6 Long-range pairs \u00b6 The separation distance for intra-chromosomal Hi-C read-pairs is bounded only by the length of the chromosome. This is unlike shotgun read-pairs whose separation is rarely more than a thousand nucleotides due to chemistry limitations on Illumina instruments. Though it is less direct than looking for evidence of cut-site duplication events, a simple count of the number of read pairs than map far apart can be used to infer the percentage of Hi-C read-pairs. The main limitation of this approach is that it requires a high quality reference assembly against which the Hi-C read pairs can be mapped. In many metagenomic projects, such an assembly is not actually available, either because a good quality shotgun metagenome dataset has not yet been generated for the sample or because there are no close reference genomes in public databases for the organisms in the sample. Counting the number of of pairs which map at long-range can as be used to infer the percentage of Hi-C read-pairs. Proximity ligation junctions \u00b6 In the Hi-C protocol outlined above, the steps of creating free-ends through enzymatic digestion, subsequent end-repair, and finally re-ligation introduces an artifact at the junction point. This artifact is a short sequence duplication and the exact sequence enzyme dependent. The duplication is produced when the cleavage site overhangs are subjected to end-repair. When the repaired blunt ends are subsequently ligated, their junction contains two copies of the overhang sequence. Proximity junction for the enzyme Sau3AI Sau3AI is a 4-cutter with a 4nt overhang. Native DNA 5`-XXXXGATCYYYY-3' 3'-xxxxCTAGyyyy-5' Free-ends post cleavage have overhangs 5`-XXXX GATCYYYY-3' 3'-xxxxCTAG yyyy-5' Blunt ends after end-repair with biotinylated nucleotides 5`-XXXXGATC GATCYYYY-3' 3'-xxxxCTAG CTAGyyyy-5' Free-end ligation produces a duplication at the junction 5`-XXXXGATCGATCYYYY-3' 3'-xxxxCTAGCTAGyyyy-5' After the steps of DNA shearing and size selection, fragments which eventually go on to DNA sequencing can contain a junction at any point along their extent. An Illumina paired-end sequencing run, which generates reads at either end of a fragment, thus has two chances to read through the junction. This approach has two main drawbacks: If the fragments in the sequencing library are long relative to the read length, there may be junctions within the fragment that remain unobserved, and this fact must be corrected for in the estimate. If the sample was degraded there may have been free ends that were not created by an enzyme cut. These could become proximity-ligated, and therefore ligation junctions may exist that do not contain the obvious junction sequence. Searching the read-set for examples of this junction sequence is one means of measuring the percentage of Hi-C read-pairs in a given library. Using a QC Tool \u00b6 Though the methodology of Hi-C QC has yet to achieve standardisation, a number of tools exist which offer forms of QC testing. These tools can be found as components of Hi-C analysis pipelines ( HiCpipe ), embedded in tool-suites ( HiCExplorer ) and as stand-alone command-line tools ( hic_qc , qc3C ) [2-5]. In this tutorial we will use qc3C to assess Hi-C library quality in two different ways: Read mapping based analysis k -mer based analysis. Get some Hi-C data and tools \u00b6 For a single timepoint, we'll first download a small Hi-C read-set and its associated shotgun read-set. Also, we'll pull down the Docker images for qc3C (quality testing) and bin3C (metagenome binning) Get some Hi-C data and the qc3C and bin3C docker images # download the read-set parallel-fastq-dump -s SRR9323809 -s SRR8960211 --threads 4 --outdir hic_data/ --minSpotId 0 --maxSpotId 1000000 --split-files --gzip # fetch the qc3C image sudo docker pull cerebis/qc3c:alpine # fetch the bin3c image sudo docker pull cerebis/bin3c:latest Make sure you have parallel-fastq-dump If you've skipped the first section on Sequencing run QC, please do this first. conda install -c bioconda parallel-fastq-dump Create a metagenome assembly and map Hi-C reads \u00b6 The shotgun dataset has been limited to 1M read-pairs, this is only 1/165 th of the total. Obviously this reduction in total coverage will lead to a much sparser sampling of the community and a more fragmented assembly. Nevertheless, it is enough data to demonstrate the concept of QC and Hi-C metagenome binning. Lets assemble the shotgun data and map both library types to the resulting contigs. Create a SPAdes assembly # enter the hic data directory cd hic_data # launch spades in metagenomic mode sudo docker run -v $PWD :/opt/app-root cerebis/bin3c:latest spades.py --meta -1 SRR8960211_1.fastq.gz -2 SRR8960211_2.fastq.gz -o asm # index the contig fasta sudo docker run -v $PWD :/opt/app-root cerebis/bin3c:latest bwa index asm/contigs.fasta # map hi-c reads to the contigs sudo docker run -v $PWD :/opt/app-root cerebis/bin3c:latest /bin/bash -c \"bwa mem -t4 -5SP asm/contigs.fasta SRR9323809_1.fastq.gz SRR9323809_2.fastq.gz | samtools view -uS -F 0x904 - | samtools sort -@4 -n -o hic_to_ctg.bam -\" # map shotgun reads to the contigs sudo docker run -v $PWD :/opt/app-root cerebis/bin3c:latest /bin/bash -c \"bwa mem -t4 -5SP asm/contigs.fasta SRR8960211_1.fastq.gz SRR8960211_2.fastq.gz | samtools view -uS -F 0x904 - | samtools sort -@4 -n -o wgs_to_ctg.bam -\" # return to your home directory cd Breaking down the read mapping command We are providing Docker access to our host filesystem using a bind mount ( -v $PWD:/opt/app-root ) Next we are specifying the image to run ( cerebis/bin3c:latest ). Finally we include the actual call, which involves pipes between bwa and samtools . To achieve this we pass the bash shell our complex command, which it then executes within the container. BAM mode analysis with qc3C \u00b6 We begin with the conceptually simpler approach to Hi-C QC, counting the number of long-range read-pairs from a Hi-C read-set. As a rule of thumb, to work well with Illumina sequencing, the physical size of fragments is likely to be <1000nt. We shall therefore simply count the number of pairs which map with a separation >1000, >5000 and >10,000 nt. As the mapped read-pair separation grows beyond 1000nt we expect an increasingly large proportion of these pairs will be due to Hi-C proximity ligation. Conversely, when analysing a shotgun read-set, we expect to see very few. Run a BAM based analysis # enter the hic data folder cd hic_data # perform bam based QC analysis on Hi-C data sudo docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine ash -c \"qc3C bam -m 400 -e Sau3AI -e MluCI -b hic_to_ctg.bam\" # now perform the analysis on shotgun data sudo docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine ash -c \"qc3C bam -m 400 -e Sau3AI -e MluCI -b wgs_to_ctg.bam\" # return to your home directory cd Estimation of fragment size \u00b6 As we are not sure of the mean insert length, we guess it to be 400nt. This is one handy benefit of the BAM mode approach, we can infer a reasonably good estimate of the fragment (insert) length. In k -mer mode, having an accurate value becomes important if we wish to estimate the probable fraction of unobserved proximity junctions. That is to say, for long fragments and short reads, much of each fragment goes unsequenced and so we will miss the evidence of junctions. In our resulting log from above, qc3C reports an observed mean fragment length of 445nt for the Hi-C read-set. INFO | 2019-06-23 18:55:18,652 | qc3C.bam_based | Observed mean of short-range pair separation: 445nt Looking at the results \u00b6 The last three lines of the output from qc3C report the absolute number and relative fraction of read-pairs that mapped far apart. For the Hi-C library, the fraction of pairs separated by more than 10kb was ~2.7%, while in contrast for the known shotgun library the fraction was <0.006%. Hi-C result INFO | 2019-06-23 18:55:18,654 | qc3C.bam_based | Long-range distance intervals: 1000nt, 5000nt, 10000nt INFO | 2019-06-23 18:55:18,654 | qc3C.bam_based | Number of observed pairs: 94, 46, 23 INFO | 2019-06-23 18:55:18,654 | qc3C.bam_based | Relative fraction of all obs: 0.1116, 0.05463, 0.02732 Shotgun result INFO | 2019-06-23 18:56:13,330 | qc3C.bam_based | Long-range distance intervals: 1000nt, 5000nt, 10000nt INFO | 2019-06-23 18:56:13,331 | qc3C.bam_based | Number of observed pairs: 284, 43, 23 INFO | 2019-06-23 18:56:13,331 | qc3C.bam_based | Relative fraction of all obs: 0.0007268, 0.0001101, 5.886e-05 It is important to keep in mind that as the assembly was made from a shotgun dataset downsampled to very low coverage, the length of contigs is significantly lower than would have been obtained with the full dataset. This will impair the odds of seeing pairs which map far away. Despite this, the 97% of pairs in the shotgun library map to the same contig sequence. In contrast, 72% of Hi-C pairs mapped to the same contig. Overall, for BAM mode, the reliability of results is connected to the quality and completeness of the available references. Breaking down the invocation of qc3C We are providing Docker access to our host filesystem using a bind mount ( -v $PWD:/opt/app-root ) Next we are specifying the image to run ( cerebis/qc3c:alpine ). Finally we include the actual call to qc3C , along with its own options. Potential Concerns Requires a reference sequence: a completed genome a draft assembly. May not be an option when: no reference is available only a very highly fragmented draft exists Computational requirements: create the shotgun assembly map Hi-C reads to the reference The distribution of assembly contig lengths will affect the maximum mapping distance. k -mer mode analysis with qc3C \u00b6 To analyze our small Hi-C read-set using a k -mer approach, we will first create a k -mer library using Jellyfish. Since this Hi-C experiment used the 4-cutter Sau3AI, which produces an 8-mer junction, we'll use a mer size of 24 for the library. This will give us 8 nucleotides either side of any prospective junction, for specificity. Choosing how large a k -mer size to use for the library is a trade-off between computational complexity and minimising false discovery. You can try sizes bigger and smaller. Running k-mer based analyses On the Hi-C data # enter the hic data folder cd hic_data # create a generator used by Jellyfish for compressed files rm -f gen_fq && for fn in ` ls SRR9323809*fastq.gz ` ; do echo gzip -dc $fn >> gen_fq ; done # use jellyfish to create a 24-mer library from FastQ reads sudo docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine jellyfish count -t 4 -m 24 -s 100M -C -g gen_fq -o hic24.jf # perform k-mer based QC analysis sudo docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine ash -c \"qc3C kmer -m 445 -s 1234 -e Sau3AI -l hic24.jf -r SRR9323809_1.fastq.gz -r SRR9323809_2.fastq.gz\" # return back to your home directory cd Now on the shotgun data # enter the hic data folder cd hic_data # create a generator used by Jellyfish for compressed files echo \"gzip -dc SRR8960211_1.fastq.gz\" > gen_sg # use jellyfish to create a 24-mer library from FastQ reads sudo docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine jellyfish count -t 4 -m 24 -s 100M -C -g gen_sg -o wgs24.jf # perform k-mer based QC analysis sudo docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine ash -c \"qc3C kmer -m 306 -s 1234 -e Sau3AI -l wgs24.jf -r SRR8960211_1.fastq.gz -p 0.05\" # return back to your home directory cd Looking at the results \u00b6 Looking at the inferred fraction of true Hi-C read-pairs, we can immediately see that the low coverage shotgun assembly appears to have drastically skewed the estimate of signal (2.7% vs 6.9 \u00b1 2.4%). After adjustment for the unobserved fraction (0.32), the signal estimate increases to 9.1 \u00b1 3.1%. In comparison, for the shotgun data the inferred fraction of Hi-C read-pairs is 0.22 \u00b1 9.4%, with an unobserved fraction of just 0.013. This value of this estimate is dubious at best. If we look further up the log we can see that the number of observed junctions was 0.098%, while by random chance this would be 0.0015%. Although this appears greater than by chance and possibly significant, this is put clear in perspective when compared to the Hi-C result of 5.9%. Therefore, the Hi-C library appears to have a reasonable fraction of Hi-C reads. As a pilot study, we should probably send this to be more deeply sequenced. Hi-C result INFO | 2019-06-23 19:45:13,826 | qc3C.kmer_based | For supplied insert length of 445nt, estimated unobserved fraction: 0.3213 INFO | 2019-06-23 19:45:13,826 | qc3C.kmer_based | Adjusted estimation of Hi-C read fraction: 9.135 \u00b1 3.148 % INFO | 2019-06-23 19:30:03,640 | qc3C.kmer_based | Estimated Hi-C read fraction via p-value sum method: 7.211 \u00b1 2.756 % Shotgun result INFO | 2019-06-23 19:47:10,690 | qc3C.kmer_based | Estimated Hi-C read fraction via p-value sum method: 0.2242 \u00b1 9.365 % INFO | 2019-06-23 19:47:10,690 | qc3C.kmer_based | For supplied insert length of 306nt, estimated unobserved fraction: 0.01307 INFO | 2019-06-23 19:47:10,690 | qc3C.kmer_based | Adjusted estimation of Hi-C read fraction: 0.2272 \u00b1 9.487 % Breaking down the invocation of jellyfish We are providing Docker access to our host filesystem using a bind mount ( -v $PWD:/opt/app-root ) Next we are specifying the image to run ( cerebis/qc3c:alpine ). Finally we include the actual call to jellyfish , along with its own options. K-mer mode requirements No-assembly or reference required Requires making a k-mer library from FastQ reads Analyse FastQ reads against library Metagenome Binning with Hi-C Data \u00b6 As we saw by processing timeseries data with MetaBAT2 , metagenome binning can also be performed using Hi-C. The primary difference here is that accurate and precise binning is possible from a single timepoint, so long as you have you have both a shotgun and Hi-C read-set. Currenty, only a few tools exist which were made specifically for this purpose ( bin3C , ProxiMeta ) [6,7]. In this tutorial we will use bin3C, as it is open-source and easy to use. With a set of reference sequences and Hi-C to reference BAM file in hand (from above), a bin3C analysis has two stages: first create the contact map, second cluster the contact map. We also need to know what enzyme(s) was used in generating the library. It is becoming more common to generate libraries from two enzymes with differing GC biases to improve Hi-C coverage within a community. In such cases, we simply specify them when creating the map, as here. Perform Hi-C metagenome binning # enter hic data directory cd hic_data # Make contact map sudo docker run -v $PWD :/opt/app-root cerebis/bin3c:latest bin3C -v mkmap -e Sau3AI -e MluCI --eta --bin-size 5000 asm/contigs.fasta hic_to_ctg.bam bin3C_out # Cluster contact map sudo docker run -v $PWD :/opt/app-root cerebis/bin3c:latest bin3C -v cluster --clobber --only-large bin3C_out/contact_map.p.gz bin3C_out # return to your home directory cd Looking at the results \u00b6 After bin3C completes the clustering step, the three outputs which are possibly the most interesting are: The comma delimited cluster_report.csv table the cluster_report details characteristics of each identified cluster. An id, name, number of contigs, total extent, mean GC and coverage, etc. Mean coverage is a reasonable gauge of relative abundance A qualitative overview of the result in the form of a heatmap cluster_plot.png contigs are organized by cluster, followed by descending length a pixel represents a 5kb bin and therefore contigs are represented proportional to their length. Clustered sequence data the fasta/ directory contains cluster FastA data The cluster plot is a visual representation of the clustered contact map. As a metagenomic assembly can easily contain 50k to 100k contigs, which as a NxN image would be very large, the plot's resolution is constrained by default to 4000x4000 pixels to conserve memory. Once organised, a good Hi-C read-set will have produced a map that is clearly in block-diagonal form with high contrast. Also, a significant proportion of the map's field is should be dark, reflecting the fact that inter-cellular interactions are rare. In cases where there was insufficient signal, there will only be a few (if any) large clusters and they will look sparse internally. Example cluster plot of a human microbome MAG clusters of a single-timepoint human faecal microbiome (PRJNA413092) as determined by bin3C . The shotgun read-set consisted of 249M pairs, while the Hi-C library contained 41M pairs (~10% Hi-C). Of the identified MAGs, 55 where >90% complete, <5% contamination as estimated by CheckM [6]. If you are familiar with the appearance of a single-genome microbial contact map, at sufficient zoom you may notice that the individual clusters do not look the same. This is due to the fact that the proper ordering of contigs (scaffolding) within the clusters has not been established. After reaching this point, standard downstream analyses would be to profile the taxonomic composition of the clusters with tools such as CheckM . Expectations from Hi-C binning Hi-C metagenome binning is dependent on both the quality of the shotgun assembly and the depth of true Hi-C read-pairs. For a Hi-C data-set with strong signal (perhaps 10%) and reasonable depth of sequencing (100M pairs), it is reasonable to expect 50-100 nearly complete MAGs [6]. References \u00b6 Lieberman-Aiden, E., van Berkum, N. L., Williams, L., Imakaev, M., Ragoczy, T., Telling, A., \u2026 Dekker, J. (2009). Comprehensive mapping of long-range interactions reveals folding principles of the human genome. Science , 326(5950), 289\u2013293. https://doi.org/10.1126/science.1181369 fchen. (n.d.). HiCpipe. Retrieved from https://github.com/ChenFengling/HiCpipe Wolff, J., Bhardwaj, V., Nothjunge, S., Richard, G., Renschler, G., Gilsbach, R., \u2026 Gr\u00fcning, B. A. (2018). Galaxy HiCExplorer: a web server for reproducible Hi-C data analysis, quality control and visualization. Nucleic Acids Research , 46(W1), W11\u2013W16. https://doi.org/10.1093/nar/gky504 DeMaere, M. Z., Darling A. E. (n.d.). qc3C. Retrieved from https://github.com/cerebis/qc3C hic_qc. (n.d.). Retrieved from https://github.com/phasegenomics/hic_qc DeMaere, M. Z., & Darling, A. E. (2019). bin3C: exploiting Hi-C sequencing data to accurately resolve metagenome-assembled genomes. Genome Biology , 20(1), 46. https://doi.org/10.1186/s13059-019-1643-1 Press, M. O., Wiser, A. H., Kronenberg, Z. N., Langford, K. W., Shakya, M., Lo, C.-C., \u2026 Liachko, I. (2017). Hi-C deconvolution of a human gut microbiome yields high-quality draft genomes and reveals plasmid-genome interactions (p. 198713). https://doi.org/10.1101/198713","title":"Hi-C"},{"location":"hic/#metagenomic-hi-c","text":"","title":"Metagenomic Hi-C"},{"location":"hic/#what-is-metagenomic-hi-c","text":"As the name suggests, Metagenomic Hi-C is an adaptation of the Hi-C protocol for chromosome 3D structur profiling to shotgun metagenome analysis. Metagenomic Hi-C is a useful addition to the metagenome analysis toolbox because it enables things like genome binning to be carried out accurately and precisely from a single sample rather than a large collection of samples. The Hi-C protocol was designed to capture genome-wide evidence of DNA molecules in close physical proximity in vivo , prior to cellular lysis. With the flexible nature of a chromosome and its capacity to bend back on itself, the most frequently observed interactions are usually those between nearby loci on the same chromosome (intra-chromosomal). After this, the next most frequent interactions are between chromosomes within the same cell (inter-chromosomal). The least common are inter-cellular interactions and are often seen at rates far below those found within a cell. The original purpose of Hi-C was to study the 3-dimensional conformation of the human genome [1], but this captured proximity information can be readily applied to improve draft genome and metagenome assemblies. In metagenomics we can use the proximity information to infer which assembly fragments (contigs) originated from the same cell (or chromosome). Though the power of the \"proximity signal\" should provide a level of precision down to individual chromosomes, metagenomic analysis tools currently aim to associate assembly fragments at the genome level into MAGs (as described in the earlier tutorial section on genome binning).","title":"What is Metagenomic Hi-C?"},{"location":"hic/#the-hi-c-library-protocol","text":"Major steps of the Hi-C protocol","title":"The Hi-C Library Protocol"},{"location":"hic/#basic-concept","text":"Though Hi-C read-sets are generated using conventional high-throughput Illumina paired-end sequencing, the more complicated Hi-C protocol produces a very different library. Unlike traditional shotgun sequencing, where read pairs originate from nearby regions on the same contiguous DNA molecule (i.e. chromosome, plasmid, etc), the Hi-C protocol can generate read-pairs from any two strands of DNA that were in close physical proximity within the cell. So long as the two DNA loci interact, regardless of their location with the genome, there is a chance of producing a Hi-C read-pair from that interaction. Hi-C read pairs can therefore originate from far-away regions of the same molecule or from entirely different molecules. To achieve this, the Hi-C library protocol involves several steps upstream of Illumina adapter ligation and is more challenging to execute in the lab. As a result, commercial kits have been developed which can simplify the process and improve quality and consistency of the resulting libraries.","title":"Basic Concept"},{"location":"hic/#protocol-outline","text":"DNA Fixation: Beginning with intact cells, the first step of the Hi-C protocol is formalin fixation. The act of cross-linking \"locks in\" close-by conformation arrangements within the DNA that existed in the cells at the time of fixation. Cell Lysis: The cells are lysed and DNA-protein complexes extracted and purified. Restriction Digest: The DNA is digested using a restriction endonuclease. In metagenomic Hi-C, enzymes with large overhangs and 4-nt recognition sites are common choices (i.e. Sau3AI, MluCI, DpnII). Differences in the GC bias of the recognition site and the target DNA is an important factor, as inefficient digestion will produce fewer Hi-C read-pairs. Biotin Tagging: The overhangs produced during digestion are end-filled with biotinylated nucleotides. Free-end Ligation: in dilute conditions or immobilised on a substrate, free-ends protruding from the DNA-protein complexes are ligated. This stochastic process favours any two ends which were nearby within the complex, however random ligation and self-ligation can occur and the reads resulting from such events create noise/error in the data. Crosslink Reversal: The bonds created by formalin fixation are removed, allowing the now free DNA to be purified. Proteinase digestion is common following this step. Un-ligated End Clean-up: Free-ends which failed to form ligation products are unwanted in subsequent steps but could still be biotin tagged. To minimise their inclusion, a light exonuclease 3' to 5' favoured chew-back can be applied. DNA Shearing: With ligation completed, the DNA is mechanically sheared and the size range of the resulting fragment selected suitability with Illumina paired-end sequencing. DNA repair: Sonication can lead to damaged ends and nicks, which can be repaired. Proximity Ligation Enrichment: Biotin tagged fragments are pulled down using affinity purification. Adapter Ligation: Illumina paired-end adapter ligation and associated steps to produce a sequencing library are now applied.","title":"Protocol Outline"},{"location":"hic/#quality-control-is-my-library-ok","text":"Due to the highly variable nature of environmental microbe samples, the possible presence of enzyme inhibitors, and other reasons, the metagenomic Hi-C protocol may not always work with high efficiency, or at all. Problems with sample processing can sometimes be identified prior to sequencing, for example if the measured library yield is very low. But other problems may be difficult to diagnose prior to sequencing, for example, it is very difficult to estimate the efficiency of proximity ligation and thus the fraction of sequencing reads that will be Hi-C reads without actually sequencing the sample. The reads that are not Hi-C reads are essentially conventional shotgun read-pairs, which in the context of Hi-C are uninformative. The value of the proximity information contained within a Hi-C read-set is high enough that even with a low efficiency library, it can be worthwhile to compensate for low efficiency by sequencing more deeply to obtain more Hi-C read pairs. Thus, determining the percentage of Hi-C read-pairs in the library is important, because it can tell us how deeply we need to sequence our library. Rather than submit a library to a large and costly sequencing run immediately, a small pilot sequencing run can be sufficient to confidently estimate the Hi-C efficiency. Armed with this information, a researcher can make informed decisions about further action; such as whether a candidate Hi-C library should be fully sequenced and to what depth. We now discuss various ways to estimate the fraction of Hi-C reads in a library.","title":"Quality Control: is my library ok?"},{"location":"hic/#evidence-of-proximity-ligation","text":"","title":"Evidence of Proximity Ligation"},{"location":"hic/#long-range-pairs","text":"The separation distance for intra-chromosomal Hi-C read-pairs is bounded only by the length of the chromosome. This is unlike shotgun read-pairs whose separation is rarely more than a thousand nucleotides due to chemistry limitations on Illumina instruments. Though it is less direct than looking for evidence of cut-site duplication events, a simple count of the number of read pairs than map far apart can be used to infer the percentage of Hi-C read-pairs. The main limitation of this approach is that it requires a high quality reference assembly against which the Hi-C read pairs can be mapped. In many metagenomic projects, such an assembly is not actually available, either because a good quality shotgun metagenome dataset has not yet been generated for the sample or because there are no close reference genomes in public databases for the organisms in the sample. Counting the number of of pairs which map at long-range can as be used to infer the percentage of Hi-C read-pairs.","title":"Long-range pairs"},{"location":"hic/#proximity-ligation-junctions","text":"In the Hi-C protocol outlined above, the steps of creating free-ends through enzymatic digestion, subsequent end-repair, and finally re-ligation introduces an artifact at the junction point. This artifact is a short sequence duplication and the exact sequence enzyme dependent. The duplication is produced when the cleavage site overhangs are subjected to end-repair. When the repaired blunt ends are subsequently ligated, their junction contains two copies of the overhang sequence. Proximity junction for the enzyme Sau3AI Sau3AI is a 4-cutter with a 4nt overhang. Native DNA 5`-XXXXGATCYYYY-3' 3'-xxxxCTAGyyyy-5' Free-ends post cleavage have overhangs 5`-XXXX GATCYYYY-3' 3'-xxxxCTAG yyyy-5' Blunt ends after end-repair with biotinylated nucleotides 5`-XXXXGATC GATCYYYY-3' 3'-xxxxCTAG CTAGyyyy-5' Free-end ligation produces a duplication at the junction 5`-XXXXGATCGATCYYYY-3' 3'-xxxxCTAGCTAGyyyy-5' After the steps of DNA shearing and size selection, fragments which eventually go on to DNA sequencing can contain a junction at any point along their extent. An Illumina paired-end sequencing run, which generates reads at either end of a fragment, thus has two chances to read through the junction. This approach has two main drawbacks: If the fragments in the sequencing library are long relative to the read length, there may be junctions within the fragment that remain unobserved, and this fact must be corrected for in the estimate. If the sample was degraded there may have been free ends that were not created by an enzyme cut. These could become proximity-ligated, and therefore ligation junctions may exist that do not contain the obvious junction sequence. Searching the read-set for examples of this junction sequence is one means of measuring the percentage of Hi-C read-pairs in a given library.","title":"Proximity ligation junctions"},{"location":"hic/#using-a-qc-tool","text":"Though the methodology of Hi-C QC has yet to achieve standardisation, a number of tools exist which offer forms of QC testing. These tools can be found as components of Hi-C analysis pipelines ( HiCpipe ), embedded in tool-suites ( HiCExplorer ) and as stand-alone command-line tools ( hic_qc , qc3C ) [2-5]. In this tutorial we will use qc3C to assess Hi-C library quality in two different ways: Read mapping based analysis k -mer based analysis.","title":"Using a QC Tool"},{"location":"hic/#get-some-hi-c-data-and-tools","text":"For a single timepoint, we'll first download a small Hi-C read-set and its associated shotgun read-set. Also, we'll pull down the Docker images for qc3C (quality testing) and bin3C (metagenome binning) Get some Hi-C data and the qc3C and bin3C docker images # download the read-set parallel-fastq-dump -s SRR9323809 -s SRR8960211 --threads 4 --outdir hic_data/ --minSpotId 0 --maxSpotId 1000000 --split-files --gzip # fetch the qc3C image sudo docker pull cerebis/qc3c:alpine # fetch the bin3c image sudo docker pull cerebis/bin3c:latest Make sure you have parallel-fastq-dump If you've skipped the first section on Sequencing run QC, please do this first. conda install -c bioconda parallel-fastq-dump","title":"Get some Hi-C data and tools"},{"location":"hic/#create-a-metagenome-assembly-and-map-hi-c-reads","text":"The shotgun dataset has been limited to 1M read-pairs, this is only 1/165 th of the total. Obviously this reduction in total coverage will lead to a much sparser sampling of the community and a more fragmented assembly. Nevertheless, it is enough data to demonstrate the concept of QC and Hi-C metagenome binning. Lets assemble the shotgun data and map both library types to the resulting contigs. Create a SPAdes assembly # enter the hic data directory cd hic_data # launch spades in metagenomic mode sudo docker run -v $PWD :/opt/app-root cerebis/bin3c:latest spades.py --meta -1 SRR8960211_1.fastq.gz -2 SRR8960211_2.fastq.gz -o asm # index the contig fasta sudo docker run -v $PWD :/opt/app-root cerebis/bin3c:latest bwa index asm/contigs.fasta # map hi-c reads to the contigs sudo docker run -v $PWD :/opt/app-root cerebis/bin3c:latest /bin/bash -c \"bwa mem -t4 -5SP asm/contigs.fasta SRR9323809_1.fastq.gz SRR9323809_2.fastq.gz | samtools view -uS -F 0x904 - | samtools sort -@4 -n -o hic_to_ctg.bam -\" # map shotgun reads to the contigs sudo docker run -v $PWD :/opt/app-root cerebis/bin3c:latest /bin/bash -c \"bwa mem -t4 -5SP asm/contigs.fasta SRR8960211_1.fastq.gz SRR8960211_2.fastq.gz | samtools view -uS -F 0x904 - | samtools sort -@4 -n -o wgs_to_ctg.bam -\" # return to your home directory cd Breaking down the read mapping command We are providing Docker access to our host filesystem using a bind mount ( -v $PWD:/opt/app-root ) Next we are specifying the image to run ( cerebis/bin3c:latest ). Finally we include the actual call, which involves pipes between bwa and samtools . To achieve this we pass the bash shell our complex command, which it then executes within the container.","title":"Create a metagenome assembly and map Hi-C reads"},{"location":"hic/#bam-mode-analysis-with-qc3c","text":"We begin with the conceptually simpler approach to Hi-C QC, counting the number of long-range read-pairs from a Hi-C read-set. As a rule of thumb, to work well with Illumina sequencing, the physical size of fragments is likely to be <1000nt. We shall therefore simply count the number of pairs which map with a separation >1000, >5000 and >10,000 nt. As the mapped read-pair separation grows beyond 1000nt we expect an increasingly large proportion of these pairs will be due to Hi-C proximity ligation. Conversely, when analysing a shotgun read-set, we expect to see very few. Run a BAM based analysis # enter the hic data folder cd hic_data # perform bam based QC analysis on Hi-C data sudo docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine ash -c \"qc3C bam -m 400 -e Sau3AI -e MluCI -b hic_to_ctg.bam\" # now perform the analysis on shotgun data sudo docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine ash -c \"qc3C bam -m 400 -e Sau3AI -e MluCI -b wgs_to_ctg.bam\" # return to your home directory cd","title":"BAM mode analysis with qc3C"},{"location":"hic/#estimation-of-fragment-size","text":"As we are not sure of the mean insert length, we guess it to be 400nt. This is one handy benefit of the BAM mode approach, we can infer a reasonably good estimate of the fragment (insert) length. In k -mer mode, having an accurate value becomes important if we wish to estimate the probable fraction of unobserved proximity junctions. That is to say, for long fragments and short reads, much of each fragment goes unsequenced and so we will miss the evidence of junctions. In our resulting log from above, qc3C reports an observed mean fragment length of 445nt for the Hi-C read-set. INFO | 2019-06-23 18:55:18,652 | qc3C.bam_based | Observed mean of short-range pair separation: 445nt","title":"Estimation of fragment size"},{"location":"hic/#looking-at-the-results","text":"The last three lines of the output from qc3C report the absolute number and relative fraction of read-pairs that mapped far apart. For the Hi-C library, the fraction of pairs separated by more than 10kb was ~2.7%, while in contrast for the known shotgun library the fraction was <0.006%. Hi-C result INFO | 2019-06-23 18:55:18,654 | qc3C.bam_based | Long-range distance intervals: 1000nt, 5000nt, 10000nt INFO | 2019-06-23 18:55:18,654 | qc3C.bam_based | Number of observed pairs: 94, 46, 23 INFO | 2019-06-23 18:55:18,654 | qc3C.bam_based | Relative fraction of all obs: 0.1116, 0.05463, 0.02732 Shotgun result INFO | 2019-06-23 18:56:13,330 | qc3C.bam_based | Long-range distance intervals: 1000nt, 5000nt, 10000nt INFO | 2019-06-23 18:56:13,331 | qc3C.bam_based | Number of observed pairs: 284, 43, 23 INFO | 2019-06-23 18:56:13,331 | qc3C.bam_based | Relative fraction of all obs: 0.0007268, 0.0001101, 5.886e-05 It is important to keep in mind that as the assembly was made from a shotgun dataset downsampled to very low coverage, the length of contigs is significantly lower than would have been obtained with the full dataset. This will impair the odds of seeing pairs which map far away. Despite this, the 97% of pairs in the shotgun library map to the same contig sequence. In contrast, 72% of Hi-C pairs mapped to the same contig. Overall, for BAM mode, the reliability of results is connected to the quality and completeness of the available references. Breaking down the invocation of qc3C We are providing Docker access to our host filesystem using a bind mount ( -v $PWD:/opt/app-root ) Next we are specifying the image to run ( cerebis/qc3c:alpine ). Finally we include the actual call to qc3C , along with its own options. Potential Concerns Requires a reference sequence: a completed genome a draft assembly. May not be an option when: no reference is available only a very highly fragmented draft exists Computational requirements: create the shotgun assembly map Hi-C reads to the reference The distribution of assembly contig lengths will affect the maximum mapping distance.","title":"Looking at the results"},{"location":"hic/#k-mer-mode-analysis-with-qc3c","text":"To analyze our small Hi-C read-set using a k -mer approach, we will first create a k -mer library using Jellyfish. Since this Hi-C experiment used the 4-cutter Sau3AI, which produces an 8-mer junction, we'll use a mer size of 24 for the library. This will give us 8 nucleotides either side of any prospective junction, for specificity. Choosing how large a k -mer size to use for the library is a trade-off between computational complexity and minimising false discovery. You can try sizes bigger and smaller. Running k-mer based analyses On the Hi-C data # enter the hic data folder cd hic_data # create a generator used by Jellyfish for compressed files rm -f gen_fq && for fn in ` ls SRR9323809*fastq.gz ` ; do echo gzip -dc $fn >> gen_fq ; done # use jellyfish to create a 24-mer library from FastQ reads sudo docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine jellyfish count -t 4 -m 24 -s 100M -C -g gen_fq -o hic24.jf # perform k-mer based QC analysis sudo docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine ash -c \"qc3C kmer -m 445 -s 1234 -e Sau3AI -l hic24.jf -r SRR9323809_1.fastq.gz -r SRR9323809_2.fastq.gz\" # return back to your home directory cd Now on the shotgun data # enter the hic data folder cd hic_data # create a generator used by Jellyfish for compressed files echo \"gzip -dc SRR8960211_1.fastq.gz\" > gen_sg # use jellyfish to create a 24-mer library from FastQ reads sudo docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine jellyfish count -t 4 -m 24 -s 100M -C -g gen_sg -o wgs24.jf # perform k-mer based QC analysis sudo docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine ash -c \"qc3C kmer -m 306 -s 1234 -e Sau3AI -l wgs24.jf -r SRR8960211_1.fastq.gz -p 0.05\" # return back to your home directory cd","title":"k-mer mode analysis with qc3C"},{"location":"hic/#looking-at-the-results_1","text":"Looking at the inferred fraction of true Hi-C read-pairs, we can immediately see that the low coverage shotgun assembly appears to have drastically skewed the estimate of signal (2.7% vs 6.9 \u00b1 2.4%). After adjustment for the unobserved fraction (0.32), the signal estimate increases to 9.1 \u00b1 3.1%. In comparison, for the shotgun data the inferred fraction of Hi-C read-pairs is 0.22 \u00b1 9.4%, with an unobserved fraction of just 0.013. This value of this estimate is dubious at best. If we look further up the log we can see that the number of observed junctions was 0.098%, while by random chance this would be 0.0015%. Although this appears greater than by chance and possibly significant, this is put clear in perspective when compared to the Hi-C result of 5.9%. Therefore, the Hi-C library appears to have a reasonable fraction of Hi-C reads. As a pilot study, we should probably send this to be more deeply sequenced. Hi-C result INFO | 2019-06-23 19:45:13,826 | qc3C.kmer_based | For supplied insert length of 445nt, estimated unobserved fraction: 0.3213 INFO | 2019-06-23 19:45:13,826 | qc3C.kmer_based | Adjusted estimation of Hi-C read fraction: 9.135 \u00b1 3.148 % INFO | 2019-06-23 19:30:03,640 | qc3C.kmer_based | Estimated Hi-C read fraction via p-value sum method: 7.211 \u00b1 2.756 % Shotgun result INFO | 2019-06-23 19:47:10,690 | qc3C.kmer_based | Estimated Hi-C read fraction via p-value sum method: 0.2242 \u00b1 9.365 % INFO | 2019-06-23 19:47:10,690 | qc3C.kmer_based | For supplied insert length of 306nt, estimated unobserved fraction: 0.01307 INFO | 2019-06-23 19:47:10,690 | qc3C.kmer_based | Adjusted estimation of Hi-C read fraction: 0.2272 \u00b1 9.487 % Breaking down the invocation of jellyfish We are providing Docker access to our host filesystem using a bind mount ( -v $PWD:/opt/app-root ) Next we are specifying the image to run ( cerebis/qc3c:alpine ). Finally we include the actual call to jellyfish , along with its own options. K-mer mode requirements No-assembly or reference required Requires making a k-mer library from FastQ reads Analyse FastQ reads against library","title":"Looking at the results"},{"location":"hic/#metagenome-binning-with-hi-c-data","text":"As we saw by processing timeseries data with MetaBAT2 , metagenome binning can also be performed using Hi-C. The primary difference here is that accurate and precise binning is possible from a single timepoint, so long as you have you have both a shotgun and Hi-C read-set. Currenty, only a few tools exist which were made specifically for this purpose ( bin3C , ProxiMeta ) [6,7]. In this tutorial we will use bin3C, as it is open-source and easy to use. With a set of reference sequences and Hi-C to reference BAM file in hand (from above), a bin3C analysis has two stages: first create the contact map, second cluster the contact map. We also need to know what enzyme(s) was used in generating the library. It is becoming more common to generate libraries from two enzymes with differing GC biases to improve Hi-C coverage within a community. In such cases, we simply specify them when creating the map, as here. Perform Hi-C metagenome binning # enter hic data directory cd hic_data # Make contact map sudo docker run -v $PWD :/opt/app-root cerebis/bin3c:latest bin3C -v mkmap -e Sau3AI -e MluCI --eta --bin-size 5000 asm/contigs.fasta hic_to_ctg.bam bin3C_out # Cluster contact map sudo docker run -v $PWD :/opt/app-root cerebis/bin3c:latest bin3C -v cluster --clobber --only-large bin3C_out/contact_map.p.gz bin3C_out # return to your home directory cd","title":"Metagenome Binning with Hi-C Data"},{"location":"hic/#looking-at-the-results_2","text":"After bin3C completes the clustering step, the three outputs which are possibly the most interesting are: The comma delimited cluster_report.csv table the cluster_report details characteristics of each identified cluster. An id, name, number of contigs, total extent, mean GC and coverage, etc. Mean coverage is a reasonable gauge of relative abundance A qualitative overview of the result in the form of a heatmap cluster_plot.png contigs are organized by cluster, followed by descending length a pixel represents a 5kb bin and therefore contigs are represented proportional to their length. Clustered sequence data the fasta/ directory contains cluster FastA data The cluster plot is a visual representation of the clustered contact map. As a metagenomic assembly can easily contain 50k to 100k contigs, which as a NxN image would be very large, the plot's resolution is constrained by default to 4000x4000 pixels to conserve memory. Once organised, a good Hi-C read-set will have produced a map that is clearly in block-diagonal form with high contrast. Also, a significant proportion of the map's field is should be dark, reflecting the fact that inter-cellular interactions are rare. In cases where there was insufficient signal, there will only be a few (if any) large clusters and they will look sparse internally. Example cluster plot of a human microbome MAG clusters of a single-timepoint human faecal microbiome (PRJNA413092) as determined by bin3C . The shotgun read-set consisted of 249M pairs, while the Hi-C library contained 41M pairs (~10% Hi-C). Of the identified MAGs, 55 where >90% complete, <5% contamination as estimated by CheckM [6]. If you are familiar with the appearance of a single-genome microbial contact map, at sufficient zoom you may notice that the individual clusters do not look the same. This is due to the fact that the proper ordering of contigs (scaffolding) within the clusters has not been established. After reaching this point, standard downstream analyses would be to profile the taxonomic composition of the clusters with tools such as CheckM . Expectations from Hi-C binning Hi-C metagenome binning is dependent on both the quality of the shotgun assembly and the depth of true Hi-C read-pairs. For a Hi-C data-set with strong signal (perhaps 10%) and reasonable depth of sequencing (100M pairs), it is reasonable to expect 50-100 nearly complete MAGs [6].","title":"Looking at the results"},{"location":"hic/#references","text":"Lieberman-Aiden, E., van Berkum, N. L., Williams, L., Imakaev, M., Ragoczy, T., Telling, A., \u2026 Dekker, J. (2009). Comprehensive mapping of long-range interactions reveals folding principles of the human genome. Science , 326(5950), 289\u2013293. https://doi.org/10.1126/science.1181369 fchen. (n.d.). HiCpipe. Retrieved from https://github.com/ChenFengling/HiCpipe Wolff, J., Bhardwaj, V., Nothjunge, S., Richard, G., Renschler, G., Gilsbach, R., \u2026 Gr\u00fcning, B. A. (2018). Galaxy HiCExplorer: a web server for reproducible Hi-C data analysis, quality control and visualization. Nucleic Acids Research , 46(W1), W11\u2013W16. https://doi.org/10.1093/nar/gky504 DeMaere, M. Z., Darling A. E. (n.d.). qc3C. Retrieved from https://github.com/cerebis/qc3C hic_qc. (n.d.). Retrieved from https://github.com/phasegenomics/hic_qc DeMaere, M. Z., & Darling, A. E. (2019). bin3C: exploiting Hi-C sequencing data to accurately resolve metagenome-assembled genomes. Genome Biology , 20(1), 46. https://doi.org/10.1186/s13059-019-1643-1 Press, M. O., Wiser, A. H., Kronenberg, Z. N., Langford, K. W., Shakya, M., Lo, C.-C., \u2026 Liachko, I. (2017). Hi-C deconvolution of a human gut microbiome yields high-quality draft genomes and reveals plasmid-genome interactions (p. 198713). https://doi.org/10.1101/198713","title":"References"},{"location":"qc/","text":"Sequencing run QC \u00b6 Get some sequence data \u00b6 The first thing we'll do is to get some sequence data to work with. If you are working on a new sequencing project the data might come from a sequencing facility. For this tutorial we'll work with data that is available in public databases. Published sequence data is usually archived in the NCBI SRA, the ENA, and the DDBJ. These databases provide a convenient interface to search the descriptions of samples by keyword, and by sample type (e.g. shotgun metagenome). For the following exercises we'll use a set of small datasets (SRA accessions SRR9323808, SRR9323810, SRR9323811, SRR9323809) which will be quick to process because they are small. The easiest way to download data from these databases is via the fastq-dump software. First, let's install the parallel version of fastq-dump using conda. To do this, start a terminal session in your Jupyter server (click the Terminal icon) and run the following command (ok to copy and paste): conda install -c bioconda parallel-fastq-dump Now that you've installed fastq-dump we can use it to download data by accession number. Copy and paste the following to your terminal: parallel-fastq-dump -s SRR9323808 -s SRR9323810 -s SRR9323811 -s SRR9323809 --threads 4 --outdir qc_data/ --split-files --gzip If the download was successful you should see something like the following on your terminal: Evaluating sequence quality with FastQC and MultiQC \u00b6 The very first thing one would normally do when working with a new dataset is to look at some basic quality statistics on the data. There are many, many tools available to compute quality metrics. For our current data we will use the FastQC software, applied to each sample, and then combine the results using MultiQC to a single report. First step is to install fastqc and multiqc . conda install -c bioconda fastqc pip install multiqc In the above we've used conda to install fastqc, but we've used another way to install multiqc -- something called pip . pip is an installer for python programs, and like conda it will download and install the software along with any dependencies. The reason we use pip in this case is because conda can be very, very slow to install some programs and in this case pip is much faster. cd qc_data find . -name \"*.fastq.gz\" -exec fastqc {} \\; Let's unpack those commands a bit. The first part, cd qc_data just changes the current directory to qc_data, so any following commands run will run in that directory. The next command is find . -name \"*.fastq.gz\" -exec fastqc {} \\; . The first part, find is a command that finds files. If you just run find . it will find all the files in and below the current directory (the . part specifies to look in the current directory). The next part, -name \"*.fastq.gz\" tells find that we only want it to find files with names that end with .fastq.gz . The * is a wildcard that matches anything. Finally, the last part -exec fastqc {} \\; tells find that whenever it finds a file, it should run fastqc on that file, putting the name of the file where the {} are. If this step has worked, then you should have several new .zip files containing the QC data in that directory, along with some html files. When we have a lot of samples it is too tedious to look at all the QC reports individually for each sample, so we can summarize them using multiqc: multiqc . At this point a multiqc file will appear inside the QC directory. First double click to open the QC folder. Once that's open a file called multiqc_report.html will appear in the listing. We can open this file from within our jupyter browser environment and inspect it. To open it, we need to right click (or two-finger tap) on the file name to get a context menu that will give several options for how to open it. It looks like this: Click the option to \"Open in a New Browser Tab\". From here we can evaluate the quality of the libraries. Taxonomic analysis \u00b6 Metagenome taxonomic analysis offers a means to estimate a microbial community profile from metagenomic sequence data. It can give us a very high-level, rough idea of what kinds of microbes are present in a sample. It can also give an idea of how complex/diverse the microbial community is -- whether there are many species or few. It is useful as an initial quality check to ensure that the microbial community composition looks roughly as expected, and to confirm that nothing obvious went wrong during the sample collection and sequencing steps. Taxonomic analysis with Metaphlan2 \u00b6 While it may be possible to install metaphlan2 via conda, at least in my experience, conda struggles with \"solving the environment\". Therefore it's suggested to install it via the simple download method described on the metaphlan tutorial page : cd ; wget -c -O metaphlan.tar.bz2 https://bitbucket.org/nsegata/metaphlan/get/default.tar.bz2 tar xvjf metaphlan.tar.bz2 mv nsegata-metaphlan* metaphlan Once metaphlan has been downloaded we can run it on our QC samples: cd ~/qc_data pig_samples=\"SRR9323808 SRR9323810 SRR9323811 SRR9323809\" for s in ${pig_samples} do zcat ${s}*.fastq.gz | python2 ~/metaphlan/metaphlan.py --input_type multifastq --bt2_ps very-sensitive --bowtie2db ~/metaphlan/bowtie2db/mpa --bowtie2out ${s}.bt2out -o ${s}.mph2 done The above series of commands creates a \"for loop\" where each iteration processes one of our metagenome samples. This is a convenient way to process many samples without having to type out the commands for each sample. The sample names get stored in a variable $pig_samples and at each loop iteration, one of the sample names is placed into the loop variable ${s} , where it can get used in the command inside the loop. Finally we can plot the taxonomic profile of the samples: python2 ~/metaphlan/utils/merge_metaphlan_tables.py *.mph2 > pig_mph2.merged python2 ~/metaphlan/plotting_scripts/metaphlan_hclust_heatmap.py -c bbcry --top 25 --minv 0.1 -s log --in pig_mph2.merged --out mph2_heatmap.png Once that has completed successfully, a new file called mph2_heatmap.png will appear in the qc_data folder of our Jupyter file browser. We can double click it to view. There are other ways to visualize the data, and they are described in the graphlan section of the metaphlan tutorial page. Taxonomic analysis with other tools \u00b6 There are a whole range of other software tools available for metagenome taxonomic analysis. They all have strengths and weaknesses. A few other commonly used tools are listed here: kraken2 MEGAN Centrifuge CLARK Evaluating the host genomic content \u00b6 In many applications of metagenomics we are working with host-associated samples. The samples might have come from an animal gut, mouth, or other surface. Similarly for plants we might be working with leaf or root surfaces or rhizobia. When such samples are collected the resulting DNA extracts can include a significant fraction of host material. Let's have a look at what this looks like in data. To do so, we'll download another set of pig gut samples: a set of samples that was taken using scrapings or swabs from different parts of the porcine digestive system, including the duodenum, jejunum, ileum, colon, and caecum. Rather than using metaphlan2 to profile these, we will use the kraken2 classifier in conjunction with the bracken tool for estimating relative abundances. We first need to install kraken2 and bracken: conda install -c bioconda kraken2 bracken Next, we need to get a kraken2 database. For this tutorial we will simply use a precomputed kraken2 database but note the very important limitation that the only non-microbial genome it includes is the human genome. If you would like to evaluate host genomic content on plants or other things you should follow the instructions on the kraken2 web site to build a complete database. We can download and unpack the kraken2 database with: cd ; wget -c ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/minikraken2_v2_8GB_201904_UPDATE.tgz tar xvzf minikraken2_v2_8GB_201904_UPDATE.tgz Finally we are ready to profile our samples with kraken2 and bracken. We'll first download the sample set with parallel-fastq-dump and then run kraken2 and bracken's est_abundance.py script on each sample using a bash for loop. The analysis can be run as follows: parallel-fastq-dump -s SRR9332442 -s SRR9332438 -s SRR9332439 -s SRR9332443 -s SRR9332440 --threads 4 --outdir host_qc/ --split-files --gzip cd host_qc samples=\"SRR9332442 SRR9332438 SRR9332439 SRR9332443 SRR9332440\" for s in $samples; do kraken2 --paired ${s}_1.fastq.gz ${s}_2.fastq.gz --db ../minikraken2_v2_8GB_201904_UPDATE/ --report ${s}.kreport > ${s}.kraken; done for s in $samples; do est_abundance.py -i ${s}.kreport -k ../minikraken2_v2_8GB_201904_UPDATE/database150mers.kmer_distrib -o ${s}.bracken; done Once the above has completed, navigate over to the host_qc folder in the Jupyter file browser and click on the *.bracken files to open them. What do you see? In particular, why does sample SRR9332438 look so different to sample SRR9332440? Keep in mind the isolation sources of the samples were as follows: Sample Source SRR9332442 Duodenum SRR9332440 Caecum SRR9332439 Ileum SRR9332438 Jejunum SRR9332443 Colon Some challenge questions \u00b6 If we sequenced samples from pigs, why is human DNA being predicted in these samples? If we were to design a large study around these samples which of them would be suitable for metagenomics, and why? How much sequencing data would we need to generate from sample SRR9332440 to reconstruct the genome of the Bifidobacterium in that sample? What about the E. coli ? Are there really six species of Lactobacillus present in SRR9332440? Go to the NCBI SRA search tool and find a metagenome of interest to you. Download the first 100000 reads from it (use the --maxSpotId parameter) and analyse it with kraken2. Is it what you expected? How does it compare to the others we've looked at? A note on negative controls \u00b6 Negative controls are a key element in any microbiome profiling or metagenome analysis work. Every aspect of the sample collection and processing can be impacted by the presence of contaminating microbial DNA. This is true even of 'sterile' material -- just because no viable cells exist in a sample collection swab, for example, does not mean there is no microbial DNA on that swab. It is well known that molecular biology reagents frequently contain contaminating DNA. Usually it is at low levels, but this is not always the case. Therefore, the best practice is to collect multiple negative control samples that are taken all the way through sequencing. These negative controls can then be used to correct for contamination artifacts in the remaining samples.","title":"QC"},{"location":"qc/#sequencing-run-qc","text":"","title":"Sequencing run QC"},{"location":"qc/#get-some-sequence-data","text":"The first thing we'll do is to get some sequence data to work with. If you are working on a new sequencing project the data might come from a sequencing facility. For this tutorial we'll work with data that is available in public databases. Published sequence data is usually archived in the NCBI SRA, the ENA, and the DDBJ. These databases provide a convenient interface to search the descriptions of samples by keyword, and by sample type (e.g. shotgun metagenome). For the following exercises we'll use a set of small datasets (SRA accessions SRR9323808, SRR9323810, SRR9323811, SRR9323809) which will be quick to process because they are small. The easiest way to download data from these databases is via the fastq-dump software. First, let's install the parallel version of fastq-dump using conda. To do this, start a terminal session in your Jupyter server (click the Terminal icon) and run the following command (ok to copy and paste): conda install -c bioconda parallel-fastq-dump Now that you've installed fastq-dump we can use it to download data by accession number. Copy and paste the following to your terminal: parallel-fastq-dump -s SRR9323808 -s SRR9323810 -s SRR9323811 -s SRR9323809 --threads 4 --outdir qc_data/ --split-files --gzip If the download was successful you should see something like the following on your terminal:","title":"Get some sequence data"},{"location":"qc/#evaluating-sequence-quality-with-fastqc-and-multiqc","text":"The very first thing one would normally do when working with a new dataset is to look at some basic quality statistics on the data. There are many, many tools available to compute quality metrics. For our current data we will use the FastQC software, applied to each sample, and then combine the results using MultiQC to a single report. First step is to install fastqc and multiqc . conda install -c bioconda fastqc pip install multiqc In the above we've used conda to install fastqc, but we've used another way to install multiqc -- something called pip . pip is an installer for python programs, and like conda it will download and install the software along with any dependencies. The reason we use pip in this case is because conda can be very, very slow to install some programs and in this case pip is much faster. cd qc_data find . -name \"*.fastq.gz\" -exec fastqc {} \\; Let's unpack those commands a bit. The first part, cd qc_data just changes the current directory to qc_data, so any following commands run will run in that directory. The next command is find . -name \"*.fastq.gz\" -exec fastqc {} \\; . The first part, find is a command that finds files. If you just run find . it will find all the files in and below the current directory (the . part specifies to look in the current directory). The next part, -name \"*.fastq.gz\" tells find that we only want it to find files with names that end with .fastq.gz . The * is a wildcard that matches anything. Finally, the last part -exec fastqc {} \\; tells find that whenever it finds a file, it should run fastqc on that file, putting the name of the file where the {} are. If this step has worked, then you should have several new .zip files containing the QC data in that directory, along with some html files. When we have a lot of samples it is too tedious to look at all the QC reports individually for each sample, so we can summarize them using multiqc: multiqc . At this point a multiqc file will appear inside the QC directory. First double click to open the QC folder. Once that's open a file called multiqc_report.html will appear in the listing. We can open this file from within our jupyter browser environment and inspect it. To open it, we need to right click (or two-finger tap) on the file name to get a context menu that will give several options for how to open it. It looks like this: Click the option to \"Open in a New Browser Tab\". From here we can evaluate the quality of the libraries.","title":"Evaluating sequence quality with FastQC and MultiQC"},{"location":"qc/#taxonomic-analysis","text":"Metagenome taxonomic analysis offers a means to estimate a microbial community profile from metagenomic sequence data. It can give us a very high-level, rough idea of what kinds of microbes are present in a sample. It can also give an idea of how complex/diverse the microbial community is -- whether there are many species or few. It is useful as an initial quality check to ensure that the microbial community composition looks roughly as expected, and to confirm that nothing obvious went wrong during the sample collection and sequencing steps.","title":"Taxonomic analysis"},{"location":"qc/#taxonomic-analysis-with-metaphlan2","text":"While it may be possible to install metaphlan2 via conda, at least in my experience, conda struggles with \"solving the environment\". Therefore it's suggested to install it via the simple download method described on the metaphlan tutorial page : cd ; wget -c -O metaphlan.tar.bz2 https://bitbucket.org/nsegata/metaphlan/get/default.tar.bz2 tar xvjf metaphlan.tar.bz2 mv nsegata-metaphlan* metaphlan Once metaphlan has been downloaded we can run it on our QC samples: cd ~/qc_data pig_samples=\"SRR9323808 SRR9323810 SRR9323811 SRR9323809\" for s in ${pig_samples} do zcat ${s}*.fastq.gz | python2 ~/metaphlan/metaphlan.py --input_type multifastq --bt2_ps very-sensitive --bowtie2db ~/metaphlan/bowtie2db/mpa --bowtie2out ${s}.bt2out -o ${s}.mph2 done The above series of commands creates a \"for loop\" where each iteration processes one of our metagenome samples. This is a convenient way to process many samples without having to type out the commands for each sample. The sample names get stored in a variable $pig_samples and at each loop iteration, one of the sample names is placed into the loop variable ${s} , where it can get used in the command inside the loop. Finally we can plot the taxonomic profile of the samples: python2 ~/metaphlan/utils/merge_metaphlan_tables.py *.mph2 > pig_mph2.merged python2 ~/metaphlan/plotting_scripts/metaphlan_hclust_heatmap.py -c bbcry --top 25 --minv 0.1 -s log --in pig_mph2.merged --out mph2_heatmap.png Once that has completed successfully, a new file called mph2_heatmap.png will appear in the qc_data folder of our Jupyter file browser. We can double click it to view. There are other ways to visualize the data, and they are described in the graphlan section of the metaphlan tutorial page.","title":"Taxonomic analysis with Metaphlan2"},{"location":"qc/#taxonomic-analysis-with-other-tools","text":"There are a whole range of other software tools available for metagenome taxonomic analysis. They all have strengths and weaknesses. A few other commonly used tools are listed here: kraken2 MEGAN Centrifuge CLARK","title":"Taxonomic analysis with other tools"},{"location":"qc/#evaluating-the-host-genomic-content","text":"In many applications of metagenomics we are working with host-associated samples. The samples might have come from an animal gut, mouth, or other surface. Similarly for plants we might be working with leaf or root surfaces or rhizobia. When such samples are collected the resulting DNA extracts can include a significant fraction of host material. Let's have a look at what this looks like in data. To do so, we'll download another set of pig gut samples: a set of samples that was taken using scrapings or swabs from different parts of the porcine digestive system, including the duodenum, jejunum, ileum, colon, and caecum. Rather than using metaphlan2 to profile these, we will use the kraken2 classifier in conjunction with the bracken tool for estimating relative abundances. We first need to install kraken2 and bracken: conda install -c bioconda kraken2 bracken Next, we need to get a kraken2 database. For this tutorial we will simply use a precomputed kraken2 database but note the very important limitation that the only non-microbial genome it includes is the human genome. If you would like to evaluate host genomic content on plants or other things you should follow the instructions on the kraken2 web site to build a complete database. We can download and unpack the kraken2 database with: cd ; wget -c ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/minikraken2_v2_8GB_201904_UPDATE.tgz tar xvzf minikraken2_v2_8GB_201904_UPDATE.tgz Finally we are ready to profile our samples with kraken2 and bracken. We'll first download the sample set with parallel-fastq-dump and then run kraken2 and bracken's est_abundance.py script on each sample using a bash for loop. The analysis can be run as follows: parallel-fastq-dump -s SRR9332442 -s SRR9332438 -s SRR9332439 -s SRR9332443 -s SRR9332440 --threads 4 --outdir host_qc/ --split-files --gzip cd host_qc samples=\"SRR9332442 SRR9332438 SRR9332439 SRR9332443 SRR9332440\" for s in $samples; do kraken2 --paired ${s}_1.fastq.gz ${s}_2.fastq.gz --db ../minikraken2_v2_8GB_201904_UPDATE/ --report ${s}.kreport > ${s}.kraken; done for s in $samples; do est_abundance.py -i ${s}.kreport -k ../minikraken2_v2_8GB_201904_UPDATE/database150mers.kmer_distrib -o ${s}.bracken; done Once the above has completed, navigate over to the host_qc folder in the Jupyter file browser and click on the *.bracken files to open them. What do you see? In particular, why does sample SRR9332438 look so different to sample SRR9332440? Keep in mind the isolation sources of the samples were as follows: Sample Source SRR9332442 Duodenum SRR9332440 Caecum SRR9332439 Ileum SRR9332438 Jejunum SRR9332443 Colon","title":"Evaluating the host genomic content"},{"location":"qc/#some-challenge-questions","text":"If we sequenced samples from pigs, why is human DNA being predicted in these samples? If we were to design a large study around these samples which of them would be suitable for metagenomics, and why? How much sequencing data would we need to generate from sample SRR9332440 to reconstruct the genome of the Bifidobacterium in that sample? What about the E. coli ? Are there really six species of Lactobacillus present in SRR9332440? Go to the NCBI SRA search tool and find a metagenome of interest to you. Download the first 100000 reads from it (use the --maxSpotId parameter) and analyse it with kraken2. Is it what you expected? How does it compare to the others we've looked at?","title":"Some challenge questions"},{"location":"qc/#a-note-on-negative-controls","text":"Negative controls are a key element in any microbiome profiling or metagenome analysis work. Every aspect of the sample collection and processing can be impacted by the presence of contaminating microbial DNA. This is true even of 'sterile' material -- just because no viable cells exist in a sample collection swab, for example, does not mean there is no microbial DNA on that swab. It is well known that molecular biology reagents frequently contain contaminating DNA. Usually it is at low levels, but this is not always the case. Therefore, the best practice is to collect multiple negative control samples that are taken all the way through sequencing. These negative controls can then be used to correct for contamination artifacts in the remaining samples.","title":"A note on negative controls"},{"location":"startup/","text":"Setting up a working environment (Jupyter) \u00b6 In these tutorials we will work in a Jupyter notebook server environment. Jupyter is a versatile web interface system for data analysis, supporting work in languages such as Python and R, as well as the unix command-line. If you are working through this tutorial in one of The Gene School workshops we will provide a pre-configured Jupyter server for you to use and there is no need to carry out the setup process described on this page. Launch a server via Amazon Web Services EC2 \u00b6 First, either log in or sign up for AWS and navigate to to the EC2 section. Then select the option to launch a new instance. We will use the Ubuntu 18.04 LTS base image in this example. As you continue to click through the instance configuration details, there are two important settings that need to be changed from defaults: (1) the disk volume size, which should be set to 500GB or more to give ample space for metagenomic datasets (note that this will incur costs), and (2) the security groups settings. In the security groups, you will need to add access to TCP port 8888 for the IP address from which you'll be connecting to the servers. If you don't know your source IP range then this can be set to 0.0.0.0/0 but note the that this will enable the whole world to connect to the server and there is a security risk, even if the Jupyter server is password or token protected. If this is the first time you've used AWS you will need to generate and save an ssh key that will be used to log into the VMs. Keep this in a safe place. It will need to be made read only to the user account before it can be used with ssh, e.g. chmod 400 my_aws_key.pem Log in and launch Jupyter \u00b6 Having started up an AWS instance you are now ready to log in and install Jupyter. The process is fairly straightforward. First ssh into the instance: ssh -i my_aws_key.pem ubuntu@XX.XX.XX.XX where XX.XX.XX.XX is the public IP address shown in the AWS EC2 console for the VM you have launched. Once logged in there are a few steps to installing the Jupyter server. First download and install anaconda: wget https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh bash Anaconda3-2018.12-Linux-x86_64.sh Don't go too fast here Be sure to answer yes to the license agreement and the question about installing the configuration to the .bashrc file. If not, conda will not be configured properly. Many of the steps in the tutorial involve command-line work, so let's also install the bash kernel: pip install bash_kernel python -m bash_kernel.install Now we're ready to launch a jupyter server: source .bashrc jupyter lab --no-browser --ip 0.0.0.0 --LabApp.token=112233445566778899 IMPORTANT Make sure to replace the string of numbers 112233445566778899 with your own string -- this is the secret key that will allow you (and only you) to connect to your jupyter server and run commands, so you want something that neither human nor robot will guess. Hexadecimal (lowercase) values are ok here too. Connect to the Jupyter web interface \u00b6 We're finally ready to connect to the web interface. To do so simply point your browser at XX.XX.XX.XX:8888 where XX.XX.XX.XX is again the public IP address of the VM that you've launched in AWS. 8888 is the TCP port number that Jupyter listens on by default, and we added a special security rule to open this port when creating the VM in AWS (remember?). If you missed that step, don't worry, it's possible to go into the EC2 dashboard and update the security settings to open port 8888. Assumming everything has worked you'll arrive at a Jupyter page asking for the security token. This is where you provide the super secret number that you selected above. And that's it, you're ready to use Jupyter! Other software \u00b6 Some of the steps in the tutorial use singularity or docker to run analysis tools. To carry out those steps singularity will need to be installed on the machine: sudo apt update sudo apt install -y singularity-container docker.io Docker requires a few extra steps to begin working sudo systemctl start docker sudo systemctl enable docker Ways to get started without using Amazon Web Services \u00b6 Not everyone will have access to Amazon EC2, or may not have access all the time. Another alternative is to run a Linux VM locally, provided that you have access to a computer with enough RAM and free storage space. One way to do this is to install VirtualBox on your machine, and then obtain an Ubuntu 18.04 LTS image to start up. This tutorial won't cover how to do this but don't worry, it's not hard. Plenty of people have documented how to do it. Look around, duckduckgo is your friend.","title":"Jupyter"},{"location":"startup/#setting-up-a-working-environment-jupyter","text":"In these tutorials we will work in a Jupyter notebook server environment. Jupyter is a versatile web interface system for data analysis, supporting work in languages such as Python and R, as well as the unix command-line. If you are working through this tutorial in one of The Gene School workshops we will provide a pre-configured Jupyter server for you to use and there is no need to carry out the setup process described on this page.","title":"Setting up a working environment (Jupyter)"},{"location":"startup/#launch-a-server-via-amazon-web-services-ec2","text":"First, either log in or sign up for AWS and navigate to to the EC2 section. Then select the option to launch a new instance. We will use the Ubuntu 18.04 LTS base image in this example. As you continue to click through the instance configuration details, there are two important settings that need to be changed from defaults: (1) the disk volume size, which should be set to 500GB or more to give ample space for metagenomic datasets (note that this will incur costs), and (2) the security groups settings. In the security groups, you will need to add access to TCP port 8888 for the IP address from which you'll be connecting to the servers. If you don't know your source IP range then this can be set to 0.0.0.0/0 but note the that this will enable the whole world to connect to the server and there is a security risk, even if the Jupyter server is password or token protected. If this is the first time you've used AWS you will need to generate and save an ssh key that will be used to log into the VMs. Keep this in a safe place. It will need to be made read only to the user account before it can be used with ssh, e.g. chmod 400 my_aws_key.pem","title":"Launch a server via Amazon Web Services EC2"},{"location":"startup/#log-in-and-launch-jupyter","text":"Having started up an AWS instance you are now ready to log in and install Jupyter. The process is fairly straightforward. First ssh into the instance: ssh -i my_aws_key.pem ubuntu@XX.XX.XX.XX where XX.XX.XX.XX is the public IP address shown in the AWS EC2 console for the VM you have launched. Once logged in there are a few steps to installing the Jupyter server. First download and install anaconda: wget https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh bash Anaconda3-2018.12-Linux-x86_64.sh Don't go too fast here Be sure to answer yes to the license agreement and the question about installing the configuration to the .bashrc file. If not, conda will not be configured properly. Many of the steps in the tutorial involve command-line work, so let's also install the bash kernel: pip install bash_kernel python -m bash_kernel.install Now we're ready to launch a jupyter server: source .bashrc jupyter lab --no-browser --ip 0.0.0.0 --LabApp.token=112233445566778899 IMPORTANT Make sure to replace the string of numbers 112233445566778899 with your own string -- this is the secret key that will allow you (and only you) to connect to your jupyter server and run commands, so you want something that neither human nor robot will guess. Hexadecimal (lowercase) values are ok here too.","title":"Log in and launch Jupyter"},{"location":"startup/#connect-to-the-jupyter-web-interface","text":"We're finally ready to connect to the web interface. To do so simply point your browser at XX.XX.XX.XX:8888 where XX.XX.XX.XX is again the public IP address of the VM that you've launched in AWS. 8888 is the TCP port number that Jupyter listens on by default, and we added a special security rule to open this port when creating the VM in AWS (remember?). If you missed that step, don't worry, it's possible to go into the EC2 dashboard and update the security settings to open port 8888. Assumming everything has worked you'll arrive at a Jupyter page asking for the security token. This is where you provide the super secret number that you selected above. And that's it, you're ready to use Jupyter!","title":"Connect to the Jupyter web interface"},{"location":"startup/#other-software","text":"Some of the steps in the tutorial use singularity or docker to run analysis tools. To carry out those steps singularity will need to be installed on the machine: sudo apt update sudo apt install -y singularity-container docker.io Docker requires a few extra steps to begin working sudo systemctl start docker sudo systemctl enable docker","title":"Other software"},{"location":"startup/#ways-to-get-started-without-using-amazon-web-services","text":"Not everyone will have access to Amazon EC2, or may not have access all the time. Another alternative is to run a Linux VM locally, provided that you have access to a computer with enough RAM and free storage space. One way to do this is to install VirtualBox on your machine, and then obtain an Ubuntu 18.04 LTS image to start up. This tutorial won't cover how to do this but don't worry, it's not hard. Plenty of people have documented how to do it. Look around, duckduckgo is your friend.","title":"Ways to get started without using Amazon Web Services"}]}