{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to the Metagenome Tutorial\n\n\nThese tutorials describe some current approaches for metagenome analysis using a genome-resolved approach.\n\n\nAuthors include:\n\n\n\n\nAaron Darling\n\n\nMatt DeMaere\n\n\n\n\nThe tutorial makes heavy use of a pig metagenomic timeseries and Hi-C dataset that was generated by Daniela Gaio and Kay Anantanawat. Several people have made key contributions to the overarching project in which the pig metagenomic data was generated, including Toni Chapman (NSW DPI), Steven P. Djordjevic (UTS), Michael YZ Liu, Tiziana Zingali, Linda Falconer, Joyce To, and Leigh Monahan.",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-the-metagenome-tutorial",
            "text": "These tutorials describe some current approaches for metagenome analysis using a genome-resolved approach.  Authors include:   Aaron Darling  Matt DeMaere   The tutorial makes heavy use of a pig metagenomic timeseries and Hi-C dataset that was generated by Daniela Gaio and Kay Anantanawat. Several people have made key contributions to the overarching project in which the pig metagenomic data was generated, including Toni Chapman (NSW DPI), Steven P. Djordjevic (UTS), Michael YZ Liu, Tiziana Zingali, Linda Falconer, Joyce To, and Leigh Monahan.",
            "title": "Welcome to the Metagenome Tutorial"
        },
        {
            "location": "/startup/",
            "text": "Setting up a working environment (Jupyter)\n\n\nIn these tutorials we will work in a Jupyter notebook server environment.\nJupyter is a versatile web interface system for data analysis, supporting work in languages such as Python and R, as well as the unix command-line.\nIf you are working through this tutorial in one of The Gene School workshops we will provide a pre-configured Jupyter server for you to use and there is no need to carry out the setup process described on this page.\n\n\nLaunch a server via Amazon Web Services EC2\n\n\nFirst, either log in or sign up for AWS and navigate to to the EC2 section.\nThen select the option to launch a new instance. We will use the Ubuntu 18.04 LTS base image in this example.\nAs you continue to click through the instance configuration details, there are two important settings that need to be changed from defaults: (1) the disk volume size, which should be set to 500GB or more to give ample space for metagenomic datasets (note that this will incur costs), and (2) the security groups settings. In the security groups, you will need to add access to TCP port 8888 for the IP address from which you'll be connecting to the servers. If you don't know your source IP range then this can be set to \n0.0.0.0/0\n but note the that this will enable the whole world to connect to the server and there is a security risk, even if the Jupyter server is password or token protected.\n\n\nIf this is the first time you've used AWS you will need to generate and save an ssh key that will be used to log into the VMs. Keep this in a safe place. It will need to be made read only to the user account before it can be used with ssh, e.g. \nchmod 400 my_aws_key.pem\n\n\nLog in and launch Jupyter\n\n\nHaving started up an AWS instance you are now ready to log in and install Jupyter.\nThe process is fairly straightforward. First ssh into the instance: \nssh -i my_aws_key.pem ubuntu@XX.XX.XX.XX\n where XX.XX.XX.XX is the public IP address shown in the AWS EC2 console for the VM you have launched.\nOnce logged in there are a few steps to installing the Jupyter server. First download and install anaconda:\n\n\nwget https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh\nbash Anaconda3-2018.12-Linux-x86_64.sh\n\n\n\n\nBe sure to answer \nyes\n to the license question and the question about installing the configuration to the \n.bashrc\n file.\n\n\nMany of the steps in the tutorial involve command-line work, so let's also install the bash kernel:\n\n\npip install bash_kernel\npython -m bash_kernel.install\n\n\n\n\nNow we're ready to launch a jupyter server:\n\n\nsource .bashrc\njupyter lab --no-browser --ip 0.0.0.0 --LabApp.token=112233445566778899\n\n\n\n\nIMPORTANT: replace the string of numbers \n112233445566778899\n with your own string -- this is the secret key that will allow you (and only you) to connect to your jupyter server and run commands, so you want something that neither human nor robot will guess. Hexadecimal (lowercase) values are ok here too.\n\n\nConnect to the Jupyter web interface\n\n\nWe're finally ready to connect to the web interface. To do so simply point your browser at \nXX.XX.XX.XX:8888\n where XX.XX.XX.XX is again the public IP address of the VM that you've launched in AWS. 8888 is the TCP port number that Jupyter listens on by default, and we added a special security rule to open this port when creating the VM in AWS (remember?). If you missed that step, don't worry, it's possible to go into the EC2 dashboard and update the security settings to open port 8888. Assumming everything has worked you'll arrive at a Jupyter page asking for the security token. This is where you provide the super secret number that you selected above. And that's it, you're ready to use Jupyter!\n\n\nOther software\n\n\nSome of the steps in the tutorial use \nsingularity\n to run analysis tools.\nTo carry out those steps singularity will need to be installed on the machine:\n\n\nsudo apt install -y singularity-container\n\n\n\n\nWays to get started without using Amazon Web Services\n\n\nNot everyone will have access to Amazon EC2, or may not have access all the time.\nAnother alternative is to run a Linux VM locally, provided that you have access to a computer with enough RAM and free storage space.\nOne way to do this is to install VirtualBox on your machine, and then obtain an Ubuntu 18.04 LTS image to start up.\nThis tutorial won't cover how to do this but don't worry, it's not hard.\nPlenty of people have documented how to do it.\nLook around, \nduckduckgo\n is your friend.",
            "title": "Jupyter"
        },
        {
            "location": "/startup/#setting-up-a-working-environment-jupyter",
            "text": "In these tutorials we will work in a Jupyter notebook server environment.\nJupyter is a versatile web interface system for data analysis, supporting work in languages such as Python and R, as well as the unix command-line.\nIf you are working through this tutorial in one of The Gene School workshops we will provide a pre-configured Jupyter server for you to use and there is no need to carry out the setup process described on this page.",
            "title": "Setting up a working environment (Jupyter)"
        },
        {
            "location": "/startup/#launch-a-server-via-amazon-web-services-ec2",
            "text": "First, either log in or sign up for AWS and navigate to to the EC2 section.\nThen select the option to launch a new instance. We will use the Ubuntu 18.04 LTS base image in this example.\nAs you continue to click through the instance configuration details, there are two important settings that need to be changed from defaults: (1) the disk volume size, which should be set to 500GB or more to give ample space for metagenomic datasets (note that this will incur costs), and (2) the security groups settings. In the security groups, you will need to add access to TCP port 8888 for the IP address from which you'll be connecting to the servers. If you don't know your source IP range then this can be set to  0.0.0.0/0  but note the that this will enable the whole world to connect to the server and there is a security risk, even if the Jupyter server is password or token protected.  If this is the first time you've used AWS you will need to generate and save an ssh key that will be used to log into the VMs. Keep this in a safe place. It will need to be made read only to the user account before it can be used with ssh, e.g.  chmod 400 my_aws_key.pem",
            "title": "Launch a server via Amazon Web Services EC2"
        },
        {
            "location": "/startup/#log-in-and-launch-jupyter",
            "text": "Having started up an AWS instance you are now ready to log in and install Jupyter.\nThe process is fairly straightforward. First ssh into the instance:  ssh -i my_aws_key.pem ubuntu@XX.XX.XX.XX  where XX.XX.XX.XX is the public IP address shown in the AWS EC2 console for the VM you have launched.\nOnce logged in there are a few steps to installing the Jupyter server. First download and install anaconda:  wget https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh\nbash Anaconda3-2018.12-Linux-x86_64.sh  Be sure to answer  yes  to the license question and the question about installing the configuration to the  .bashrc  file.  Many of the steps in the tutorial involve command-line work, so let's also install the bash kernel:  pip install bash_kernel\npython -m bash_kernel.install  Now we're ready to launch a jupyter server:  source .bashrc\njupyter lab --no-browser --ip 0.0.0.0 --LabApp.token=112233445566778899  IMPORTANT: replace the string of numbers  112233445566778899  with your own string -- this is the secret key that will allow you (and only you) to connect to your jupyter server and run commands, so you want something that neither human nor robot will guess. Hexadecimal (lowercase) values are ok here too.",
            "title": "Log in and launch Jupyter"
        },
        {
            "location": "/startup/#connect-to-the-jupyter-web-interface",
            "text": "We're finally ready to connect to the web interface. To do so simply point your browser at  XX.XX.XX.XX:8888  where XX.XX.XX.XX is again the public IP address of the VM that you've launched in AWS. 8888 is the TCP port number that Jupyter listens on by default, and we added a special security rule to open this port when creating the VM in AWS (remember?). If you missed that step, don't worry, it's possible to go into the EC2 dashboard and update the security settings to open port 8888. Assumming everything has worked you'll arrive at a Jupyter page asking for the security token. This is where you provide the super secret number that you selected above. And that's it, you're ready to use Jupyter!",
            "title": "Connect to the Jupyter web interface"
        },
        {
            "location": "/startup/#other-software",
            "text": "Some of the steps in the tutorial use  singularity  to run analysis tools.\nTo carry out those steps singularity will need to be installed on the machine:  sudo apt install -y singularity-container",
            "title": "Other software"
        },
        {
            "location": "/startup/#ways-to-get-started-without-using-amazon-web-services",
            "text": "Not everyone will have access to Amazon EC2, or may not have access all the time.\nAnother alternative is to run a Linux VM locally, provided that you have access to a computer with enough RAM and free storage space.\nOne way to do this is to install VirtualBox on your machine, and then obtain an Ubuntu 18.04 LTS image to start up.\nThis tutorial won't cover how to do this but don't worry, it's not hard.\nPlenty of people have documented how to do it.\nLook around,  duckduckgo  is your friend.",
            "title": "Ways to get started without using Amazon Web Services"
        },
        {
            "location": "/qc/",
            "text": "Sequencing run QC\n\n\nGet some sequence data\n\n\nThe first thing we'll do is to get some sequence data to work with. If you are working on a new sequencing project the data might come from a sequencing facility. For this tutorial we'll work with data that is available in public databases.\nPublished sequence data is usually archived in the NCBI SRA, the ENA, and the DDBJ.\nThese databases provide a \nconvenient interface to search\n the descriptions of samples by keyword, and by sample type (e.g. shotgun metagenome).\nFor the following exercises we'll use a set of small datasets (SRA accessions SRR9323808, SRR9323810, SRR9323811, SRR9323809) which will be quick to process because they are small.\nThe easiest way to download data from these databases is via the \nfastq-dump\n software.\nFirst, let's install the parallel version of fastq-dump using conda.\nTo do this, start a terminal session in your Jupyter server (click the Terminal icon) and run the following command (ok to copy and paste):\n\n\nconda install -c bioconda parallel-fastq-dump \n\n\n\n\nNow that you've installed fastq-dump we can use it to download data by accession number. Copy and paste the following to your terminal:\n\n\nparallel-fastq-dump -s SRR9323808 -s SRR9323810 -s SRR9323811 -s SRR9323809 --threads 4 --outdir qc_data/ --split-files --gzip\n\n\n\n\nIf the download was successful you should see something like the following on your terminal:\n\n\n\n\nEvaluating sequence quality with FastQC and MultiQC\n\n\nThe very first thing one would normally do when working with a new dataset is to look at some basic quality statistics on the data.\nThere are many, many tools available to compute quality metrics. For our current data we will use the FastQC software, applied to each sample, and then combine the results using MultiQC to a single report. First step is to install \nfastqc\n and \nmultiqc\n.\n\n\nconda install -c bioconda fastqc\npip install multiqc\n\n\n\n\nIn the above we've used conda to install fastqc, but we've used another way to install multiqc -- something called \npip\n. pip is an installer for python programs, and like conda it will download and install the software along with any dependencies. The reason we use pip in this case is because conda can be very, very slow to install some programs and in this case pip is much faster.\n\n\ncd qc_data\nfind . -name \"*.fastq.gz\" -exec fastqc {} \\;\n\n\n\n\nLet's unpack those commands a bit. The first part, \ncd qc_data\n just changes the current directory to qc_data, so any following commands run will run in that directory.\nThe next command is \nfind . -name \"*.fastq.gz\" -exec fastqc {} \\;\n. The first part, \nfind\n is a command that finds files. If you just run \nfind .\n it will find all the files in and below the current directory (the \n.\n part specifies to look in the current directory). The next part, \n-name \"*.fastq.gz\"\n tells \nfind\n that we only want it to find files with names that end with \n.fastq.gz\n. The \n*\n is a wildcard that matches anything. Finally, the last part \n-exec fastqc {} \\;\n tells \nfind\n that whenever it finds a file, it should run \nfastqc\n on that file, putting the name of the file where the \n{}\n are.\n\n\nIf this step has worked, then you should have several new \n.zip\n files containing the QC data in that directory, along with some html files. When we have a lot of samples it is too tedious to look at all the QC reports individually for each sample, so we can summarize them using multiqc:\n\n\nmultiqc .\n\n\n\n\nAt this point a multiqc file will appear inside the QC directory. First double click to open the QC folder.\n\n\n\nOnce that's open a file called \nmultiqc_report.html\n will appear in the listing. \nWe can open this file from within our jupyter browser environment and inspect it.\nTo open it, we need to right click (or two-finger tap) on the file name to get a context menu that will give several options for how to open it. It looks like this:\n\n\n\n\nClick the option to \"Open in a New Browser Tab\". From here we can evaluate the quality of the libraries.\n\n\nTaxonomic analysis\n\n\nMetagenome taxonomic analysis offers a means to estimate a microbial community profile from metagenomic sequence data.\nIt can give us a very high-level, rough idea of what kinds of microbes are present in a sample.\nIt can also give an idea of how complex/diverse the microbial community is -- whether there are many species or few.\nIt is useful as an initial quality check to ensure that the microbial community composition looks roughly as expected, and to confirm that nothing obvious went wrong during the sample collection and sequencing steps.\n\n\nTaxonomic analysis with Metaphlan2\n\n\nWhile it may be possible to install metaphlan2 via conda, at least in my experience, conda struggles with \"solving the environment\".\nTherefore it's suggested to install it via the simple download method described on the \nmetaphlan tutorial page\n:\n\n\ncd ; wget -c -O metaphlan.tar.bz2 https://bitbucket.org/nsegata/metaphlan/get/default.tar.bz2\ntar xvjf metaphlan.tar.bz2\nmv nsegata-metaphlan* metaphlan\n\n\n\n\nOnce metaphlan has been downloaded we can run it on our QC samples:\n\n\ncd ~/qc_data\npig_samples=\"SRR9323808 SRR9323810 SRR9323811 SRR9323809\"\nfor s in ${pig_samples}\ndo\n     zcat ${s}*.fastq.gz | python2 ~/metaphlan/metaphlan.py --input_type multifastq --bt2_ps very-sensitive --bowtie2db ~/metaphlan/bowtie2db/mpa  --bowtie2out ${s}.bt2out -o ${s}.mph2\ndone\n\n\n\n\nThe above series of commands creates a \"for loop\" where each iteration processes one of our metagenome samples. This is a convenient way to process many samples without having to type out the commands for each sample. The sample names get stored in a variable \n$pig_samples\n and at each loop iteration, one of the sample names is placed into the loop variable \n${s}\n, where it can get used in the command inside the loop.\n\n\nFinally we can plot the taxonomic profile of the samples:\n\n\npython2 ~/metaphlan/utils/merge_metaphlan_tables.py *.mph2 > pig_mph2.merged\npython2 ~/metaphlan/plotting_scripts/metaphlan_hclust_heatmap.py -c bbcry --top 25 --minv 0.1 -s log --in pig_mph2.merged --out mph2_heatmap.png\n\n\n\n\nOnce that has completed successfully, a new file called \nmph2_heatmap.png\n will appear in the qc_data folder of our Jupyter file browser. We can double click it to view.\n\n\nThere are other ways to visualize the data, and they are described in the graphlan section of the \nmetaphlan tutorial\n page.\n\n\nTaxonomic analysis with other tools\n\n\nThere are a whole range of other software tools available for metagenome taxonomic analysis. \nThey all have strengths and weaknesses.\n\n\nA few other commonly used tools are listed here:\n\n\n\n\nkraken2\n\n\nMEGAN\n\n\nCentrifuge\n\n\nCLARK\n\n\n\n\nEvaluating the host genomic content\n\n\nIn many applications of metagenomics we are working with host-associated samples. \nThe samples might have come from an animal gut, mouth, or other surface.\nSimilarly for plants we might be working with leaf or root surfaces or rhizobia.\nWhen such samples are collected the resulting DNA extracts can include a significant fraction of host material.\nLet's have a look at what this looks like in data.\nTo do so, we'll download another set of pig gut samples: a set of samples that was taken using scrapings or swabs from different parts of the porcine digestive system, including the duodenum, jejunum, ileum, colon, and caecum.\nRather than using metaphlan2 to profile these, we will use the kraken2 classifier in conjunction with the bracken tool for estimating relative abundances.\nWe first need to install kraken2 and bracken:\n\n\nconda install -c bioconda kraken2 bracken\n\n\n\n\nNext, we need to get a kraken2 database.\nFor this tutorial we will simply use a precomputed kraken2 database but note the very important limitation that the only non-microbial genome it includes is the human genome.\nIf you would like to evaluate host genomic content on plants or other things you should follow the instructions on the kraken2 web site to build a complete database.\nWe can download and unpack the kraken2 database with:\n\n\ncd ; wget -c ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/minikraken2_v2_8GB_201904_UPDATE.tgz\ntar xvzf minikraken2_v2_8GB_201904_UPDATE.tgz\n\n\n\n\nFinally we are ready to profile our samples with kraken2 and bracken.\nWe'll first download the sample set with \nparallel-fastq-dump\n and then run kraken2 and bracken's \nest_abundance.py\n script on each sample using a bash for loop.\nThe analysis can be run as follows:\n\n\nparallel-fastq-dump  -s SRR9332442 -s SRR9332438 -s SRR9332439 -s SRR9332443 -s SRR9332440 --threads 4 --outdir host_qc/ --split-files --gzip\ncd host_qc\nsamples=\"SRR9332442 SRR9332438 SRR9332439 SRR9332443 SRR9332440\"\nfor s in $samples; do kraken2 --paired ${s}_1.fastq.gz ${s}_2.fastq.gz --db ../minikraken2_v2_8GB_201904_UPDATE/ --report ${s}.kreport > ${s}.kraken; done\nfor s in $samples; do est_abundance.py -i ${s}.kreport -k ../minikraken2_v2_8GB_201904_UPDATE/database150mers.kmer_distrib -o ${s}.bracken; done\n\n\n\n\nOnce the above has completed, navigate over to the \nhost_qc\n folder in the Jupyter file browser and click on the \n*.bracken\n files to open them.\nWhat do you see?\nIn particular, why does sample SRR9332438 look so different to sample SRR9332440?\nKeep in mind the isolation sources of the samples were as follows:\n\n\n\n\n\n\n\n\nSample\n\n\nSource\n\n\n\n\n\n\n\n\n\n\nSRR9332442\n\n\nDuodenum\n\n\n\n\n\n\nSRR9332440\n\n\nCaecum\n\n\n\n\n\n\nSRR9332439\n\n\nIleum\n\n\n\n\n\n\nSRR9332438\n\n\nJejunum\n\n\n\n\n\n\nSRR9332443\n\n\nColon\n\n\n\n\n\n\n\n\nSome challenge questions\n\n\n\n\nIf we sequenced samples from pigs, why is human DNA being predicted in these samples?\n\n\nIf we were to design a large study around these samples which of them would be suitable for metagenomics, and why?\n\n\nHow much sequencing data would we need to generate from sample SRR9332440 to reconstruct the genome of the \nBifidobacterium\n in that sample? What about the \nE. coli\n?\n\n\nAre there really six species of \nLactobacillus\n present in SRR9332440?\n\n\nGo to the \nNCBI SRA search tool\n and find a metagenome of interest to you. Download the first 100000 reads from it (use the \n--maxSpotId\n parameter) and analyse it with kraken2. Is it what you expected? How does it compare to the others we've looked at?\n\n\n\n\nA note on negative controls\n\n\nNegative controls are a key element in any microbiome profiling or metagenome analysis work.\nEvery aspect of the sample collection and processing can be impacted by the presence of contaminating microbial DNA.\nThis is true even of 'sterile' material -- just because no viable cells exist in a sample collection swab, for example, does not mean there is no microbial DNA on that swab.\nIt is well known that molecular biology reagents frequently contain contaminating DNA.\nUsually it is at low levels, but this is not always the case.\n\n\nTherefore, the best practice is to collect multiple negative control samples that are taken all the way through sequencing.\nThese negative controls can then be used to correct for contamination artifacts in the remaining samples.",
            "title": "QC"
        },
        {
            "location": "/qc/#sequencing-run-qc",
            "text": "",
            "title": "Sequencing run QC"
        },
        {
            "location": "/qc/#get-some-sequence-data",
            "text": "The first thing we'll do is to get some sequence data to work with. If you are working on a new sequencing project the data might come from a sequencing facility. For this tutorial we'll work with data that is available in public databases.\nPublished sequence data is usually archived in the NCBI SRA, the ENA, and the DDBJ.\nThese databases provide a  convenient interface to search  the descriptions of samples by keyword, and by sample type (e.g. shotgun metagenome).\nFor the following exercises we'll use a set of small datasets (SRA accessions SRR9323808, SRR9323810, SRR9323811, SRR9323809) which will be quick to process because they are small.\nThe easiest way to download data from these databases is via the  fastq-dump  software.\nFirst, let's install the parallel version of fastq-dump using conda.\nTo do this, start a terminal session in your Jupyter server (click the Terminal icon) and run the following command (ok to copy and paste):  conda install -c bioconda parallel-fastq-dump   Now that you've installed fastq-dump we can use it to download data by accession number. Copy and paste the following to your terminal:  parallel-fastq-dump -s SRR9323808 -s SRR9323810 -s SRR9323811 -s SRR9323809 --threads 4 --outdir qc_data/ --split-files --gzip  If the download was successful you should see something like the following on your terminal:",
            "title": "Get some sequence data"
        },
        {
            "location": "/qc/#evaluating-sequence-quality-with-fastqc-and-multiqc",
            "text": "The very first thing one would normally do when working with a new dataset is to look at some basic quality statistics on the data.\nThere are many, many tools available to compute quality metrics. For our current data we will use the FastQC software, applied to each sample, and then combine the results using MultiQC to a single report. First step is to install  fastqc  and  multiqc .  conda install -c bioconda fastqc\npip install multiqc  In the above we've used conda to install fastqc, but we've used another way to install multiqc -- something called  pip . pip is an installer for python programs, and like conda it will download and install the software along with any dependencies. The reason we use pip in this case is because conda can be very, very slow to install some programs and in this case pip is much faster.  cd qc_data\nfind . -name \"*.fastq.gz\" -exec fastqc {} \\;  Let's unpack those commands a bit. The first part,  cd qc_data  just changes the current directory to qc_data, so any following commands run will run in that directory.\nThe next command is  find . -name \"*.fastq.gz\" -exec fastqc {} \\; . The first part,  find  is a command that finds files. If you just run  find .  it will find all the files in and below the current directory (the  .  part specifies to look in the current directory). The next part,  -name \"*.fastq.gz\"  tells  find  that we only want it to find files with names that end with  .fastq.gz . The  *  is a wildcard that matches anything. Finally, the last part  -exec fastqc {} \\;  tells  find  that whenever it finds a file, it should run  fastqc  on that file, putting the name of the file where the  {}  are.  If this step has worked, then you should have several new  .zip  files containing the QC data in that directory, along with some html files. When we have a lot of samples it is too tedious to look at all the QC reports individually for each sample, so we can summarize them using multiqc:  multiqc .  At this point a multiqc file will appear inside the QC directory. First double click to open the QC folder.  Once that's open a file called  multiqc_report.html  will appear in the listing. \nWe can open this file from within our jupyter browser environment and inspect it.\nTo open it, we need to right click (or two-finger tap) on the file name to get a context menu that will give several options for how to open it. It looks like this:   Click the option to \"Open in a New Browser Tab\". From here we can evaluate the quality of the libraries.",
            "title": "Evaluating sequence quality with FastQC and MultiQC"
        },
        {
            "location": "/qc/#taxonomic-analysis",
            "text": "Metagenome taxonomic analysis offers a means to estimate a microbial community profile from metagenomic sequence data.\nIt can give us a very high-level, rough idea of what kinds of microbes are present in a sample.\nIt can also give an idea of how complex/diverse the microbial community is -- whether there are many species or few.\nIt is useful as an initial quality check to ensure that the microbial community composition looks roughly as expected, and to confirm that nothing obvious went wrong during the sample collection and sequencing steps.  Taxonomic analysis with Metaphlan2  While it may be possible to install metaphlan2 via conda, at least in my experience, conda struggles with \"solving the environment\".\nTherefore it's suggested to install it via the simple download method described on the  metaphlan tutorial page :  cd ; wget -c -O metaphlan.tar.bz2 https://bitbucket.org/nsegata/metaphlan/get/default.tar.bz2\ntar xvjf metaphlan.tar.bz2\nmv nsegata-metaphlan* metaphlan  Once metaphlan has been downloaded we can run it on our QC samples:  cd ~/qc_data\npig_samples=\"SRR9323808 SRR9323810 SRR9323811 SRR9323809\"\nfor s in ${pig_samples}\ndo\n     zcat ${s}*.fastq.gz | python2 ~/metaphlan/metaphlan.py --input_type multifastq --bt2_ps very-sensitive --bowtie2db ~/metaphlan/bowtie2db/mpa  --bowtie2out ${s}.bt2out -o ${s}.mph2\ndone  The above series of commands creates a \"for loop\" where each iteration processes one of our metagenome samples. This is a convenient way to process many samples without having to type out the commands for each sample. The sample names get stored in a variable  $pig_samples  and at each loop iteration, one of the sample names is placed into the loop variable  ${s} , where it can get used in the command inside the loop.  Finally we can plot the taxonomic profile of the samples:  python2 ~/metaphlan/utils/merge_metaphlan_tables.py *.mph2 > pig_mph2.merged\npython2 ~/metaphlan/plotting_scripts/metaphlan_hclust_heatmap.py -c bbcry --top 25 --minv 0.1 -s log --in pig_mph2.merged --out mph2_heatmap.png  Once that has completed successfully, a new file called  mph2_heatmap.png  will appear in the qc_data folder of our Jupyter file browser. We can double click it to view.  There are other ways to visualize the data, and they are described in the graphlan section of the  metaphlan tutorial  page.  Taxonomic analysis with other tools  There are a whole range of other software tools available for metagenome taxonomic analysis. \nThey all have strengths and weaknesses.  A few other commonly used tools are listed here:   kraken2  MEGAN  Centrifuge  CLARK",
            "title": "Taxonomic analysis"
        },
        {
            "location": "/qc/#evaluating-the-host-genomic-content",
            "text": "In many applications of metagenomics we are working with host-associated samples. \nThe samples might have come from an animal gut, mouth, or other surface.\nSimilarly for plants we might be working with leaf or root surfaces or rhizobia.\nWhen such samples are collected the resulting DNA extracts can include a significant fraction of host material.\nLet's have a look at what this looks like in data.\nTo do so, we'll download another set of pig gut samples: a set of samples that was taken using scrapings or swabs from different parts of the porcine digestive system, including the duodenum, jejunum, ileum, colon, and caecum.\nRather than using metaphlan2 to profile these, we will use the kraken2 classifier in conjunction with the bracken tool for estimating relative abundances.\nWe first need to install kraken2 and bracken:  conda install -c bioconda kraken2 bracken  Next, we need to get a kraken2 database.\nFor this tutorial we will simply use a precomputed kraken2 database but note the very important limitation that the only non-microbial genome it includes is the human genome.\nIf you would like to evaluate host genomic content on plants or other things you should follow the instructions on the kraken2 web site to build a complete database.\nWe can download and unpack the kraken2 database with:  cd ; wget -c ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/minikraken2_v2_8GB_201904_UPDATE.tgz\ntar xvzf minikraken2_v2_8GB_201904_UPDATE.tgz  Finally we are ready to profile our samples with kraken2 and bracken.\nWe'll first download the sample set with  parallel-fastq-dump  and then run kraken2 and bracken's  est_abundance.py  script on each sample using a bash for loop.\nThe analysis can be run as follows:  parallel-fastq-dump  -s SRR9332442 -s SRR9332438 -s SRR9332439 -s SRR9332443 -s SRR9332440 --threads 4 --outdir host_qc/ --split-files --gzip\ncd host_qc\nsamples=\"SRR9332442 SRR9332438 SRR9332439 SRR9332443 SRR9332440\"\nfor s in $samples; do kraken2 --paired ${s}_1.fastq.gz ${s}_2.fastq.gz --db ../minikraken2_v2_8GB_201904_UPDATE/ --report ${s}.kreport > ${s}.kraken; done\nfor s in $samples; do est_abundance.py -i ${s}.kreport -k ../minikraken2_v2_8GB_201904_UPDATE/database150mers.kmer_distrib -o ${s}.bracken; done  Once the above has completed, navigate over to the  host_qc  folder in the Jupyter file browser and click on the  *.bracken  files to open them.\nWhat do you see?\nIn particular, why does sample SRR9332438 look so different to sample SRR9332440?\nKeep in mind the isolation sources of the samples were as follows:     Sample  Source      SRR9332442  Duodenum    SRR9332440  Caecum    SRR9332439  Ileum    SRR9332438  Jejunum    SRR9332443  Colon     Some challenge questions   If we sequenced samples from pigs, why is human DNA being predicted in these samples?  If we were to design a large study around these samples which of them would be suitable for metagenomics, and why?  How much sequencing data would we need to generate from sample SRR9332440 to reconstruct the genome of the  Bifidobacterium  in that sample? What about the  E. coli ?  Are there really six species of  Lactobacillus  present in SRR9332440?  Go to the  NCBI SRA search tool  and find a metagenome of interest to you. Download the first 100000 reads from it (use the  --maxSpotId  parameter) and analyse it with kraken2. Is it what you expected? How does it compare to the others we've looked at?",
            "title": "Evaluating the host genomic content"
        },
        {
            "location": "/qc/#a-note-on-negative-controls",
            "text": "Negative controls are a key element in any microbiome profiling or metagenome analysis work.\nEvery aspect of the sample collection and processing can be impacted by the presence of contaminating microbial DNA.\nThis is true even of 'sterile' material -- just because no viable cells exist in a sample collection swab, for example, does not mean there is no microbial DNA on that swab.\nIt is well known that molecular biology reagents frequently contain contaminating DNA.\nUsually it is at low levels, but this is not always the case.  Therefore, the best practice is to collect multiple negative control samples that are taken all the way through sequencing.\nThese negative controls can then be used to correct for contamination artifacts in the remaining samples.",
            "title": "A note on negative controls"
        },
        {
            "location": "/assembly/",
            "text": "Metagenome assembly\n\n\nWhat is assembly?\n\n\nAssembly is the process of reconstructing long DNA sequences from a collection of overlapping shorter pieces.\nIn the context of single genomes (e.g. microbial isolates) the goal is usually to reconstruct a single chromosome and any associated plasmids or other replicons.\nWhen presented with metagenomic data though, the microbial community might in reality have millions of genomes present in it.\nGenerally speaking, it is impossible to accurately reconstruct every genome of every cell in a metagenomic sample with current technology.\nInstead we usually aim to reconstruct chromosome sequences that represent a consensus of many cells.\n\n\nAssembling metagenomes\n\n\nThere are several available metagenome assembly tools. \nSome examples include \nmetaSPAdes\n and \nMEGAHIT\n.\nIn this tutorial we will use MEGAHIT, and we will use it via a docker container.\nAs such, no installation step is necessary.\n\n\nCoassembly or single sample?\n\n\nJust as important as deciding what assembler to use is deciding on \nwhat to assemble\n.\nOne approach is to assemble all of the metagenome samples together.\nThis can be useful if there is not much data per sample, or if low abundance organisms exist in the samples that would not have enough coverage in a single sample to be assembled.\nBut it can create problems if closely related species or strains exist in the different samples.\n\n\nLimitations of metagenome assembly\n\n\nCurrent assembly methods are limited in their ability to resolve chromosomes of closely related species and strains in a sample.\nThis is because most assembly methods collapse sequences > 95% or 98% identity into a single contig sequence, usually as a means to cope with sequencing error.\nTherefore as a consensus representation, these assembled chromosome sequences may mask fine-scale genetic variation in the population.\n\n\nAssembling some example data\n\n\nparallel-fastq-dump -t 4 --outdir asm --split-files --gzip -s SRR8960410 -s SRR8960409 -s SRR8960402 -s SRR8960368 -s SRR8960420 -s SRR8960739 -s SRR8960679 -s SRR8960627 -s SRR8960591 -s SRR8960887 --minSpotId 0 --maxSpotId 50000 && mv asm assembly\n\n\n\n\nThe above command will download the first 50000 read-pairs of a set of samples.\nAll of these samples come from the same pig, and were collected at different time points in consecutive weeks.\n\n\nNow we can assemble with megahit:\n\n\nsingularity exec -B ~/assembly/:/data docker://quay.io/biocontainers/megahit:1.1.3--py36_0 megahit -1 /data/SRR8960410_1.fastq.gz,/data/SRR8960409_1.fastq.gz,/data/SRR8960402_1.fastq.gz,/data/SRR8960368_1.fastq.gz,/data/SRR8960420_1.fastq.gz,/data/SRR8960739_1.fastq.gz,/data/SRR8960679_1.fastq.gz,/data/SRR8960627_1.fastq.gz,/data/SRR8960591_1.fastq.gz,/data/SRR8960887_1.fastq.gz -2 /data/SRR8960410_2.fastq.gz,/data/SRR8960409_2.fastq.gz,/data/SRR8960402_2.fastq.gz,/data/SRR8960368_2.fastq.gz,/data/SRR8960420_2.fastq.gz,/data/SRR8960739_2.fastq.gz,/data/SRR8960679_2.fastq.gz,/data/SRR8960627_2.fastq.gz,/data/SRR8960591_2.fastq.gz,/data/SRR8960887_2.fastq.gz -o /data/metaasm\n\n\n\n\nThat's a big command-line, so let's unpack what's happening. \nFirst, we're invoking \nsingularity\n. \nSingularity is a container service that can download and run programs that have been packaged as \ncontainers\n -- a system that allows all needed dependency software to be specified and obtained automatically. \nBy invoking \nsingularity exec\n we are saying that we want to run a command inside a container. \nSimply put it allows us to run the software easily and reliably, avoiding the manual software install process. \nThe container we want to use is specified as \ndocker://quay.io/biocontainers/megahit:1.1.3--py36_0\n. \nThis is a docker container, and the \ndocker://\n syntax tells singularity that it can download the container from a public server. \nThe \n:1.1.3--py36_0\n specifies the exact version of the megahit container to use. \nBefore the container specification we have the argument \n-B ~/assembly/:/data\n. \nThis argument binds the \nassembly/\n directory in our current path to appear as \n/data\n inside the running container. \nTherefore the programs running inside the container (e.g. megahit) will be able to see all the files inside \n~/assembly/\n at the path \n/data\n. Next we have the megahit command line. \nThis includes parameters \n-1\n and \n-2\n with a list of the FastQ files we want to assemble. \nFinally we ask megahit to save the assembly in the container path \n/data/metaasm\n, so it will show up in \n~/assembly/metaasm\n when the container has finished running.\n\n\nAt the end of this process we will have a metagenome assembly saved in the file \n~/assembly/final.contigs.fa\n. \nWe can use this file for subsequent analyses.\nOne small detail we need to resolve is the formatting of contig names in the assembly file.\nmegahit creates contigs with names like ``, but the whitespace in these names causes problems for certain downstream analyses, such as visualization with anvi'o.\nTherefore as a final step in the assembly process we need to rename the contigs with the following commands:\n\n\ncd assembly\ncp final.contigs.fa contigs-fixnames.fa\nperl -p -i -e \"s/(>\\w+) flag.*/\\$1/g\" contigs-fixnames.fa",
            "title": "Assembly"
        },
        {
            "location": "/assembly/#metagenome-assembly",
            "text": "",
            "title": "Metagenome assembly"
        },
        {
            "location": "/assembly/#what-is-assembly",
            "text": "Assembly is the process of reconstructing long DNA sequences from a collection of overlapping shorter pieces.\nIn the context of single genomes (e.g. microbial isolates) the goal is usually to reconstruct a single chromosome and any associated plasmids or other replicons.\nWhen presented with metagenomic data though, the microbial community might in reality have millions of genomes present in it.\nGenerally speaking, it is impossible to accurately reconstruct every genome of every cell in a metagenomic sample with current technology.\nInstead we usually aim to reconstruct chromosome sequences that represent a consensus of many cells.",
            "title": "What is assembly?"
        },
        {
            "location": "/assembly/#assembling-metagenomes",
            "text": "There are several available metagenome assembly tools. \nSome examples include  metaSPAdes  and  MEGAHIT .\nIn this tutorial we will use MEGAHIT, and we will use it via a docker container.\nAs such, no installation step is necessary.  Coassembly or single sample?  Just as important as deciding what assembler to use is deciding on  what to assemble .\nOne approach is to assemble all of the metagenome samples together.\nThis can be useful if there is not much data per sample, or if low abundance organisms exist in the samples that would not have enough coverage in a single sample to be assembled.\nBut it can create problems if closely related species or strains exist in the different samples.  Limitations of metagenome assembly  Current assembly methods are limited in their ability to resolve chromosomes of closely related species and strains in a sample.\nThis is because most assembly methods collapse sequences > 95% or 98% identity into a single contig sequence, usually as a means to cope with sequencing error.\nTherefore as a consensus representation, these assembled chromosome sequences may mask fine-scale genetic variation in the population.",
            "title": "Assembling metagenomes"
        },
        {
            "location": "/assembly/#assembling-some-example-data",
            "text": "parallel-fastq-dump -t 4 --outdir asm --split-files --gzip -s SRR8960410 -s SRR8960409 -s SRR8960402 -s SRR8960368 -s SRR8960420 -s SRR8960739 -s SRR8960679 -s SRR8960627 -s SRR8960591 -s SRR8960887 --minSpotId 0 --maxSpotId 50000 && mv asm assembly  The above command will download the first 50000 read-pairs of a set of samples.\nAll of these samples come from the same pig, and were collected at different time points in consecutive weeks.  Now we can assemble with megahit:  singularity exec -B ~/assembly/:/data docker://quay.io/biocontainers/megahit:1.1.3--py36_0 megahit -1 /data/SRR8960410_1.fastq.gz,/data/SRR8960409_1.fastq.gz,/data/SRR8960402_1.fastq.gz,/data/SRR8960368_1.fastq.gz,/data/SRR8960420_1.fastq.gz,/data/SRR8960739_1.fastq.gz,/data/SRR8960679_1.fastq.gz,/data/SRR8960627_1.fastq.gz,/data/SRR8960591_1.fastq.gz,/data/SRR8960887_1.fastq.gz -2 /data/SRR8960410_2.fastq.gz,/data/SRR8960409_2.fastq.gz,/data/SRR8960402_2.fastq.gz,/data/SRR8960368_2.fastq.gz,/data/SRR8960420_2.fastq.gz,/data/SRR8960739_2.fastq.gz,/data/SRR8960679_2.fastq.gz,/data/SRR8960627_2.fastq.gz,/data/SRR8960591_2.fastq.gz,/data/SRR8960887_2.fastq.gz -o /data/metaasm  That's a big command-line, so let's unpack what's happening. \nFirst, we're invoking  singularity . \nSingularity is a container service that can download and run programs that have been packaged as  containers  -- a system that allows all needed dependency software to be specified and obtained automatically. \nBy invoking  singularity exec  we are saying that we want to run a command inside a container. \nSimply put it allows us to run the software easily and reliably, avoiding the manual software install process. \nThe container we want to use is specified as  docker://quay.io/biocontainers/megahit:1.1.3--py36_0 . \nThis is a docker container, and the  docker://  syntax tells singularity that it can download the container from a public server. \nThe  :1.1.3--py36_0  specifies the exact version of the megahit container to use. \nBefore the container specification we have the argument  -B ~/assembly/:/data . \nThis argument binds the  assembly/  directory in our current path to appear as  /data  inside the running container. \nTherefore the programs running inside the container (e.g. megahit) will be able to see all the files inside  ~/assembly/  at the path  /data . Next we have the megahit command line. \nThis includes parameters  -1  and  -2  with a list of the FastQ files we want to assemble. \nFinally we ask megahit to save the assembly in the container path  /data/metaasm , so it will show up in  ~/assembly/metaasm  when the container has finished running.  At the end of this process we will have a metagenome assembly saved in the file  ~/assembly/final.contigs.fa . \nWe can use this file for subsequent analyses.\nOne small detail we need to resolve is the formatting of contig names in the assembly file.\nmegahit creates contigs with names like ``, but the whitespace in these names causes problems for certain downstream analyses, such as visualization with anvi'o.\nTherefore as a final step in the assembly process we need to rename the contigs with the following commands:  cd assembly\ncp final.contigs.fa contigs-fixnames.fa\nperl -p -i -e \"s/(>\\w+) flag.*/\\$1/g\" contigs-fixnames.fa",
            "title": "Assembling some example data"
        },
        {
            "location": "/binning/",
            "text": "Metagenome Assembled Genomes (MAGs) from metagenome binning\n\n\nWhat is a MAG?\n\n\nA MAG, or Metagenome Assembled Genome, is a genome that has been reconstructed from metagenomic data.\nBecause these genomes do not derive from a clonal culture they typically represent a consensus of the genomes of many closely related cells in one or more metagenomic samples, and for this reason they are sometimes called \npopulation genomes\n.\nIt is important to remember that because of the way MAGs are inferred from the data the resulting sequence is usually fragmentary and may not accurately represent the genome of \nany\n cell in the community.\nIt is merely an average estimate, and there are well-known problems with taking averages, see for example the wikipedia page on \nSimpson's paradox\n to get an idea of how averages can go wrong.\n\n\nWhat is a good MAG?\n\n\nThe international genomics community has made an effort to define quality standards for MAGs via the \nMinimum Information about a Metagenome Assembled Genome (MIMAG)\n standards.\nThe basic idea is to require certain minimum levels of estimated completenes and maximal levels of estimated contamination in the MAG for it to be considered as one of \"Low-quality\", \"Medium-quality\", \"High-quality\", or \"Finished\". See \nTable 1\n in the above-linked paper for the full details.\nGenerally speaking it is hard to achieve \"High-quality\" and nearly impossible to get \"Finished\" with short read sequencing data alone.\nLong read metagenome data has been shown to produce results in these categories, although it can be difficult to obtain sufficient DNA for those methods, and they currently remain more expensive than short read sequencing.\n\n\nMaking MAGs\n\n\nThe process of reconstructing genomes from a metagenome is often referred to as \nmetagenome binning\n or just \nbinning\n, from the process of assigning contigs to one 'bin' per genome.\nThere are many binning tools available to extract MAGs from metagenome assemblies.\nWhen timeseries data is available, \nMetaBAT2\n is a good choice because it is both easy to use and offers good performance.\nMetaBAT2 can be installed via conda as follows:\n\n\nconda install -c bioconda metabat2\n\n\n\n\nMetaBAT2 input data\n\n\nAs input, MetaBAT2 requires reads from each of the shotgun metagenome samples to be mapped back to the metagenome assembly.\nWe will not cover the read mapping process in this tutorial, but it can be carried out using standard mapping software such as \nbwa mem\n or \nbowtie2\n.\nBe sure to sort and index the bam files with \nsamtools\n prior to running metabat.\nAssuming we have bam files of mapped reads and the metagenome assembly available in a directory called \n~/data\n we can compute genome bins as follows:\n\n\nmkdir ~/metabat ; cd ~/metabat\nrunMetaBat.sh ~/data/contigs-fixnames.fa ~/data/*.bam \n\n\n\n\nDepending on how much data you've got this can take a long time to compute.\nLuckily MetaBAT2 has a good parallel implementation so it can go faster if you run on a large multi-core machine.\nAt the end of the process MetaBAT2 will produce a directory called \ncontigs-fixnames.fa.metabat-bins\n, which contains one FastA file of contigs for each genome bin that it inferred.",
            "title": "Binning"
        },
        {
            "location": "/binning/#metagenome-assembled-genomes-mags-from-metagenome-binning",
            "text": "",
            "title": "Metagenome Assembled Genomes (MAGs) from metagenome binning"
        },
        {
            "location": "/binning/#what-is-a-mag",
            "text": "A MAG, or Metagenome Assembled Genome, is a genome that has been reconstructed from metagenomic data.\nBecause these genomes do not derive from a clonal culture they typically represent a consensus of the genomes of many closely related cells in one or more metagenomic samples, and for this reason they are sometimes called  population genomes .\nIt is important to remember that because of the way MAGs are inferred from the data the resulting sequence is usually fragmentary and may not accurately represent the genome of  any  cell in the community.\nIt is merely an average estimate, and there are well-known problems with taking averages, see for example the wikipedia page on  Simpson's paradox  to get an idea of how averages can go wrong.",
            "title": "What is a MAG?"
        },
        {
            "location": "/binning/#what-is-a-good-mag",
            "text": "The international genomics community has made an effort to define quality standards for MAGs via the  Minimum Information about a Metagenome Assembled Genome (MIMAG)  standards.\nThe basic idea is to require certain minimum levels of estimated completenes and maximal levels of estimated contamination in the MAG for it to be considered as one of \"Low-quality\", \"Medium-quality\", \"High-quality\", or \"Finished\". See  Table 1  in the above-linked paper for the full details.\nGenerally speaking it is hard to achieve \"High-quality\" and nearly impossible to get \"Finished\" with short read sequencing data alone.\nLong read metagenome data has been shown to produce results in these categories, although it can be difficult to obtain sufficient DNA for those methods, and they currently remain more expensive than short read sequencing.",
            "title": "What is a good MAG?"
        },
        {
            "location": "/binning/#making-mags",
            "text": "The process of reconstructing genomes from a metagenome is often referred to as  metagenome binning  or just  binning , from the process of assigning contigs to one 'bin' per genome.\nThere are many binning tools available to extract MAGs from metagenome assemblies.\nWhen timeseries data is available,  MetaBAT2  is a good choice because it is both easy to use and offers good performance.\nMetaBAT2 can be installed via conda as follows:  conda install -c bioconda metabat2  MetaBAT2 input data  As input, MetaBAT2 requires reads from each of the shotgun metagenome samples to be mapped back to the metagenome assembly.\nWe will not cover the read mapping process in this tutorial, but it can be carried out using standard mapping software such as  bwa mem  or  bowtie2 .\nBe sure to sort and index the bam files with  samtools  prior to running metabat.\nAssuming we have bam files of mapped reads and the metagenome assembly available in a directory called  ~/data  we can compute genome bins as follows:  mkdir ~/metabat ; cd ~/metabat\nrunMetaBat.sh ~/data/contigs-fixnames.fa ~/data/*.bam   Depending on how much data you've got this can take a long time to compute.\nLuckily MetaBAT2 has a good parallel implementation so it can go faster if you run on a large multi-core machine.\nAt the end of the process MetaBAT2 will produce a directory called  contigs-fixnames.fa.metabat-bins , which contains one FastA file of contigs for each genome bin that it inferred.",
            "title": "Making MAGs"
        },
        {
            "location": "/anvio/",
            "text": "Visualization of metagenome data\n\n\nIn this section we will explore ways to visualize the metagenomic data, with a view toward understanding the MAGs that we have reconstructed from our metagenomes.\nFor this we will use software called anvi'o, a highly versatile visualization and analysis environment for metagenomic (and pan-genomic) data.\nThe anvi'o software is extensively documented on \nthe anvi'o website\n, and rather than recapitulate all of that material here we will just go through the basic steps involved in getting our data loaded into anvi'o.\nFor further details on using the variety of features in anvi'o you can refer to the above site.\n\n\nInstalling anvi'o\n\n\nThe simplest way to install anvi'o is via conda:\n\n\nconda create -n anvio5 -c bioconda -c conda-forge anvio=5.5.0\n\n\n\n\nNotice this command is slightly different to our previous conda software installations, in this command we are invoking \nconda create\n which creates a new conda environment for anvi'o.\nOnce the installation has completed, we will need to activate the environment with \nconda activate anvio5\n in order to use anvi'o.\n\n\nPreparing data for anvi'o\n\n\nFirst we need to prepare our data for use in anvi'o.\nBecause we already have binning results from MetaBAT2 we will ask anvi'o to import those bins rather than computing new bins.\nThis will allow us to use anvi'o to browse the bins and interactively check and refine them as necessary.\nBut before we get to that, we need to get anvi'o to process the metagenome assembly contigs and the bam files of mapped reads:\n\n\nanvi-gen-contigs-database -f contigs-fixnames.fa -o contigs.db -n 'A gene school DB'\nanvi-run-hmms -c contigs.db\nfor bam in `ls *.bam`; do anvi-profile -i $bam -c contigs.db; done\nanvi-merge */PROFILE.db -o SAMPLES-MERGED -c contigs.db --skip-hierarchical-clustering\n\n\n\n\nThe above series of commands will take us from assembly contigs to a working anvi'o database, but there is a lot of compute along the way so if the data set is anything more than extremely trivial you will have to be \nvery patient\n.\nThe pig metagenome timeseries dataset we are using in this tutorial requires over hours of CPU time to process with the above commands.\nIf you have access to a large multicore machine (or large AWS instance) this process can be sped up by running many threads via the \n-T\n command-line parameter.\n\n\nNext, we need to create a file that will allow us to import our MetaBAT2 bins into anvi'o.\nThis is a pretty simple process:\n\n\ncd contigs-fixnames.fa.metabat-bins/\ngrep \">\" bin.*fa | perl -p -i -e \"s/bin\\.(.+).fa:>(.+)/\\$2\\tbin_\\$1/g\" > ../binning_results.txt\ncd ..\nanvi-import-collection binning_results.txt -p SAMPLES-MERGED/PROFILE.db -c contigs.db -C \"MetaBAT2\" --contigs-mode\nanvi-summarize -p SAMPLES-MERGED/PROFILE.db -c contigs.db -C MetaBAT2 -o MERGED_SUMMARY\n\n\n\n\nThe idea is to create a tab-delimited text file with two columns: the first is contig name and the second is the name of the bin that contig belongs to.\nThe command \ngrep \">\" bin.*fa\n pulls out the contig names from each FastA bin file, and pipes the result to this command \nperl -p -i -e \"s/(.+).fa:>(.+)/\\$2\\t\\$1/g\"\n which is using a perl regular expression to extract the contig name (in $2) and bin ID (in $1) and report them in two columns separated by the tab character \n\\t\n.\nThe results are saved in a file called \nbinning_results.txt\n.\nWe can then import the binning results into our anvi'o database with \nanvi-import-collection\n, and then compute some useful summaries of the bins with \nanvi-summarize\n.\n\n\nConnecting to an anvi'o server\n\n\nWhen used in interactive mode anvi'o is usually expected to be running locally on your own machine.\nHowever, we are working on VMs in the cloud and so we need to start up an anvi'o server on our remote machine and then connect to it with our web browser.\nNote that \nthis will only work with google chrome browser\n. \nanvi'o's interactive mode does not currently work with any other browsers. \nYou can try it of course but you're on your own when something goes wrong (which, in fact, could be said of almost anything in bioinformatics).\n\n\nanvi-interactive -p SAMPLES-MERGED/PROFILE.db -c contigs.db --server-only -P 8080 --password-protected -C MetaBAT2\n\n\n\n\nwhen anvi'o launches it will ask you to provide a password. \nMake one up, and be sure to choose one you can remember at least long enough to log into the server!\nOnce the server is running you can log into it via the chrome web browser by providing the IP address and port 8080 in the location bar, e.g. \nhttp://AA.BB.CC.DD:8080\n where AA.BB.CC.DD is the IP of your VM.\n\n\nRefining MAGs with anvi'o\n\n\nWhile the automated binning process implemented in MetaBAT2 is relatively easy to apply even to large datasets, these methods are not perfect and sometimes they can produce erroneous genome bins.\nanvi'o offers some useful data visualizations methods that can help us to identify cases where a MAG appears to have dubious features.\nThis might be especially relevant if there is a genome that is particularly relevant for your study, for example a nitrogen fixing organism associated with a plant.\nYou might want to confirm that the genome bin looks correct prior to metabolic analysis, for example, to predict culture conditions for isolating an organism.",
            "title": "Visualization"
        },
        {
            "location": "/anvio/#visualization-of-metagenome-data",
            "text": "In this section we will explore ways to visualize the metagenomic data, with a view toward understanding the MAGs that we have reconstructed from our metagenomes.\nFor this we will use software called anvi'o, a highly versatile visualization and analysis environment for metagenomic (and pan-genomic) data.\nThe anvi'o software is extensively documented on  the anvi'o website , and rather than recapitulate all of that material here we will just go through the basic steps involved in getting our data loaded into anvi'o.\nFor further details on using the variety of features in anvi'o you can refer to the above site.",
            "title": "Visualization of metagenome data"
        },
        {
            "location": "/anvio/#installing-anvio",
            "text": "The simplest way to install anvi'o is via conda:  conda create -n anvio5 -c bioconda -c conda-forge anvio=5.5.0  Notice this command is slightly different to our previous conda software installations, in this command we are invoking  conda create  which creates a new conda environment for anvi'o.\nOnce the installation has completed, we will need to activate the environment with  conda activate anvio5  in order to use anvi'o.",
            "title": "Installing anvi'o"
        },
        {
            "location": "/anvio/#preparing-data-for-anvio",
            "text": "First we need to prepare our data for use in anvi'o.\nBecause we already have binning results from MetaBAT2 we will ask anvi'o to import those bins rather than computing new bins.\nThis will allow us to use anvi'o to browse the bins and interactively check and refine them as necessary.\nBut before we get to that, we need to get anvi'o to process the metagenome assembly contigs and the bam files of mapped reads:  anvi-gen-contigs-database -f contigs-fixnames.fa -o contigs.db -n 'A gene school DB'\nanvi-run-hmms -c contigs.db\nfor bam in `ls *.bam`; do anvi-profile -i $bam -c contigs.db; done\nanvi-merge */PROFILE.db -o SAMPLES-MERGED -c contigs.db --skip-hierarchical-clustering  The above series of commands will take us from assembly contigs to a working anvi'o database, but there is a lot of compute along the way so if the data set is anything more than extremely trivial you will have to be  very patient .\nThe pig metagenome timeseries dataset we are using in this tutorial requires over hours of CPU time to process with the above commands.\nIf you have access to a large multicore machine (or large AWS instance) this process can be sped up by running many threads via the  -T  command-line parameter.  Next, we need to create a file that will allow us to import our MetaBAT2 bins into anvi'o.\nThis is a pretty simple process:  cd contigs-fixnames.fa.metabat-bins/\ngrep \">\" bin.*fa | perl -p -i -e \"s/bin\\.(.+).fa:>(.+)/\\$2\\tbin_\\$1/g\" > ../binning_results.txt\ncd ..\nanvi-import-collection binning_results.txt -p SAMPLES-MERGED/PROFILE.db -c contigs.db -C \"MetaBAT2\" --contigs-mode\nanvi-summarize -p SAMPLES-MERGED/PROFILE.db -c contigs.db -C MetaBAT2 -o MERGED_SUMMARY  The idea is to create a tab-delimited text file with two columns: the first is contig name and the second is the name of the bin that contig belongs to.\nThe command  grep \">\" bin.*fa  pulls out the contig names from each FastA bin file, and pipes the result to this command  perl -p -i -e \"s/(.+).fa:>(.+)/\\$2\\t\\$1/g\"  which is using a perl regular expression to extract the contig name (in $2) and bin ID (in $1) and report them in two columns separated by the tab character  \\t .\nThe results are saved in a file called  binning_results.txt .\nWe can then import the binning results into our anvi'o database with  anvi-import-collection , and then compute some useful summaries of the bins with  anvi-summarize .",
            "title": "Preparing data for anvi'o"
        },
        {
            "location": "/anvio/#connecting-to-an-anvio-server",
            "text": "When used in interactive mode anvi'o is usually expected to be running locally on your own machine.\nHowever, we are working on VMs in the cloud and so we need to start up an anvi'o server on our remote machine and then connect to it with our web browser.\nNote that  this will only work with google chrome browser . \nanvi'o's interactive mode does not currently work with any other browsers. \nYou can try it of course but you're on your own when something goes wrong (which, in fact, could be said of almost anything in bioinformatics).  anvi-interactive -p SAMPLES-MERGED/PROFILE.db -c contigs.db --server-only -P 8080 --password-protected -C MetaBAT2  when anvi'o launches it will ask you to provide a password. \nMake one up, and be sure to choose one you can remember at least long enough to log into the server!\nOnce the server is running you can log into it via the chrome web browser by providing the IP address and port 8080 in the location bar, e.g.  http://AA.BB.CC.DD:8080  where AA.BB.CC.DD is the IP of your VM.",
            "title": "Connecting to an anvi'o server"
        },
        {
            "location": "/anvio/#refining-mags-with-anvio",
            "text": "While the automated binning process implemented in MetaBAT2 is relatively easy to apply even to large datasets, these methods are not perfect and sometimes they can produce erroneous genome bins.\nanvi'o offers some useful data visualizations methods that can help us to identify cases where a MAG appears to have dubious features.\nThis might be especially relevant if there is a genome that is particularly relevant for your study, for example a nitrogen fixing organism associated with a plant.\nYou might want to confirm that the genome bin looks correct prior to metabolic analysis, for example, to predict culture conditions for isolating an organism.",
            "title": "Refining MAGs with anvi'o"
        },
        {
            "location": "/hic/",
            "text": "Metagenomic HiC\n\n\nWhat is Metagenomic HiC?\n\n\nAs the name suggest, Metagenomic Hi-C incorporates a new form of sequencing datas, named Hi-C, in to traditional shotgun metagenomes. In doing so, modern analyses, such as genome binning, can be performed accurately and precisely using only a single time-point.\n\n\nThe Hi-C protocol was designed to capture genome-wide evidence of DNA molecules in close physical proximity \nin vivo\n, prior to cellular lysis. With the flexible nature of a chromosome and its capacity to bend back on itself, the most frequently captured interactions are usually those between loci on the same chromosome. After this, the most frequently observed interactions are inter-chromosomal. Lastly come inter-cellular interactions, which are often far below the rates of those found within a cell.\n\n\nThe original purpose of Hi-C was to study the 3-dimensional conformation of the human genome [1], but the proximity information can just as well be exploited to infer which DNA assembly fragments belong together in a bin.  Depending on the subject of study, the implied precision of \"together\" can range from a genome to chromosome. For metagenomic studies, methods currently aim to associate assembly fragments believed to come from the same genome.\n\n\nThe Hi-C library protocol\n\n\nHi-C data-sets are generated using conventional high-throughput Illumina paired-end sequencing; the difference lies in the library protocol.\n\n\n\n\n\n\n:camera: \nFig 1\n Major steps of the Hi-C protocol\n\n{.center .small}\n\n\n\n\n\n\nProtocol outline\n\n\n\n\n\n\nDNA Fixation:\n Beginning with intact cells, the first step of the Hi-C protocol is formalin fixation. The act of cross-linking freezes close-by conformation arrangements within the DNA that existed in the cells at the time of fixation.\n\n\n\n\n\n\nCell Lysis:\n The cells are lysed and DNA-protein complexes extracted and purified.\n\n\n\n\n\n\nRestriction Digest:\n The DNA is digested using a restriction endonuclease. In metagenomic Hi-C, enzymes with large overhangs and 4-nt recognition sites are common choices (i.e. Sau3AI, MluCI, DpnII). Differences in the GC bias of the recognition site and the target DNA is an important factor, as inefficient digestion will produce fewer Hi-C read-pairs.\n\n\n\n\n\n\nBiotin Tagging:\n The overhangs produced during digestion are end-filled with biotinylated nucleotides.\n\n\n\n\n\n\nFree-end Ligation:\n in dilute conditions or immobilised on a substrate, free-ends protruding from the DNA-protein complexes are ligated. This stochastic process favours any two ends which were nearby within the complex, however random ligation and self-ligation can occur and are non-informative.\n\n\n\n\n\n\nCrosslink Reversal:\n The formalin fixation is reversed, allowing the now free DNA to be purified.\n\n\n\n\n\n\nUn-ligated End Clean-up:\n Free-ends which failed to form ligation products are unwanted in subsequent steps but could still be biotin tagged. To minimise their inclusion, a light exonuclease 3' to 5' favoured chew-back can be applied.\n\n\n\n\n\n\nDNA Sheering:\n With ligation completed, the DNA is mechanically sheered and the size range of the resulting fragment selected suitability with Illumina paired-end sequencing. \n\n\n\n\n\n\nDNA repair:\n Sonication can lead to damaged ends and nicks, which can be repaired.\n\n\n\n\n\n\nProximity Ligation Enrichment:\n Biotin tagged fragments are pulled down using affinity purification. \n\n\n\n\n\n\nAdpater Ligation:\n Illumina paired-end adapter ligation and associated steps to produce a sequencing library are now applied.\n\n\n\n\n\n\nQC: is my library ok?\n\n\nMetagenome binning with HiC data\n\n\nReferences\n\n\n\n\nLieberman-Aiden, E., van Berkum, N. L., Williams, L., Imakaev, M., Ragoczy, T., Telling, A., \u2026 Dekker, J. (2009). Comprehensive mapping of long-range interactions reveals folding principles of the human genome. \nScience\n, 326(5950), 289\u2013293. https://doi.org/10.1126/science.1181369",
            "title": "HiC"
        },
        {
            "location": "/hic/#metagenomic-hic",
            "text": "",
            "title": "Metagenomic HiC"
        },
        {
            "location": "/hic/#what-is-metagenomic-hic",
            "text": "As the name suggest, Metagenomic Hi-C incorporates a new form of sequencing datas, named Hi-C, in to traditional shotgun metagenomes. In doing so, modern analyses, such as genome binning, can be performed accurately and precisely using only a single time-point.  The Hi-C protocol was designed to capture genome-wide evidence of DNA molecules in close physical proximity  in vivo , prior to cellular lysis. With the flexible nature of a chromosome and its capacity to bend back on itself, the most frequently captured interactions are usually those between loci on the same chromosome. After this, the most frequently observed interactions are inter-chromosomal. Lastly come inter-cellular interactions, which are often far below the rates of those found within a cell.  The original purpose of Hi-C was to study the 3-dimensional conformation of the human genome [1], but the proximity information can just as well be exploited to infer which DNA assembly fragments belong together in a bin.  Depending on the subject of study, the implied precision of \"together\" can range from a genome to chromosome. For metagenomic studies, methods currently aim to associate assembly fragments believed to come from the same genome.",
            "title": "What is Metagenomic HiC?"
        },
        {
            "location": "/hic/#the-hi-c-library-protocol",
            "text": "Hi-C data-sets are generated using conventional high-throughput Illumina paired-end sequencing; the difference lies in the library protocol.",
            "title": "The Hi-C library protocol"
        },
        {
            "location": "/hic/#qc-is-my-library-ok",
            "text": "",
            "title": "QC: is my library ok?"
        },
        {
            "location": "/hic/#metagenome-binning-with-hic-data",
            "text": "",
            "title": "Metagenome binning with HiC data"
        },
        {
            "location": "/hic/#references",
            "text": "Lieberman-Aiden, E., van Berkum, N. L., Williams, L., Imakaev, M., Ragoczy, T., Telling, A., \u2026 Dekker, J. (2009). Comprehensive mapping of long-range interactions reveals folding principles of the human genome.  Science , 326(5950), 289\u2013293. https://doi.org/10.1126/science.1181369",
            "title": "References"
        },
        {
            "location": "/everythingelse/",
            "text": "What we didn't cover\n\n\nExperimental design\n\n\nThere are an almost endless array of questions that can be addressed with metagenomics and in this tutorial material we have only begun to scratch the surface of what is possible.\nAs a research tool, metagenomics is now beginning to move away from being purely discovery-oriented towards becoming a tool that is used to test specific hypotheses.\nThere are many, many things to consider when designing an experiment that involves metagenomics as a means to test a hypothesis.\nFirst among these is whether the effect would be measurable via metagenomic data, e.g. is metagenomics the right tool for the job?\nThen comes questions like:\n\n\n\n\nHow deeply will you sequence?\n\n\nHow many samples will you need?\n\n\nWill samples be structured as a time-series, or transect of some kind?\n\n\nHow much background variation exists in the metagenomes that will be sampled?\n\n\nAnd based on the above, using a particular set of metagenome analysis tools, what is our statistical power to reject the hypothesis in question?\n\n\n\n\nSimulation for experimental design\n\n\nOne way to design and power a large metagenomic experiment is to carry out a computational simulation of the metagenomic sequencing that follows the experimental structure.\nWhile this may seem tedious, I would argue that if the experiment is worth doing, then it is almost certainly worth doing a simulation study first.\nThere are a number of advantages to this kind of an approach.\nFor example:\n\n\n\n\nHidden obstacles to the data analysis will be identified \na priori\n, before the long and expensive experimental work and data generation begins.\n\n\nA data analysis workflow can be developed up front, so that when the data arrives the work to analyse it becomes simpler.\n\n\nThe experiment can be rationally designed, with clear expectations about the number and type of samples required to measure an effect of a particular size.\n\n\n\n\nOne of the challenges to taking this approach is that reasonable simulation parameters may not be known.\nIf you're working in a well-studied microbial ecosystem like the human gut it may be possible to design an experiment entirely using knowledge gained from existing public datasets.\nBut if you're working in a more obscure system then there may not be much, if any, public data to use for experimental design.\nIn general the solution to this problem requires an iterative process, in which an initial small batch of pilot data is created, from which reasonable parameters can be estimated for the design of a larger study.\n\n\nResolving strain mixtures\n\n\nWhen a metagenomic sample contains genomes from more than one strain of the same species, or from two closely related species, it can cause the assembly to become highly fragmentary. \nThe resulting assembly contigs can be very difficult to analyse with standard genome binning methods, for multiple reasons.\nFirst, many binning tools will not process contigs if they are below a particular size.\nDepending on the exact level of sequence identity among the two strains, only a small fraction of the genome might be present in contigs above the lower size limit.\nSecond, some of the contigs can represent a coassembly of the two strains.\nNot only can this result in contig sequences that look unlike any of the individual genomes, but it also poses a problem for standard metagenome binning software because the contig belongs in more than one bin, yet these softwares can only assign contigs to single bins.\nWhile there has been some work to try to resolve strain mixtures from metagenome assemblies, such as that implemented in the \nDESMAN software\n, the problem remains challenging and better solutions are needed.\n\n\nAlpha and beta diversity, ecological networks\n\n\nAlthough people usually carry out 16S amplicon sequencing to answer questions about microbial ecology it is also possible to address these with metagenomic data.\nThere are a range of computational methods and tools for taxonomy-driven metagenome community profiling.\nSome of these were reviewed and evaluated in the \nCAMI publication\n.\nThe taxonomy-based methods are limited, however, in that they can only resolve named taxonomic groups.\nMuch of the microbial diversity on our planet remains undescribed, and this is where phylogenetic methods have the potential to greatly outperform the taxonomic methods.\nPhylogenetic methods don't depend on human-curated taxonomic structures, and therefore are capable of analysing wholly novel groups of organisms.\nThe equivalent class of methods in the 16S amplicon sequencing world are the reference-free \nde novo\n OTU or Amplicon Sequence Variant (ASV) analysis methods.\n\n\nThe list goes on\n\n\nThere are many, many more ways to analyze metagenomic data. \nOnce MAGs have been reconstructed from the metagenomes pretty much any of the analyses that could be carried out on isolate genomes can be applied.\nThere are also a range of metagenome-specific analyses that could be applied to MAGs, such as association studies, phylogenetic studies, functional and metabolic analyses, and more.",
            "title": "Everything Else"
        },
        {
            "location": "/everythingelse/#what-we-didnt-cover",
            "text": "",
            "title": "What we didn't cover"
        },
        {
            "location": "/everythingelse/#experimental-design",
            "text": "There are an almost endless array of questions that can be addressed with metagenomics and in this tutorial material we have only begun to scratch the surface of what is possible.\nAs a research tool, metagenomics is now beginning to move away from being purely discovery-oriented towards becoming a tool that is used to test specific hypotheses.\nThere are many, many things to consider when designing an experiment that involves metagenomics as a means to test a hypothesis.\nFirst among these is whether the effect would be measurable via metagenomic data, e.g. is metagenomics the right tool for the job?\nThen comes questions like:   How deeply will you sequence?  How many samples will you need?  Will samples be structured as a time-series, or transect of some kind?  How much background variation exists in the metagenomes that will be sampled?  And based on the above, using a particular set of metagenome analysis tools, what is our statistical power to reject the hypothesis in question?   Simulation for experimental design  One way to design and power a large metagenomic experiment is to carry out a computational simulation of the metagenomic sequencing that follows the experimental structure.\nWhile this may seem tedious, I would argue that if the experiment is worth doing, then it is almost certainly worth doing a simulation study first.\nThere are a number of advantages to this kind of an approach.\nFor example:   Hidden obstacles to the data analysis will be identified  a priori , before the long and expensive experimental work and data generation begins.  A data analysis workflow can be developed up front, so that when the data arrives the work to analyse it becomes simpler.  The experiment can be rationally designed, with clear expectations about the number and type of samples required to measure an effect of a particular size.   One of the challenges to taking this approach is that reasonable simulation parameters may not be known.\nIf you're working in a well-studied microbial ecosystem like the human gut it may be possible to design an experiment entirely using knowledge gained from existing public datasets.\nBut if you're working in a more obscure system then there may not be much, if any, public data to use for experimental design.\nIn general the solution to this problem requires an iterative process, in which an initial small batch of pilot data is created, from which reasonable parameters can be estimated for the design of a larger study.",
            "title": "Experimental design"
        },
        {
            "location": "/everythingelse/#resolving-strain-mixtures",
            "text": "When a metagenomic sample contains genomes from more than one strain of the same species, or from two closely related species, it can cause the assembly to become highly fragmentary. \nThe resulting assembly contigs can be very difficult to analyse with standard genome binning methods, for multiple reasons.\nFirst, many binning tools will not process contigs if they are below a particular size.\nDepending on the exact level of sequence identity among the two strains, only a small fraction of the genome might be present in contigs above the lower size limit.\nSecond, some of the contigs can represent a coassembly of the two strains.\nNot only can this result in contig sequences that look unlike any of the individual genomes, but it also poses a problem for standard metagenome binning software because the contig belongs in more than one bin, yet these softwares can only assign contigs to single bins.\nWhile there has been some work to try to resolve strain mixtures from metagenome assemblies, such as that implemented in the  DESMAN software , the problem remains challenging and better solutions are needed.",
            "title": "Resolving strain mixtures"
        },
        {
            "location": "/everythingelse/#alpha-and-beta-diversity-ecological-networks",
            "text": "Although people usually carry out 16S amplicon sequencing to answer questions about microbial ecology it is also possible to address these with metagenomic data.\nThere are a range of computational methods and tools for taxonomy-driven metagenome community profiling.\nSome of these were reviewed and evaluated in the  CAMI publication .\nThe taxonomy-based methods are limited, however, in that they can only resolve named taxonomic groups.\nMuch of the microbial diversity on our planet remains undescribed, and this is where phylogenetic methods have the potential to greatly outperform the taxonomic methods.\nPhylogenetic methods don't depend on human-curated taxonomic structures, and therefore are capable of analysing wholly novel groups of organisms.\nThe equivalent class of methods in the 16S amplicon sequencing world are the reference-free  de novo  OTU or Amplicon Sequence Variant (ASV) analysis methods.",
            "title": "Alpha and beta diversity, ecological networks"
        },
        {
            "location": "/everythingelse/#the-list-goes-on",
            "text": "There are many, many more ways to analyze metagenomic data. \nOnce MAGs have been reconstructed from the metagenomes pretty much any of the analyses that could be carried out on isolate genomes can be applied.\nThere are also a range of metagenome-specific analyses that could be applied to MAGs, such as association studies, phylogenetic studies, functional and metabolic analyses, and more.",
            "title": "The list goes on"
        }
    ]
}