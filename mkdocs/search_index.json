{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to the Metagenome Tutorial\n\n\nThese tutorials describe some current approaches for metagenome analysis using a genome-resolved approach.\n\n\nAuthors\n\n\n\n\nAaron E Darling\n\n\nMatt Z DeMaere",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-the-metagenome-tutorial",
            "text": "These tutorials describe some current approaches for metagenome analysis using a genome-resolved approach.",
            "title": "Welcome to the Metagenome Tutorial"
        },
        {
            "location": "/#authors",
            "text": "Aaron E Darling  Matt Z DeMaere",
            "title": "Authors"
        },
        {
            "location": "/startup/",
            "text": "Setting up a working environment (Jupyter)\n\n\nIn these tutorials we will work in a Jupyter notebook server environment.\nJupyter is a versatile web interface system for data analysis, supporting work in languages such as Python and R, as well as the unix command-line.\nIf you are working through this tutorial in one of The Gene School workshops we will provide a pre-configured Jupyter server for you to use and there is no need to carry out the setup process described on this page.\n\n\nLaunch a server via Amazon Web Services EC2\n\n\nFirst, either log in or sign up for AWS and navigate to to the EC2 section.\nThen select the option to launch a new instance. We will use the Ubuntu 18.04 LTS base image in this example.\nAs you continue to click through the instance configuration details, there are two important settings that need to be changed from defaults: (1) the disk volume size, which should be set to 500GB or more to give ample space for metagenomic datasets (note that this will incur costs), and (2) the security groups settings. In the security groups, you will need to add access to TCP port 8888 for the IP address from which you'll be connecting to the servers. If you don't know your source IP range then this can be set to \n0.0.0.0/0\n but note the that this will enable the whole world to connect to the server and there is a security risk, even if the Jupyter server is password or token protected.\n\n\nIf this is the first time you've used AWS you will need to generate and save an ssh key that will be used to log into the VMs. Keep this in a safe place. It will need to be made read only to the user account before it can be used with ssh, e.g. \nchmod 400 my_aws_key.pem\n\n\nLog in and launch Jupyter\n\n\nHaving started up an AWS instance you are now ready to log in and install Jupyter.\nThe process is fairly straightforward. First ssh into the instance: \nssh -i my_aws_key.pem ubuntu@XX.XX.XX.XX\n where XX.XX.XX.XX is the public IP address shown in the AWS EC2 console for the VM you have launched.\nOnce logged in there are a few steps to installing the Jupyter server. First download and install anaconda:\n\n\nwget https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh\nbash Anaconda3-2018.12-Linux-x86_64.sh\n\n\n\n\nBe sure to answer \nyes\n to the license question and the question about installing the configuration to the \n.bashrc\n file.\n\n\nNow we're ready to launch a jupyter server:\n\n\nsource .bashrc\njupyter lab --no-browser --ip 0.0.0.0 --LabApp.token=112233445566778899\n\n\n\n\nIMPORTANT: replace the string of numbers \n112233445566778899\n with your own string -- this is the secret key that will allow you (and only you) to connect to your jupyter server and run commands, so you want something that neither human nor robot will guess. Hexadecimal (lowercase) values are ok here too.\n\n\nConnect to the Jupyter web interface\n\n\nWe're finally ready to connect to the web interface. To do so simply point your browser at \nXX.XX.XX.XX:8888\n where XX.XX.XX.XX is again the public IP address of the VM that you've launched in AWS. 8888 is the TCP port number that Jupyter listens on by default, and we added a special security rule to open this port when creating the VM in AWS (remember?). If you missed that step, don't worry, it's possible to go into the EC2 dashboard and update the security settings to open port 8888. Assumming everything has worked you'll arrive at a Jupyter page asking for the security token. This is where you provide the super secret number that you selected above. And that's it, you're ready to use Jupyter!",
            "title": "Jupyter"
        },
        {
            "location": "/startup/#setting-up-a-working-environment-jupyter",
            "text": "In these tutorials we will work in a Jupyter notebook server environment.\nJupyter is a versatile web interface system for data analysis, supporting work in languages such as Python and R, as well as the unix command-line.\nIf you are working through this tutorial in one of The Gene School workshops we will provide a pre-configured Jupyter server for you to use and there is no need to carry out the setup process described on this page.",
            "title": "Setting up a working environment (Jupyter)"
        },
        {
            "location": "/startup/#launch-a-server-via-amazon-web-services-ec2",
            "text": "First, either log in or sign up for AWS and navigate to to the EC2 section.\nThen select the option to launch a new instance. We will use the Ubuntu 18.04 LTS base image in this example.\nAs you continue to click through the instance configuration details, there are two important settings that need to be changed from defaults: (1) the disk volume size, which should be set to 500GB or more to give ample space for metagenomic datasets (note that this will incur costs), and (2) the security groups settings. In the security groups, you will need to add access to TCP port 8888 for the IP address from which you'll be connecting to the servers. If you don't know your source IP range then this can be set to  0.0.0.0/0  but note the that this will enable the whole world to connect to the server and there is a security risk, even if the Jupyter server is password or token protected.  If this is the first time you've used AWS you will need to generate and save an ssh key that will be used to log into the VMs. Keep this in a safe place. It will need to be made read only to the user account before it can be used with ssh, e.g.  chmod 400 my_aws_key.pem",
            "title": "Launch a server via Amazon Web Services EC2"
        },
        {
            "location": "/startup/#log-in-and-launch-jupyter",
            "text": "Having started up an AWS instance you are now ready to log in and install Jupyter.\nThe process is fairly straightforward. First ssh into the instance:  ssh -i my_aws_key.pem ubuntu@XX.XX.XX.XX  where XX.XX.XX.XX is the public IP address shown in the AWS EC2 console for the VM you have launched.\nOnce logged in there are a few steps to installing the Jupyter server. First download and install anaconda:  wget https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh\nbash Anaconda3-2018.12-Linux-x86_64.sh  Be sure to answer  yes  to the license question and the question about installing the configuration to the  .bashrc  file.  Now we're ready to launch a jupyter server:  source .bashrc\njupyter lab --no-browser --ip 0.0.0.0 --LabApp.token=112233445566778899  IMPORTANT: replace the string of numbers  112233445566778899  with your own string -- this is the secret key that will allow you (and only you) to connect to your jupyter server and run commands, so you want something that neither human nor robot will guess. Hexadecimal (lowercase) values are ok here too.",
            "title": "Log in and launch Jupyter"
        },
        {
            "location": "/startup/#connect-to-the-jupyter-web-interface",
            "text": "We're finally ready to connect to the web interface. To do so simply point your browser at  XX.XX.XX.XX:8888  where XX.XX.XX.XX is again the public IP address of the VM that you've launched in AWS. 8888 is the TCP port number that Jupyter listens on by default, and we added a special security rule to open this port when creating the VM in AWS (remember?). If you missed that step, don't worry, it's possible to go into the EC2 dashboard and update the security settings to open port 8888. Assumming everything has worked you'll arrive at a Jupyter page asking for the security token. This is where you provide the super secret number that you selected above. And that's it, you're ready to use Jupyter!",
            "title": "Connect to the Jupyter web interface"
        },
        {
            "location": "/qc/",
            "text": "Sequencing run QC\n\n\nGet some sequence data\n\n\nThe first thing we'll do is to get some sequence data to work with.\nPublished sequence data is usually archived in the NCBI SRA, the ENA, and the DDBJ.\nThese databases provide a \nconvenient interface to search\n the descriptions of samples by keyword, and by sample type (e.g. shotgun metagenome).\nFor the following exercises we'll use a set of small datasets (SRA accessions SRR9323808, SRR9323810, SRR9323811, SRR9323809) which will be quick to process because they are small.\nThe easiest way to download data from these databases is via the \nfastq-dump\n software.\nFirst, let's install the parallel version of fastq-dump using conda.\nTo do this, start a terminal session in your Jupyter server (click the Terminal icon) and run the following command (ok to copy and paste):\n\n\nconda install -c bioconda parallel-fastq-dump \n\n\n\n\nNow that you've installed fastq-dump we can use it to download data by accession number. Copy and paste the following to your terminal:\n\n\nparallel-fastq-dump -s SRR9323808 -s SRR9323810 -s SRR9323811 -s SRR9323809 --threads 4 --outdir qc_data/ --split-files --gzip\n\n\n\n\nIf the download was successful you should see something like the following on your terminal:\n\n\n\n\nEvaluating sequence quality with FastQC and MultiQC\n\n\nThe very first thing one would normally do when working with a new dataset is to look at some basic quality statistics on the data.\nThere are many, many tools available to compute quality metrics. For our current data we will use the FastQC software, applied to each sample, and then combine the results using MultiQC to a single report. First step is to install \nfastqc\n and \nmultiqc\n.\n\n\nconda install -c bioconda fastqc\npip install multiqc\n\n\n\n\nIn the above we've used conda to install fastqc, but we've used another way to install multiqc -- something called \npip\n. pip is an installer for python programs, and like conda it will download and install the software along with any dependencies. The reason we use pip in this case is because conda can be very, very slow to install some programs and in this case pip is much faster.\n\n\ncd qc_data\nfind . -name \"*.fastq.gz\" -exec fastqc {} \\;\n\n\n\n\nLet's unpack those commands a bit. The first part, \ncd qc_data\n just changes the current directory to qc_data, so any following commands run will run in that directory.\nThe next command is \nfind . -name \"*.fastq.gz\" -exec fastqc {} \\;\n. The first part, \nfind\n is a command that finds files. If you just run \nfind .\n it will find all the files in and below the current directory (the \n.\n part specifies to look in the current directory). The next part, \n-name \"*.fastq.gz\"\n tells \nfind\n that we only want it to find files with names that end with \n.fastq.gz\n. The \n*\n is a wildcard that matches anything. Finally, the last part \n-exec fastqc {} \\;\n tells \nfind\n that whenever it finds a file, it should run \nfastqc\n on that file, putting the name of the file where the \n{}\n are.\n\n\nIf this step has worked, then you should have several new \n.zip\n files containing the QC data in that directory, along with some html files. When we have a lot of samples it is too tedious to look at all the QC reports individually for each sample, so we can summarize them using multiqc:\n\n\nmultiqc .\n\n\n\n\nAt this point a multiqc file will appear inside the QC directory. First double click to open the QC folder.\n\n\n\nOnce that's open a file called \nmultiqc_report.html\n will appear in the listing. \nWe can open this file from within our jupyter browser environment and inspect it.\nTo open it, we need to right click (or two-finger tap) on the file name to get a context menu that will give several options for how to open it. It looks like this:\n\n\n\n\nClick the option to \"Open in a New Browser Tab\". From here we can evaluate the quality of the libraries.\n\n\nTaxonomic analysis\n\n\nMetagenome taxonomic analysis offers a means to estimate a microbial community profile from metagenomic sequence data.\n\n\nTaxonomic analysis with Kraken2\n\n\nconda install -c bioconda kraken2\n\n\n\n\nTaxonomic analysis with Metaphlan2\n\n\nWhile it may be possible to install metaphlan2 via conda, at least in my experience, conda struggles with \"solving the environment\".\nAnother way to install it if you're using a debian-derived Linux (e.g. ubuntu) is via apt-get, thanks to the debian-med team who have packaged it:\n\n\nsudo apt-get install metaphlan2\n\n\n\n\nThe install can be quite slow because it needs to create a bowtie index of the database of marker gene sequences that metaphlan2 uses. It also requires a fair bit of RAM, so ensure your machine has at least 8G RAM before trying to install this.\n\n\nTaxonomic analysis with other tools\n\n\nThere are a whole range of other software tools available for metagenome taxonomic analysis. \nThey all have strengths and weaknesses.\n\n\nA few other commonly used tools are listed here:\n\n\n\n\nMEGAN\n\n\nCentrifuge\n\n\nCLARK\n\n\n\n\nEvaluating the host genomic content\n\n\nThe above samples were sourced from a pig gut. Each one of the samples was taken from a different part of the pig gut, including the duodenum, jejunum, ileum, colon, and caecum.\n\n\nIf we were to design a large study around these samples which of them would be suitable for metagenomics, and why?\n\n\nA note on negative controls\n\n\nNegative controls are a key element in any microbiome profiling or metagenome analysis work.\nEvery aspect of the sample collection and processing can be impacted by the presence of contaminating microbial DNA.\nThis is true even of 'sterile' material -- just because no viable cells exist in a sample collection swab, for example, does not mean there is no microbial DNA on that swab.\nIt is well known that molecular biology reagents frequently contain contaminating DNA.\nUsually it is at low levels, but this is not always the case.\n\n\nTherefore, the best practice is to collect multiple negative control samples that are taken all the way through sequencing.\nThese negative controls can then be used to correct for contamination artifacts in the remaining samples.",
            "title": "QC"
        },
        {
            "location": "/qc/#sequencing-run-qc",
            "text": "",
            "title": "Sequencing run QC"
        },
        {
            "location": "/qc/#get-some-sequence-data",
            "text": "The first thing we'll do is to get some sequence data to work with.\nPublished sequence data is usually archived in the NCBI SRA, the ENA, and the DDBJ.\nThese databases provide a  convenient interface to search  the descriptions of samples by keyword, and by sample type (e.g. shotgun metagenome).\nFor the following exercises we'll use a set of small datasets (SRA accessions SRR9323808, SRR9323810, SRR9323811, SRR9323809) which will be quick to process because they are small.\nThe easiest way to download data from these databases is via the  fastq-dump  software.\nFirst, let's install the parallel version of fastq-dump using conda.\nTo do this, start a terminal session in your Jupyter server (click the Terminal icon) and run the following command (ok to copy and paste):  conda install -c bioconda parallel-fastq-dump   Now that you've installed fastq-dump we can use it to download data by accession number. Copy and paste the following to your terminal:  parallel-fastq-dump -s SRR9323808 -s SRR9323810 -s SRR9323811 -s SRR9323809 --threads 4 --outdir qc_data/ --split-files --gzip  If the download was successful you should see something like the following on your terminal:",
            "title": "Get some sequence data"
        },
        {
            "location": "/qc/#evaluating-sequence-quality-with-fastqc-and-multiqc",
            "text": "The very first thing one would normally do when working with a new dataset is to look at some basic quality statistics on the data.\nThere are many, many tools available to compute quality metrics. For our current data we will use the FastQC software, applied to each sample, and then combine the results using MultiQC to a single report. First step is to install  fastqc  and  multiqc .  conda install -c bioconda fastqc\npip install multiqc  In the above we've used conda to install fastqc, but we've used another way to install multiqc -- something called  pip . pip is an installer for python programs, and like conda it will download and install the software along with any dependencies. The reason we use pip in this case is because conda can be very, very slow to install some programs and in this case pip is much faster.  cd qc_data\nfind . -name \"*.fastq.gz\" -exec fastqc {} \\;  Let's unpack those commands a bit. The first part,  cd qc_data  just changes the current directory to qc_data, so any following commands run will run in that directory.\nThe next command is  find . -name \"*.fastq.gz\" -exec fastqc {} \\; . The first part,  find  is a command that finds files. If you just run  find .  it will find all the files in and below the current directory (the  .  part specifies to look in the current directory). The next part,  -name \"*.fastq.gz\"  tells  find  that we only want it to find files with names that end with  .fastq.gz . The  *  is a wildcard that matches anything. Finally, the last part  -exec fastqc {} \\;  tells  find  that whenever it finds a file, it should run  fastqc  on that file, putting the name of the file where the  {}  are.  If this step has worked, then you should have several new  .zip  files containing the QC data in that directory, along with some html files. When we have a lot of samples it is too tedious to look at all the QC reports individually for each sample, so we can summarize them using multiqc:  multiqc .  At this point a multiqc file will appear inside the QC directory. First double click to open the QC folder.  Once that's open a file called  multiqc_report.html  will appear in the listing. \nWe can open this file from within our jupyter browser environment and inspect it.\nTo open it, we need to right click (or two-finger tap) on the file name to get a context menu that will give several options for how to open it. It looks like this:   Click the option to \"Open in a New Browser Tab\". From here we can evaluate the quality of the libraries.",
            "title": "Evaluating sequence quality with FastQC and MultiQC"
        },
        {
            "location": "/qc/#taxonomic-analysis",
            "text": "Metagenome taxonomic analysis offers a means to estimate a microbial community profile from metagenomic sequence data.  Taxonomic analysis with Kraken2  conda install -c bioconda kraken2  Taxonomic analysis with Metaphlan2  While it may be possible to install metaphlan2 via conda, at least in my experience, conda struggles with \"solving the environment\".\nAnother way to install it if you're using a debian-derived Linux (e.g. ubuntu) is via apt-get, thanks to the debian-med team who have packaged it:  sudo apt-get install metaphlan2  The install can be quite slow because it needs to create a bowtie index of the database of marker gene sequences that metaphlan2 uses. It also requires a fair bit of RAM, so ensure your machine has at least 8G RAM before trying to install this.  Taxonomic analysis with other tools  There are a whole range of other software tools available for metagenome taxonomic analysis. \nThey all have strengths and weaknesses.  A few other commonly used tools are listed here:   MEGAN  Centrifuge  CLARK   Evaluating the host genomic content  The above samples were sourced from a pig gut. Each one of the samples was taken from a different part of the pig gut, including the duodenum, jejunum, ileum, colon, and caecum.  If we were to design a large study around these samples which of them would be suitable for metagenomics, and why?",
            "title": "Taxonomic analysis"
        },
        {
            "location": "/qc/#a-note-on-negative-controls",
            "text": "Negative controls are a key element in any microbiome profiling or metagenome analysis work.\nEvery aspect of the sample collection and processing can be impacted by the presence of contaminating microbial DNA.\nThis is true even of 'sterile' material -- just because no viable cells exist in a sample collection swab, for example, does not mean there is no microbial DNA on that swab.\nIt is well known that molecular biology reagents frequently contain contaminating DNA.\nUsually it is at low levels, but this is not always the case.  Therefore, the best practice is to collect multiple negative control samples that are taken all the way through sequencing.\nThese negative controls can then be used to correct for contamination artifacts in the remaining samples.",
            "title": "A note on negative controls"
        },
        {
            "location": "/assembly/",
            "text": "Metagenome assembly\n\n\nWhat is assembly?\n\n\nAssembly is the process of reconstructing long DNA sequences from a collection of overlapping shorter pieces.\nIn the context of single genomes (e.g. microbial isolates) the goal is usually to reconstruct a single chromosome and any associated plasmids or other replicons.\nWhen presented with metagenomic data though, the microbial community might in reality have millions of genomes present in it.\nGenerally speaking, it is impossible to accurately reconstruct every genome of every cell in a metagenomic sample with current technology.\nInstead we usually aim to reconstruct chromosome sequences that represent a consensus of many cells.\n\n\nAssembling metagenomes\n\n\nThere are several available metagenome assembly tools. \nSome examples include metaSPAdes and MEGAHIT.\nIn this tutorial we will use MEGAHIT.\n\n\nsingularity run docker://bioboxes/megahit:latest \n\n\n\n\n\nCoassembly or single sample?\n\n\nJust as important as deciding what assembler to use is deciding on \nwhat to assemble\n.\nOne approach is to assemble all of the metagenome samples together.\nThis can be useful if there is not much data per sample, or if low abundance organisms exist in the samples that would not have enough coverage in a single sample to be assembled.\n\n\nLimitations of metagenome assembly\n\n\nCurrent assembly methods are limited in their ability to resolve chromosomes of closely related species and strains in a sample.\nThis is because most assembly methods collapse sequences > 95% or 98% identity into a single contig sequence, usually as a means to cope with sequencing error.\nTherefore as a consensus representation, these assembled chromosome sequences may mask fine-scale genetic variation in the population.",
            "title": "Assembly"
        },
        {
            "location": "/assembly/#metagenome-assembly",
            "text": "",
            "title": "Metagenome assembly"
        },
        {
            "location": "/assembly/#what-is-assembly",
            "text": "Assembly is the process of reconstructing long DNA sequences from a collection of overlapping shorter pieces.\nIn the context of single genomes (e.g. microbial isolates) the goal is usually to reconstruct a single chromosome and any associated plasmids or other replicons.\nWhen presented with metagenomic data though, the microbial community might in reality have millions of genomes present in it.\nGenerally speaking, it is impossible to accurately reconstruct every genome of every cell in a metagenomic sample with current technology.\nInstead we usually aim to reconstruct chromosome sequences that represent a consensus of many cells.",
            "title": "What is assembly?"
        },
        {
            "location": "/assembly/#assembling-metagenomes",
            "text": "There are several available metagenome assembly tools. \nSome examples include metaSPAdes and MEGAHIT.\nIn this tutorial we will use MEGAHIT.  singularity run docker://bioboxes/megahit:latest   Coassembly or single sample?  Just as important as deciding what assembler to use is deciding on  what to assemble .\nOne approach is to assemble all of the metagenome samples together.\nThis can be useful if there is not much data per sample, or if low abundance organisms exist in the samples that would not have enough coverage in a single sample to be assembled.",
            "title": "Assembling metagenomes"
        },
        {
            "location": "/assembly/#limitations-of-metagenome-assembly",
            "text": "Current assembly methods are limited in their ability to resolve chromosomes of closely related species and strains in a sample.\nThis is because most assembly methods collapse sequences > 95% or 98% identity into a single contig sequence, usually as a means to cope with sequencing error.\nTherefore as a consensus representation, these assembled chromosome sequences may mask fine-scale genetic variation in the population.",
            "title": "Limitations of metagenome assembly"
        },
        {
            "location": "/binning/",
            "text": "Metagenome Assembled Genomes (MAGs) from metagenome binning\n\n\nWhat is a MAG?\n\n\nWhat is a good MAG?\n\n\nMaking MAGs",
            "title": "Binning"
        },
        {
            "location": "/binning/#metagenome-assembled-genomes-mags-from-metagenome-binning",
            "text": "",
            "title": "Metagenome Assembled Genomes (MAGs) from metagenome binning"
        },
        {
            "location": "/binning/#what-is-a-mag",
            "text": "",
            "title": "What is a MAG?"
        },
        {
            "location": "/binning/#what-is-a-good-mag",
            "text": "",
            "title": "What is a good MAG?"
        },
        {
            "location": "/binning/#making-mags",
            "text": "",
            "title": "Making MAGs"
        },
        {
            "location": "/anvio/",
            "text": "",
            "title": "Visualization"
        },
        {
            "location": "/hic/",
            "text": "Metagenomic HiC\n\n\nWhat is Metagenomic HiC?\n\n\nAs the name suggest, Metagenomic Hi-C incorporates a new form of sequencing datas, named Hi-C, in to traditional shotgun metagenomes. In doing so, modern analyses, such as genome binning, can be performed accurately and precisely using only a single time-point.\n\n\nThe Hi-C protocol was designed to capture genome-wide evidence of DNA molecules in close physical proximity \nin vivo\n, prior to cellular lysis. With the flexible nature of a chromosome and its capacity to bend back on itself, the most frequently captured interactions are usually those between loci on the same chromosome. After this, the most frequently observed interactions are inter-chromosomal. Lastly come inter-cellular interactions, which are often far below the rates of those found within a cell.\n\n\nThe original purpose of Hi-C was to study the 3-dimensional conformation of the human genome [1], but the proximity information can just as well be exploited to infer which DNA assembly fragments belong together in a bin.  Depending on the subject of study, the implied precision of \"together\" can range from a genome to chromosome. For metagenomic studies, methods currently aim to associate assembly fragments believed to come from the same genome.\n\n\nThe Hi-C library protocol\n\n\nHi-C data-sets are generated using conventional high-throughput Illumina paired-end sequencing; the difference lies in the library protocol.\n\n\n\n\n\n\n:camera: \nFig 1\n Major steps of the Hi-C protocol\n\n{.center .small}\n\n\n\n\n\n\nProtocol outline\n\n\n\n\n\n\nDNA Fixation:\n Beginning with intact cells, the first step of the Hi-C protocol is formalin fixation. The act of cross-linking freezes close-by conformation arrangements within the DNA that existed in the cells at the time of fixation.\n\n\n\n\n\n\nCell Lysis:\n The cells are lysed and DNA-protein complexes extracted and purified.\n\n\n\n\n\n\nRestriction Digest:\n The DNA is digested using a restriction endonuclease. In metagenomic Hi-C, enzymes with large overhangs and 4-nt recognition sites are common choices (i.e. Sau3AI, MluCI, DpnII). Differences in the GC bias of the recognition site and the target DNA is an important factor, as inefficient digestion will produce fewer Hi-C read-pairs.\n\n\n\n\n\n\nBiotin Tagging:\n The overhangs produced during digestion are end-filled with biotinylated nucleotides.\n\n\n\n\n\n\nFree-end Ligation:\n in dilute conditions or immobilised on a substrate, free-ends protruding from the DNA-protein complexes are ligated. This stochastic process favours any two ends which were nearby within the complex, however random ligation and self-ligation can occur and are non-informative.\n\n\n\n\n\n\nCrosslink Reversal:\n The formalin fixation is reversed, allowing the now free DNA to be purified.\n\n\n\n\n\n\nUn-ligated End Clean-up:\n Free-ends which failed to form ligation products are unwanted in subsequent steps but could still be biotin tagged. To minimise their inclusion, a light exonuclease 3' to 5' favoured chew-back can be applied.\n\n\n\n\n\n\nDNA Sheering:\n With ligation completed, the DNA is mechanically sheered and the size range of the resulting fragment selected suitability with Illumina paired-end sequencing. \n\n\n\n\n\n\nDNA repair:\n Sonication can lead to damaged ends and nicks, which can be repaired.\n\n\n\n\n\n\nProximity Ligation Enrichment:\n Biotin tagged fragments are pulled down using affinity purification. \n\n\n\n\n\n\nAdpater Ligation:\n Illumina paired-end adapter ligation and associated steps to produce a sequencing library are now applied.\n\n\n\n\n\n\nQC: is my library ok?\n\n\nMetagenome binning with HiC data\n\n\nReferences\n\n\n\n\nLieberman-Aiden, E., van Berkum, N. L., Williams, L., Imakaev, M., Ragoczy, T., Telling, A., \u2026 Dekker, J. (2009). Comprehensive mapping of long-range interactions reveals folding principles of the human genome. \nScience\n, 326(5950), 289\u2013293. https://doi.org/10.1126/science.1181369",
            "title": "HiC"
        },
        {
            "location": "/hic/#metagenomic-hic",
            "text": "",
            "title": "Metagenomic HiC"
        },
        {
            "location": "/hic/#what-is-metagenomic-hic",
            "text": "As the name suggest, Metagenomic Hi-C incorporates a new form of sequencing datas, named Hi-C, in to traditional shotgun metagenomes. In doing so, modern analyses, such as genome binning, can be performed accurately and precisely using only a single time-point.  The Hi-C protocol was designed to capture genome-wide evidence of DNA molecules in close physical proximity  in vivo , prior to cellular lysis. With the flexible nature of a chromosome and its capacity to bend back on itself, the most frequently captured interactions are usually those between loci on the same chromosome. After this, the most frequently observed interactions are inter-chromosomal. Lastly come inter-cellular interactions, which are often far below the rates of those found within a cell.  The original purpose of Hi-C was to study the 3-dimensional conformation of the human genome [1], but the proximity information can just as well be exploited to infer which DNA assembly fragments belong together in a bin.  Depending on the subject of study, the implied precision of \"together\" can range from a genome to chromosome. For metagenomic studies, methods currently aim to associate assembly fragments believed to come from the same genome.",
            "title": "What is Metagenomic HiC?"
        },
        {
            "location": "/hic/#the-hi-c-library-protocol",
            "text": "Hi-C data-sets are generated using conventional high-throughput Illumina paired-end sequencing; the difference lies in the library protocol.",
            "title": "The Hi-C library protocol"
        },
        {
            "location": "/hic/#qc-is-my-library-ok",
            "text": "",
            "title": "QC: is my library ok?"
        },
        {
            "location": "/hic/#metagenome-binning-with-hic-data",
            "text": "",
            "title": "Metagenome binning with HiC data"
        },
        {
            "location": "/hic/#references",
            "text": "Lieberman-Aiden, E., van Berkum, N. L., Williams, L., Imakaev, M., Ragoczy, T., Telling, A., \u2026 Dekker, J. (2009). Comprehensive mapping of long-range interactions reveals folding principles of the human genome.  Science , 326(5950), 289\u2013293. https://doi.org/10.1126/science.1181369",
            "title": "References"
        }
    ]
}